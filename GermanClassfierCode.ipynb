{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3naoHFZJAs7",
        "outputId": "b5299e92-3b11-4816-e58f-5ef86aa7a9b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aif360[LFR]\n",
            "  Downloading aif360-0.4.0-py3-none-any.whl (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 18.5 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 10.8 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 112 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 122 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 174 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 7.7 MB/s \n",
            "\u001b[33mWARNING: aif360 0.4.0 does not provide the extra 'lfr'\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.21.6)\n",
            "Requirement already satisfied: scipy<1.6.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.4.1)\n",
            "Collecting tempeh\n",
            "  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360[LFR]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360[LFR]) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360[LFR]) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360[LFR]) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360[LFR]) (3.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[LFR]) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[LFR]) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[LFR]) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->aif360[LFR]) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360[LFR]) (2.23.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360[LFR]) (3.6.4)\n",
            "Collecting shap\n",
            "  Downloading shap-0.40.0-cp37-cp37m-manylinux2010_x86_64.whl (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 42.3 MB/s \n",
            "\u001b[?25hCollecting memory-profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler->tempeh->aif360[LFR]) (5.4.8)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (8.12.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (21.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (57.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (2021.10.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (0.51.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (4.64.0)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (21.3)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap->tempeh->aif360[LFR]) (0.34.0)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=ad9a3bfdb1234aa92b34c82feea7d4825199ee3493110919ca9ebdc8bbd47a9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: slicer, shap, memory-profiler, tempeh, aif360\n",
            "Successfully installed aif360-0.4.0 memory-profiler-0.60.0 shap-0.40.0 slicer-0.0.7 tempeh-0.1.12\n"
          ]
        }
      ],
      "source": [
        "!pip install 'aif360[LFR]'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /usr/local/lib/python3.7/dist-packages/aif360/data/raw/german"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpD9C8iUJNir",
        "outputId": "15e2cdb9-ddf6-4048-b3ed-63253dca4f8f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/aif360/data/raw/german\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGkQjadsJQE6",
        "outputId": "0a138cec-bbed-4384-989e-d5e4eaca43a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-13 10:56:41--  https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 79793 (78K) [application/x-httpd-php]\n",
            "Saving to: ‘german.data’\n",
            "\n",
            "german.data         100%[===================>]  77.92K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-05-13 10:56:41 (2.98 MB/s) - ‘german.data’ saved [79793/79793]\n",
            "\n",
            "--2022-05-13 10:56:41--  https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4679 (4.6K) [application/x-httpd-php]\n",
            "Saving to: ‘german.doc’\n",
            "\n",
            "german.doc          100%[===================>]   4.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-13 10:56:41 (95.0 MB/s) - ‘german.doc’ saved [4679/4679]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aequitas==0.42.0\n",
        "!pip install fairlearn==0.4.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiLMfLlZQx4O",
        "outputId": "e33aa25a-ffe0-40d6-ad8e-6469d08ab9a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aequitas==0.42.0\n",
            "  Downloading aequitas-0.42.0-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 10.1 MB/s \n",
            "\u001b[?25hCollecting ohio>=0.2.0\n",
            "  Downloading ohio-0.5.0-py3-none-any.whl (26 kB)\n",
            "Collecting xhtml2pdf==0.2.2\n",
            "  Downloading xhtml2pdf-0.2.2.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aequitas==0.42.0) (1.4.36)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from aequitas==0.42.0) (3.2.2)\n",
            "Collecting tabulate==0.8.2\n",
            "  Downloading tabulate-0.8.2.tar.gz (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from aequitas==0.42.0) (0.11.2)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 36.3 MB/s \n",
            "\u001b[?25hCollecting altair==4.1.0\n",
            "  Downloading altair-4.1.0-py3-none-any.whl (727 kB)\n",
            "\u001b[K     |████████████████████████████████| 727 kB 40.1 MB/s \n",
            "\u001b[?25hCollecting markdown2==2.3.5\n",
            "  Downloading markdown2-2.3.5.zip (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 38.1 MB/s \n",
            "\u001b[?25hCollecting Flask==0.12.2\n",
            "  Downloading Flask-0.12.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from aequitas==0.42.0) (1.3.5)\n",
            "Collecting Flask-Bootstrap==3.3.7.1\n",
            "  Downloading Flask-Bootstrap-3.3.7.1.tar.gz (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 45.2 MB/s \n",
            "\u001b[?25hCollecting millify==0.1.1\n",
            "  Downloading millify-0.1.1.tar.gz (1.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from altair==4.1.0->aequitas==0.42.0) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair==4.1.0->aequitas==0.42.0) (2.11.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair==4.1.0->aequitas==0.42.0) (0.11.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair==4.1.0->aequitas==0.42.0) (0.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair==4.1.0->aequitas==0.42.0) (4.3.3)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.7/dist-packages (from Flask==0.12.2->aequitas==0.42.0) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.7/dist-packages (from Flask==0.12.2->aequitas==0.42.0) (1.0.1)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask==0.12.2->aequitas==0.42.0) (7.1.2)\n",
            "Collecting dominate\n",
            "  Downloading dominate-2.6.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting visitor\n",
            "  Downloading visitor-0.1.3.tar.gz (3.3 kB)\n",
            "Requirement already satisfied: html5lib>=1.0 in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf==0.2.2->aequitas==0.42.0) (1.0.1)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf==0.2.2->aequitas==0.42.0) (0.17.4)\n",
            "Collecting pyPdf2\n",
            "  Downloading PyPDF2-1.27.12-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf==0.2.2->aequitas==0.42.0) (7.1.2)\n",
            "Collecting reportlab>=3.0\n",
            "  Downloading reportlab-3.6.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 35.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf==0.2.2->aequitas==0.42.0) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib>=1.0->xhtml2pdf==0.2.2->aequitas==0.42.0) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair==4.1.0->aequitas==0.42.0) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->aequitas==0.42.0) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->aequitas==0.42.0) (3.0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->aequitas==0.42.0) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->aequitas==0.42.0) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.3->aequitas==0.42.0) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.1->aequitas==0.42.0) (2022.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.9.0->aequitas==0.42.0) (1.4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy>=1.1.1->aequitas==0.42.0) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy>=1.1.1->aequitas==0.42.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->SQLAlchemy>=1.1.1->aequitas==0.42.0) (3.8.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1.0->aequitas==0.42.0) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1.0->aequitas==0.42.0) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1.0->aequitas==0.42.0) (5.7.1)\n",
            "Building wheels for collected packages: Flask-Bootstrap, markdown2, millify, tabulate, xhtml2pdf, visitor\n",
            "  Building wheel for Flask-Bootstrap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Flask-Bootstrap: filename=Flask_Bootstrap-3.3.7.1-py3-none-any.whl size=460123 sha256=29db1d09457a028e47b8b6f19d3d4c943d5b65f21d42cd68b9f5372f5c8ceb79\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/a2/d6/50d039c9b59b4caca6d7b53839c8100354a52ab7553d2456eb\n",
            "  Building wheel for markdown2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markdown2: filename=markdown2-2.3.5-py3-none-any.whl size=33327 sha256=a221739d475b3b99a69393684ebd0b7114efff54ad1826c0a2100974a1739f7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/b9/ae/4050b5eeeedc7cba8ed5a0203189c89c0fa980f683822bfa31\n",
            "  Building wheel for millify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for millify: filename=millify-0.1.1-py3-none-any.whl size=1866 sha256=7b0ed8ebf863e58b33a50763624ecc1beba2b8c3046b00830e8aedf7d9c0139b\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/26/25/c2a8bb99a5cf348903e6ac35a29878e221cc9daeb698545148\n",
            "  Building wheel for tabulate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tabulate: filename=tabulate-0.8.2-py3-none-any.whl size=23550 sha256=3fa804932f59de228fe4427bf5d0951d0a03637fa417f7ea3cc15a48e99d025c\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/63/72/4156fe55e8e06830d7aed3d20a6d1aacc753536843ab7330f6\n",
            "  Building wheel for xhtml2pdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xhtml2pdf: filename=xhtml2pdf-0.2.2-py3-none-any.whl size=230265 sha256=63f83426b69da70a5a3c1b678d4f8bb3929a464cc68e719b208ff5e64becd753\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/e6/3a/9851102d40dd8e643a4ff3ce5d69988f95d1d9b7448e37a916\n",
            "  Building wheel for visitor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visitor: filename=visitor-0.1.3-py3-none-any.whl size=3946 sha256=edc74023d0e2bb3de5c241d8eb7a69bcd627a956c1886d41a6cbcd94ec2e3f2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/34/11/053f47218984c9a31a00f911ed98dda036b867481dcc527a12\n",
            "Successfully built Flask-Bootstrap markdown2 millify tabulate xhtml2pdf visitor\n",
            "Installing collected packages: visitor, reportlab, pyPdf2, Flask, dominate, xhtml2pdf, tabulate, pyyaml, ohio, millify, markdown2, Flask-Bootstrap, altair, aequitas\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.8.9\n",
            "    Uninstalling tabulate-0.8.9:\n",
            "      Successfully uninstalled tabulate-0.8.9\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.0\n",
            "    Uninstalling altair-4.2.0:\n",
            "      Successfully uninstalled altair-4.2.0\n",
            "Successfully installed Flask-0.12.2 Flask-Bootstrap-3.3.7.1 aequitas-0.42.0 altair-4.1.0 dominate-2.6.0 markdown2-2.3.5 millify-0.1.1 ohio-0.5.0 pyPdf2-1.27.12 pyyaml-6.0 reportlab-3.6.9 tabulate-0.8.2 visitor-0.1.3 xhtml2pdf-0.2.2\n",
            "Collecting fairlearn==0.4.6\n",
            "  Downloading fairlearn-0.4.6-py3-none-any.whl (21.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: ipywidgets>=7.5.0 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6) (7.7.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (0.2.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (1.1.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (5.1.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (3.6.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (4.10.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (5.3.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.1.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (57.4.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.10.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (2.15.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.18.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.7.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn==0.4.6) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn==0.4.6) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn==0.4.6) (3.1.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.13.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (2.11.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn==0.4.6) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (2.0.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.0.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.6.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (1.5.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.8.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.5.1)\n",
            "Installing collected packages: fairlearn\n",
            "Successfully installed fairlearn-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing all libraries**"
      ],
      "metadata": {
        "id": "ZE4oS2UUJkZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler  #MinMaxScaler\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing"
      ],
      "metadata": {
        "id": "g6zzbx7wJShZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aif360.datasets import AdultDataset\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_german\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from aif360.algorithms.preprocessing.reweighing import Reweighing"
      ],
      "metadata": {
        "id": "iIa3PbYfJqW4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the fairness Matrixs"
      ],
      "metadata": {
        "id": "ELguKCpVJy1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "races = ['Young','Old']\n",
        "\n",
        "privileged_groups = [{'age': 0}]\n",
        "unprivileged_groups = [{'age': 1}]\n",
        "dataset_orig = load_preproc_data_german(['age'])"
      ],
      "metadata": {
        "id": "8TGF_bJ5Jurh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Managing the dataset into train and test"
      ],
      "metadata": {
        "id": "o1l-yak0J5qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train, test = dataset_orig.split([0.7], shuffle=True)\n",
        "print(\"training data size\", train.features.shape)\n",
        "print(\"dataset feature names\", train.feature_names)\n",
        "\n",
        "#Normalize the dataset, both train and test. This should always be done in any machine learning pipeline!\n",
        "scale_orig = StandardScaler()\n",
        "X_train = scale_orig.fit_transform(train.features)\n",
        "y_train = train.labels.ravel()\n",
        "\n",
        "X_test = scale_orig.transform(test.features) \n",
        "y_test = test.labels.ravel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oev9CI5lJ9rJ",
        "outputId": "e4522b0f-3e84-49c1-d64e-a5976cbd31f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data size (700, 11)\n",
            "dataset feature names ['age', 'sex', 'credit_history=Delay', 'credit_history=None/Paid', 'credit_history=Other', 'savings=500+', 'savings=<500', 'savings=Unknown/None', 'employment=1-4 years', 'employment=4+ years', 'employment=Unemployed']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions used in all the tasks"
      ],
      "metadata": {
        "id": "U7-GcZwBK6sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_score(predictions, labels):\n",
        "    \n",
        "    correct_ans = []\n",
        "    \n",
        "    for i in labels:\n",
        "      correct_ans.append(i)\n",
        "    \n",
        "    lists = zip(predictions, correct_ans)\n",
        "    \n",
        "    preds = []\n",
        "    corrects = []\n",
        "    \n",
        "    score = 0\n",
        "    \n",
        "    for i in lists:\n",
        "        if int(round(i[0])) == int(round(i[1])):\n",
        "            score = score + 1\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        var = round(i[0])\n",
        "        preds.append(int(var))\n",
        "        corrects.append(i[1])\n",
        "      \n",
        "    \n",
        "    accuracy = score/len(labels)\n",
        "    print(\"Model Accuracy\", accuracy)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "RXmJCZFeLAU6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 Finding an 5 Fold Cross validation for Accuracy and Fairness**"
      ],
      "metadata": {
        "id": "GMoOXQQSKEF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.features.shape)\n",
        "print()\n",
        "trainVar, trainA = train.split([0.8], shuffle=True)\n",
        "\n",
        "print(trainA.features.shape)\n",
        "\n",
        "trainVar, trainB = trainVar.split([0.75], shuffle=True)\n",
        "\n",
        "print(trainB.features.shape)\n",
        "\n",
        "trainVar, trainC = trainVar.split([0.66666], shuffle=True)\n",
        "\n",
        "print(trainC.features.shape)\n",
        "\n",
        "trainD, trainE = trainVar.split([0.5], shuffle=True)\n",
        "\n",
        "print(trainD.features.shape)\n",
        "print(trainE.features.shape)\n",
        "\n",
        "train_cross = [trainA, trainB, trainC, trainD, trainE]\n",
        "X_train_cross = train_cross\n",
        "Y_train_cross = [0] * 5\n",
        "\n",
        "for i in range(len(train_cross)):\n",
        "    Y_train_cross[i] = train_cross[i].labels.ravel()\n",
        "\n",
        "scale_orig = StandardScaler()   #setup the scaler object\n",
        "\n",
        "for i in range(len(train_cross)):\n",
        "    X_train_cross[i] = scale_orig.fit_transform(train_cross[i].features) #scale both features and labels\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuZAm616KNCJ",
        "outputId": "5bfb1867-8c68-4da7-802b-54cb9dec4711"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(700, 11)\n",
            "\n",
            "(140, 11)\n",
            "(140, 11)\n",
            "(141, 11)\n",
            "(139, 11)\n",
            "(140, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seaborn import matrix\n",
        "Cs = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "scores = []\n",
        "\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "\n",
        "datas = {}\n",
        "\n",
        "with open('task1crossval2Results.csv', 'w') as csvfile:\n",
        "    field_names = ['Activation', 'C' , 'crossVal1', 'crossVal2', 'crossVal3', 'crossVal4', 'crossVal5','Accuracy', 'general', \n",
        "                   'Par_diff1', 'Par_diff2', 'Par_diff3', 'Par_diff4', 'Par_diff5', 'par_diff_avg',\n",
        "                   'ep_opp1', 'ep_opp2','ep_opp3', 'ep_opp4', 'ep_opp5', 'ep_opp_avg',\n",
        "                   'avg_odd1', 'avg_odd2', 'avg_odd3', 'avg_odd4', 'avg_odd5','avg_odds_avg' ,'TPR', 'FPR'] #Name of the columns for the .csv file\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
        "    writer.writeheader()\n",
        "\n",
        "\n",
        "    for solv in solvers:\n",
        "          for c in Cs:\n",
        "              classifier = LogisticRegression(C=c, solver=solv)\n",
        "\n",
        "              data = {}\n",
        "              accuracies = []\n",
        "              cs = []\n",
        "              activations = []\n",
        "              par_diff = []\n",
        "              ep_opp = []\n",
        "              avg_odds = []\n",
        "              TPR = []\n",
        "              FPR = []\n",
        "\n",
        "\n",
        "              for i in range(len(train_cross)):\n",
        "\n",
        "                  X_test_cross = []\n",
        "                  Y_test_cross = []\n",
        "                  indexs = []\n",
        "                  ind = []\n",
        "                  X = []\n",
        "                  Y = []\n",
        "\n",
        "                  for j in range(len(train_cross)):\n",
        "                      if i == j:\n",
        "                          X_test_cross = X_train_cross[j]\n",
        "                          Y_test_cross = Y_train_cross[j]\n",
        "                          ind.append(j)\n",
        "                      else:\n",
        "                          indexs.append(j)\n",
        "\n",
        "\n",
        "                  X = np.concatenate([X_train_cross[indexs[0]], X_train_cross[indexs[1]], X_train_cross[indexs[2]]])\n",
        "                  Y = np.concatenate([Y_train_cross[indexs[0]], Y_train_cross[indexs[1]], Y_train_cross[indexs[2]]])\n",
        "                  X = np.concatenate([X, X_train_cross[indexs[3]]])\n",
        "                  Y = np.concatenate([Y, Y_train_cross[indexs[3]]])\n",
        "\n",
        "                  LR = classifier.fit(X, Y) #fit all the data that is not in the test\n",
        "\n",
        "                  predictions = LR.predict(X_test_cross)\n",
        "                  \n",
        "                  acc = find_score(predictions, Y_test_cross)\n",
        "\n",
        "                  train_copy = trainA\n",
        "\n",
        "\n",
        "                  if i == 0:\n",
        "                    train_copy = trainA\n",
        "\n",
        "                  elif i == 1:\n",
        "                    train_copy = trainB\n",
        "                  \n",
        "                  elif i == 2:\n",
        "                    train_copy = trainC\n",
        "\n",
        "                  elif i == 3:\n",
        "                    train_copy = trainD\n",
        "                  \n",
        "                  elif i == 4:\n",
        "                    train_copy = trainE\n",
        "\n",
        "                  else:\n",
        "                    print(\"Error\")\n",
        "\n",
        "                  test_pred = train_copy.copy()\n",
        "                  predictions.resize((len(predictions),1))\n",
        "                  test_pred.labels = predictions\n",
        "\n",
        "                  metric = ClassificationMetric(train_copy, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "                  metric_arrs = {}\n",
        "                  metric_arrs['C'] = c\n",
        "                  metric_arrs['Activation'] = solv\n",
        "                  metric_arrs['Accuracy'] = acc\n",
        "                  metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "                  metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "                  metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "                  #metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "                  #metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "                  metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "                  metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "                  #print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, acc))\n",
        "                  #print(metric_arrs)\n",
        "                  #print('\\n')\n",
        "\n",
        "\n",
        "                  accuracies.append(acc)\n",
        "                  cs.append(c)\n",
        "                  activations.append(solv)\n",
        "                  par_diff.append((metric.statistical_parity_difference()))\n",
        "                  ep_opp.append((metric.equal_opportunity_difference()))\n",
        "                  avg_odds.append((metric.average_odds_difference()))\n",
        "                  TPR.append((metric.generalized_true_positive_rate()))\n",
        "                  FPR.append((metric.num_generalized_false_positives()))\n",
        "\n",
        "\n",
        "              print(\"Accuracies: \", accuracies)\n",
        "              print(\"Cs\", c)\n",
        "              print(\"Activation: \", activations)\n",
        "              print(\"Par_diff: \", par_diff)\n",
        "              print(\"ep_opp: \", ep_opp)\n",
        "              print(\"avg_odd: \", avg_odds)\n",
        "              print(\"TPR: \", TPR)\n",
        "              print(\"FPR: \", FPR)\n",
        "\n",
        "              pred_test = LR.predict(X_test)\n",
        "              test_acc = find_score(pred_test, y_test)\n",
        "              print(\"Generalisability --- \", test_acc)\n",
        "\n",
        "              acc = sum(accuracies)/len(accuracies)\n",
        "              par_diff_avg = sum(par_diff)/len(par_diff)\n",
        "              ep_opp_avg = sum(ep_opp)/len(ep_opp)\n",
        "              avg_odds_avg = sum(avg_odds)/len(avg_odds)\n",
        "\n",
        "              writer.writerow({'Activation': solv, 'C': c , 'crossVal1': accuracies[0], 'crossVal2': accuracies[1], 'crossVal3': accuracies[2], 'crossVal4': accuracies[3], 'crossVal5': accuracies[4],'Accuracy': acc, 'general': test_acc, \n",
        "                               'Par_diff1': par_diff[0], 'Par_diff2': par_diff[1], 'Par_diff3': par_diff[2], 'Par_diff4': par_diff[3], 'Par_diff5': par_diff[4], 'par_diff_avg': par_diff_avg,\n",
        "                   'ep_opp1': ep_opp[0], 'ep_opp2': ep_opp[1],'ep_opp3': ep_opp[2], 'ep_opp4': ep_opp[3], 'ep_opp5': ep_opp[4], 'ep_opp_avg': ep_opp_avg,\n",
        "                   'avg_odd1': avg_odds[0], 'avg_odd2': avg_odds[1], 'avg_odd3': avg_odds[2], 'avg_odd4': avg_odds[3], 'avg_odd5': avg_odds[4],'avg_odds_avg': avg_odds_avg , 'TPR': TPR[0], 'FPR': FPR[0]}) #save the data into .csv\n",
        "\n",
        "          \n",
        "files.download('task1crossval2Results.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CuE4D0sKKSHZ",
        "outputId": "3f8a4cae-340c-4be6-a74c-583db17ae6e2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.6928571428571428\n",
            "Model Accuracy 0.65\n",
            "Model Accuracy 0.6666666666666666\n",
            "Model Accuracy 0.6618705035971223\n",
            "Model Accuracy 0.6928571428571428\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 1e-05\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "Model Accuracy 0.65\n",
            "Model Accuracy 0.6666666666666666\n",
            "Model Accuracy 0.6618705035971223\n",
            "Model Accuracy 0.6928571428571428\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 0.0001\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "Model Accuracy 0.65\n",
            "Model Accuracy 0.6666666666666666\n",
            "Model Accuracy 0.6618705035971223\n",
            "Model Accuracy 0.6928571428571428\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 0.001\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6857142857142857\n",
            "Model Accuracy 0.6571428571428571\n",
            "Model Accuracy 0.6666666666666666\n",
            "Model Accuracy 0.6618705035971223\n",
            "Model Accuracy 0.7\n",
            "Accuracies:  [0.6857142857142857, 0.6571428571428571, 0.6666666666666666, 0.6618705035971223, 0.7]\n",
            "Cs 0.01\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.29032258064516125, 0.21739130434782605, 0.0, 0.0, 0.36]\n",
            "ep_opp:  [0.25, 0.18181818181818177, 0.0, 0.0, 0.33333333333333337]\n",
            "avg_odd:  [0.3068181818181818, 0.21590909090909088, 0.0, 0.0, 0.358974358974359]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.77\n",
            "Generalisability ---  0.77\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 0.1\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.5806451612903225, 0.4347826086956522, 0.4193548387096774, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.5, 0.2727272727272727, 0.38888888888888884, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6136363636363636, 0.428030303030303, 0.4252136752136752, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6453900709219859\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6453900709219859, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 1\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.5806451612903225, 0.6086956521739131, 0.4838709677419355, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.5, 0.4545454545454546, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6136363636363636, 0.6022727272727273, 0.4807692307692308, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 10\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.5806451612903225, 0.6086956521739131, 0.5483870967741935, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.5, 0.4545454545454546, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6136363636363636, 0.6022727272727273, 0.5576923076923077, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6714285714285714, 0.6714285714285714, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 100\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.6129032258064516, 0.4782608695652174, 0.5483870967741935, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.55, 0.36363636363636365, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6386363636363637, 0.47348484848484845, 0.5576923076923077, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6714285714285714, 0.6714285714285714, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 1000\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.6129032258064516, 0.4782608695652174, 0.5483870967741935, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.55, 0.36363636363636365, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6386363636363637, 0.47348484848484845, 0.5576923076923077, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6928571428571428\n",
            "Model Accuracy 0.65\n",
            "Model Accuracy 0.6666666666666666\n",
            "Model Accuracy 0.6618705035971223\n",
            "Model Accuracy 0.6928571428571428\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 1e-05\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "Model Accuracy 0.65\n",
            "Model Accuracy 0.6666666666666666\n",
            "Model Accuracy 0.6618705035971223\n",
            "Model Accuracy 0.6928571428571428\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 0.0001\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "Model Accuracy 0.65\n",
            "Model Accuracy 0.6666666666666666\n",
            "Model Accuracy 0.6618705035971223\n",
            "Model Accuracy 0.6928571428571428\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 0.001\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6857142857142857\n",
            "Model Accuracy 0.6571428571428571\n",
            "Model Accuracy 0.6666666666666666\n",
            "Model Accuracy 0.6618705035971223\n",
            "Model Accuracy 0.7\n",
            "Accuracies:  [0.6857142857142857, 0.6571428571428571, 0.6666666666666666, 0.6618705035971223, 0.7]\n",
            "Cs 0.01\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.29032258064516125, 0.21739130434782605, 0.0, 0.0, 0.36]\n",
            "ep_opp:  [0.25, 0.18181818181818177, 0.0, 0.0, 0.33333333333333337]\n",
            "avg_odd:  [0.3068181818181818, 0.21590909090909088, 0.0, 0.0, 0.358974358974359]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.77\n",
            "Generalisability ---  0.77\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 0.1\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.5806451612903225, 0.4347826086956522, 0.4193548387096774, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.5, 0.2727272727272727, 0.38888888888888884, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6136363636363636, 0.428030303030303, 0.4252136752136752, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6453900709219859\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6453900709219859, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 1\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.5806451612903225, 0.6086956521739131, 0.4838709677419355, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.5, 0.4545454545454546, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6136363636363636, 0.6022727272727273, 0.4807692307692308, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 10\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.5806451612903225, 0.6086956521739131, 0.5483870967741935, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.5, 0.4545454545454546, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6136363636363636, 0.6022727272727273, 0.5576923076923077, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6714285714285714, 0.6714285714285714, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 100\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.6129032258064516, 0.4782608695652174, 0.5483870967741935, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.55, 0.36363636363636365, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6386363636363637, 0.47348484848484845, 0.5576923076923077, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6714285714285714, 0.6714285714285714, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 1000\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.6129032258064516, 0.4782608695652174, 0.5483870967741935, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.55, 0.36363636363636365, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6386363636363637, 0.47348484848484845, 0.5576923076923077, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6357142857142857\n",
            "Model Accuracy 0.6642857142857143\n",
            "Model Accuracy 0.6312056737588653\n",
            "Model Accuracy 0.6187050359712231\n",
            "Model Accuracy 0.6142857142857143\n",
            "Accuracies:  [0.6357142857142857, 0.6642857142857143, 0.6312056737588653, 0.6187050359712231, 0.6142857142857143]\n",
            "Cs 1e-05\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.5676235572654632, 0.4384986993682646, 0.27565982404692085, 0.26358543417366953, 0.5791304347826086]\n",
            "ep_opp:  [0.5571428571428572, 0.3829545454545455, 0.2763157894736842, 0.23684210526315785, 0.5862745098039216]\n",
            "avg_odd:  [0.5626623376623376, 0.4043151105651106, 0.25467373184091446, 0.23432143577334139, 0.5662141779788838]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.6633333333333333\n",
            "Generalisability ---  0.6633333333333333\n",
            "Model Accuracy 0.6428571428571429\n",
            "Model Accuracy 0.6642857142857143\n",
            "Model Accuracy 0.6312056737588653\n",
            "Model Accuracy 0.6187050359712231\n",
            "Model Accuracy 0.6142857142857143\n",
            "Accuracies:  [0.6428571428571429, 0.6642857142857143, 0.6312056737588653, 0.6187050359712231, 0.6142857142857143]\n",
            "Cs 0.0001\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.4708493637170761, 0.4384986993682646, 0.27565982404692085, 0.26358543417366953, 0.5791304347826086]\n",
            "ep_opp:  [0.4571428571428571, 0.3829545454545455, 0.2763157894736842, 0.23684210526315785, 0.5862745098039216]\n",
            "avg_odd:  [0.4672077922077922, 0.4043151105651106, 0.25467373184091446, 0.23432143577334139, 0.5662141779788838]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.6633333333333333\n",
            "Generalisability ---  0.6633333333333333\n",
            "Model Accuracy 0.6571428571428571\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.624113475177305\n",
            "Model Accuracy 0.6187050359712231\n",
            "Model Accuracy 0.6142857142857143\n",
            "Accuracies:  [0.6571428571428571, 0.6714285714285714, 0.624113475177305, 0.6187050359712231, 0.6142857142857143]\n",
            "Cs 0.001\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.406333234684818, 0.4470457079152731, 0.2847507331378299, 0.26358543417366953, 0.5791304347826086]\n",
            "ep_opp:  [0.3571428571428571, 0.39545454545454545, 0.2763157894736842, 0.23684210526315785, 0.5862745098039216]\n",
            "avg_odd:  [0.4172077922077922, 0.4105651105651106, 0.2693796141938557, 0.23432143577334139, 0.5662141779788838]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.6633333333333333\n",
            "Generalisability ---  0.6633333333333333\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.6785714285714286\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6595744680851063, 0.6834532374100719, 0.6785714285714286]\n",
            "Cs 0.01\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.5806451612903225, 0.6086956521739131, 0.5483870967741935, 0.5588235294117647, 0.6330434782608695]\n",
            "ep_opp:  [0.5, 0.4545454545454546, 0.5, 0.5, 0.584313725490196]\n",
            "avg_odd:  [0.6136363636363636, 0.6022727272727273, 0.5576923076923077, 0.5555555555555556, 0.6267722473604826]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6453900709219859\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6453900709219859, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 0.1\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.5806451612903225, 0.6086956521739131, 0.4838709677419355, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.5, 0.4545454545454546, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6136363636363636, 0.6022727272727273, 0.4807692307692308, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 1\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.5806451612903225, 0.6086956521739131, 0.5483870967741935, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.5, 0.4545454545454546, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6136363636363636, 0.6022727272727273, 0.5576923076923077, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6785714285714286\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 10\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.5806451612903225, 0.6086956521739131, 0.5483870967741935, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.5, 0.4545454545454546, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6136363636363636, 0.6022727272727273, 0.5576923076923077, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6714285714285714, 0.6714285714285714, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 100\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.6129032258064516, 0.4782608695652174, 0.5483870967741935, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.55, 0.36363636363636365, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6386363636363637, 0.47348484848484845, 0.5576923076923077, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6714285714285714\n",
            "Model Accuracy 0.6595744680851063\n",
            "Model Accuracy 0.6834532374100719\n",
            "Model Accuracy 0.7142857142857143\n",
            "Accuracies:  [0.6714285714285714, 0.6714285714285714, 0.6595744680851063, 0.6834532374100719, 0.7142857142857143]\n",
            "Cs 1000\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.6129032258064516, 0.4782608695652174, 0.5483870967741935, 0.5588235294117647, 0.6799999999999999]\n",
            "ep_opp:  [0.55, 0.36363636363636365, 0.5, 0.5, 0.5833333333333333]\n",
            "avg_odd:  [0.6386363636363637, 0.47348484848484845, 0.5576923076923077, 0.5555555555555556, 0.6762820512820512]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_64b94a86-b2fb-4a0b-b12c-3c64b92e9eef\", \"task1crossval2Results.csv\", 11021)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting the Hyperparamters for TASK1** "
      ],
      "metadata": {
        "id": "WNBZAzYgrnKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task1_df = pd.read_csv(\"task1crossval2Results.csv\")\n",
        "task1_df"
      ],
      "metadata": {
        "id": "MTmGYml6sATl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "outputId": "5f950cf5-eca9-40b4-e7d7-49016e612a7d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Activation           C  crossVal1  crossVal2  crossVal3  crossVal4  \\\n",
              "0   newton-cg     0.00001   0.692857   0.650000   0.666667   0.661871   \n",
              "1   newton-cg     0.00010   0.692857   0.650000   0.666667   0.661871   \n",
              "2   newton-cg     0.00100   0.692857   0.650000   0.666667   0.661871   \n",
              "3   newton-cg     0.01000   0.685714   0.657143   0.666667   0.661871   \n",
              "4   newton-cg     0.10000   0.678571   0.678571   0.659574   0.683453   \n",
              "5   newton-cg     1.00000   0.678571   0.678571   0.645390   0.683453   \n",
              "6   newton-cg    10.00000   0.678571   0.678571   0.659574   0.683453   \n",
              "7   newton-cg   100.00000   0.671429   0.671429   0.659574   0.683453   \n",
              "8   newton-cg  1000.00000   0.671429   0.671429   0.659574   0.683453   \n",
              "9       lbfgs     0.00001   0.692857   0.650000   0.666667   0.661871   \n",
              "10      lbfgs     0.00010   0.692857   0.650000   0.666667   0.661871   \n",
              "11      lbfgs     0.00100   0.692857   0.650000   0.666667   0.661871   \n",
              "12      lbfgs     0.01000   0.685714   0.657143   0.666667   0.661871   \n",
              "13      lbfgs     0.10000   0.678571   0.678571   0.659574   0.683453   \n",
              "14      lbfgs     1.00000   0.678571   0.678571   0.645390   0.683453   \n",
              "15      lbfgs    10.00000   0.678571   0.678571   0.659574   0.683453   \n",
              "16      lbfgs   100.00000   0.671429   0.671429   0.659574   0.683453   \n",
              "17      lbfgs  1000.00000   0.671429   0.671429   0.659574   0.683453   \n",
              "18  liblinear     0.00001   0.635714   0.664286   0.631206   0.618705   \n",
              "19  liblinear     0.00010   0.642857   0.664286   0.631206   0.618705   \n",
              "20  liblinear     0.00100   0.657143   0.671429   0.624113   0.618705   \n",
              "21  liblinear     0.01000   0.678571   0.678571   0.659574   0.683453   \n",
              "22  liblinear     0.10000   0.678571   0.678571   0.645390   0.683453   \n",
              "23  liblinear     1.00000   0.678571   0.678571   0.659574   0.683453   \n",
              "24  liblinear    10.00000   0.678571   0.678571   0.659574   0.683453   \n",
              "25  liblinear   100.00000   0.671429   0.671429   0.659574   0.683453   \n",
              "26  liblinear  1000.00000   0.671429   0.671429   0.659574   0.683453   \n",
              "\n",
              "    crossVal5  Accuracy   general  Par_diff1  ...   ep_opp5  ep_opp_avg  \\\n",
              "0    0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "1    0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "2    0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "3    0.700000  0.674279  0.770000   0.290323  ...  0.333333    0.153030   \n",
              "4    0.714286  0.682891  0.746667   0.580645  ...  0.583333    0.448990   \n",
              "5    0.714286  0.680054  0.746667   0.580645  ...  0.583333    0.507576   \n",
              "6    0.714286  0.682891  0.746667   0.580645  ...  0.583333    0.507576   \n",
              "7    0.714286  0.680034  0.746667   0.612903  ...  0.583333    0.499394   \n",
              "8    0.714286  0.680034  0.746667   0.612903  ...  0.583333    0.499394   \n",
              "9    0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "10   0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "11   0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "12   0.700000  0.674279  0.770000   0.290323  ...  0.333333    0.153030   \n",
              "13   0.714286  0.682891  0.746667   0.580645  ...  0.583333    0.448990   \n",
              "14   0.714286  0.680054  0.746667   0.580645  ...  0.583333    0.507576   \n",
              "15   0.714286  0.682891  0.746667   0.580645  ...  0.583333    0.507576   \n",
              "16   0.714286  0.680034  0.746667   0.612903  ...  0.583333    0.499394   \n",
              "17   0.714286  0.680034  0.746667   0.612903  ...  0.583333    0.499394   \n",
              "18   0.614286  0.632839  0.663333   0.567624  ...  0.586275    0.407906   \n",
              "19   0.614286  0.634268  0.663333   0.470849  ...  0.586275    0.387906   \n",
              "20   0.614286  0.637135  0.663333   0.406333  ...  0.586275    0.370406   \n",
              "21   0.678571  0.675748  0.740000   0.580645  ...  0.584314    0.507772   \n",
              "22   0.714286  0.680054  0.746667   0.580645  ...  0.583333    0.507576   \n",
              "23   0.714286  0.682891  0.746667   0.580645  ...  0.583333    0.507576   \n",
              "24   0.714286  0.682891  0.746667   0.580645  ...  0.583333    0.507576   \n",
              "25   0.714286  0.680034  0.746667   0.612903  ...  0.583333    0.499394   \n",
              "26   0.714286  0.680034  0.746667   0.612903  ...  0.583333    0.499394   \n",
              "\n",
              "    avg_odd1  avg_odd2  avg_odd3  avg_odd4  avg_odd5  avg_odds_avg  TPR  FPR  \n",
              "0   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "1   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "2   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "3   0.306818  0.215909  0.000000  0.000000  0.358974      0.176340  1.0  0.0  \n",
              "4   0.613636  0.428030  0.425214  0.555556  0.676282      0.539744  1.0  0.0  \n",
              "5   0.613636  0.602273  0.480769  0.555556  0.676282      0.585703  1.0  0.0  \n",
              "6   0.613636  0.602273  0.557692  0.555556  0.676282      0.601088  1.0  0.0  \n",
              "7   0.638636  0.473485  0.557692  0.555556  0.676282      0.580330  1.0  0.0  \n",
              "8   0.638636  0.473485  0.557692  0.555556  0.676282      0.580330  1.0  0.0  \n",
              "9   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "10  0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "11  0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "12  0.306818  0.215909  0.000000  0.000000  0.358974      0.176340  1.0  0.0  \n",
              "13  0.613636  0.428030  0.425214  0.555556  0.676282      0.539744  1.0  0.0  \n",
              "14  0.613636  0.602273  0.480769  0.555556  0.676282      0.585703  1.0  0.0  \n",
              "15  0.613636  0.602273  0.557692  0.555556  0.676282      0.601088  1.0  0.0  \n",
              "16  0.638636  0.473485  0.557692  0.555556  0.676282      0.580330  1.0  0.0  \n",
              "17  0.638636  0.473485  0.557692  0.555556  0.676282      0.580330  1.0  0.0  \n",
              "18  0.562662  0.404315  0.254674  0.234321  0.566214      0.404437  1.0  0.0  \n",
              "19  0.467208  0.404315  0.254674  0.234321  0.566214      0.385346  1.0  0.0  \n",
              "20  0.417208  0.410565  0.269380  0.234321  0.566214      0.379538  1.0  0.0  \n",
              "21  0.613636  0.602273  0.557692  0.555556  0.626772      0.591186  1.0  0.0  \n",
              "22  0.613636  0.602273  0.480769  0.555556  0.676282      0.585703  1.0  0.0  \n",
              "23  0.613636  0.602273  0.557692  0.555556  0.676282      0.601088  1.0  0.0  \n",
              "24  0.613636  0.602273  0.557692  0.555556  0.676282      0.601088  1.0  0.0  \n",
              "25  0.638636  0.473485  0.557692  0.555556  0.676282      0.580330  1.0  0.0  \n",
              "26  0.638636  0.473485  0.557692  0.555556  0.676282      0.580330  1.0  0.0  \n",
              "\n",
              "[27 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2fec5704-5fdf-4a7b-974e-4f694a979822\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Activation</th>\n",
              "      <th>C</th>\n",
              "      <th>crossVal1</th>\n",
              "      <th>crossVal2</th>\n",
              "      <th>crossVal3</th>\n",
              "      <th>crossVal4</th>\n",
              "      <th>crossVal5</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>general</th>\n",
              "      <th>Par_diff1</th>\n",
              "      <th>...</th>\n",
              "      <th>ep_opp5</th>\n",
              "      <th>ep_opp_avg</th>\n",
              "      <th>avg_odd1</th>\n",
              "      <th>avg_odd2</th>\n",
              "      <th>avg_odd3</th>\n",
              "      <th>avg_odd4</th>\n",
              "      <th>avg_odd5</th>\n",
              "      <th>avg_odds_avg</th>\n",
              "      <th>TPR</th>\n",
              "      <th>FPR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.674279</td>\n",
              "      <td>0.770000</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.153030</td>\n",
              "      <td>0.306818</td>\n",
              "      <td>0.215909</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.358974</td>\n",
              "      <td>0.176340</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.682891</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.448990</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.428030</td>\n",
              "      <td>0.425214</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.539744</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.645390</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.680054</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.507576</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.602273</td>\n",
              "      <td>0.480769</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.585703</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.682891</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.507576</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.602273</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.601088</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.680034</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.499394</td>\n",
              "      <td>0.638636</td>\n",
              "      <td>0.473485</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.580330</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.680034</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.499394</td>\n",
              "      <td>0.638636</td>\n",
              "      <td>0.473485</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.580330</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.685714</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.674279</td>\n",
              "      <td>0.770000</td>\n",
              "      <td>0.290323</td>\n",
              "      <td>...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.153030</td>\n",
              "      <td>0.306818</td>\n",
              "      <td>0.215909</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.358974</td>\n",
              "      <td>0.176340</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.682891</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.448990</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.428030</td>\n",
              "      <td>0.425214</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.539744</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.645390</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.680054</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.507576</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.602273</td>\n",
              "      <td>0.480769</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.585703</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.682891</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.507576</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.602273</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.601088</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.680034</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.499394</td>\n",
              "      <td>0.638636</td>\n",
              "      <td>0.473485</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.580330</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.680034</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.499394</td>\n",
              "      <td>0.638636</td>\n",
              "      <td>0.473485</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.580330</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.635714</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.631206</td>\n",
              "      <td>0.618705</td>\n",
              "      <td>0.614286</td>\n",
              "      <td>0.632839</td>\n",
              "      <td>0.663333</td>\n",
              "      <td>0.567624</td>\n",
              "      <td>...</td>\n",
              "      <td>0.586275</td>\n",
              "      <td>0.407906</td>\n",
              "      <td>0.562662</td>\n",
              "      <td>0.404315</td>\n",
              "      <td>0.254674</td>\n",
              "      <td>0.234321</td>\n",
              "      <td>0.566214</td>\n",
              "      <td>0.404437</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.631206</td>\n",
              "      <td>0.618705</td>\n",
              "      <td>0.614286</td>\n",
              "      <td>0.634268</td>\n",
              "      <td>0.663333</td>\n",
              "      <td>0.470849</td>\n",
              "      <td>...</td>\n",
              "      <td>0.586275</td>\n",
              "      <td>0.387906</td>\n",
              "      <td>0.467208</td>\n",
              "      <td>0.404315</td>\n",
              "      <td>0.254674</td>\n",
              "      <td>0.234321</td>\n",
              "      <td>0.566214</td>\n",
              "      <td>0.385346</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.657143</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.624113</td>\n",
              "      <td>0.618705</td>\n",
              "      <td>0.614286</td>\n",
              "      <td>0.637135</td>\n",
              "      <td>0.663333</td>\n",
              "      <td>0.406333</td>\n",
              "      <td>...</td>\n",
              "      <td>0.586275</td>\n",
              "      <td>0.370406</td>\n",
              "      <td>0.417208</td>\n",
              "      <td>0.410565</td>\n",
              "      <td>0.269380</td>\n",
              "      <td>0.234321</td>\n",
              "      <td>0.566214</td>\n",
              "      <td>0.379538</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.675748</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.584314</td>\n",
              "      <td>0.507772</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.602273</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.626772</td>\n",
              "      <td>0.591186</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.645390</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.680054</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.507576</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.602273</td>\n",
              "      <td>0.480769</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.585703</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.682891</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.507576</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.602273</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.601088</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.682891</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.580645</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.507576</td>\n",
              "      <td>0.613636</td>\n",
              "      <td>0.602273</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.601088</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.680034</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.499394</td>\n",
              "      <td>0.638636</td>\n",
              "      <td>0.473485</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.580330</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.659574</td>\n",
              "      <td>0.683453</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.680034</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.612903</td>\n",
              "      <td>...</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.499394</td>\n",
              "      <td>0.638636</td>\n",
              "      <td>0.473485</td>\n",
              "      <td>0.557692</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.676282</td>\n",
              "      <td>0.580330</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27 rows × 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2fec5704-5fdf-4a7b-974e-4f694a979822')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2fec5704-5fdf-4a7b-974e-4f694a979822 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2fec5704-5fdf-4a7b-974e-4f694a979822');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 Most Fair Model**\n",
        "\n",
        "Using the test data which was split from train before the cross validation."
      ],
      "metadata": {
        "id": "8quPShZiO0kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formula(acc, gen, equ, odds, diff):\n",
        "\n",
        "    return (acc + gen)/ 1 - (equ + odds + diff) #The algirthm to select the model\n",
        "\n",
        "df = task1_df\n",
        "\n",
        "accuacy = df['Accuracy'] #get only the wanted information\n",
        "general = df['general']\n",
        "equility = df['ep_opp_avg']\n",
        "avg_odds = df['avg_odds_avg']\n",
        "par_diff = df['par_diff_avg']\n",
        "\n",
        "\n",
        "C = df['C']             #get the paramters of the model\n",
        "act = df['Activation']\n",
        "\n",
        "\n",
        "answers = 0\n",
        "\n",
        "for i in range(len(accuacy)):\n",
        "    \n",
        "    ans = formula(accuacy[i], general[i], equility[i], avg_odds[i], par_diff[i]) #loop through varibales and find answer\n",
        "    \n",
        "    if ans > answers:             #if it is th best the save the score and variables\n",
        "        answers = ans\n",
        "        bestC = C[i]\n",
        "        bestActivation = act[i]\n",
        "        \n",
        "print(bestC, bestActivation, answers) #print the best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK5oqyr7RhKB",
        "outputId": "386a24f4-8ebd-40d3-fdf7-60dd61c81991"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1e-05 newton-cg 1.4361836245289483\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "solv = 'lbfgs'\n",
        "c = 10\n",
        "\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train)\n",
        "predictions = learner.predict(X_test)\n",
        "\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "anV2l-5qOz4X",
        "outputId": "ecc3cd83-a282-4203-febb-dd9f870f05a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.7466666666666667\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.7466666666666667\n",
            "{'stat_par_diff': 0.4130434782608695, 'eq_opp_diff': 0.36363636363636365, 'avg_odds_diff': 0.45104895104895104, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 Most Accurate Model**"
      ],
      "metadata": {
        "id": "9NtW14kyO5t8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "solv = 'lbfgs'\n",
        "c = 0.00001\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train)\n",
        "predictions = learner.predict(X_test)\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "0-2dO9dgO8rl",
        "outputId": "3c0ef3af-5750-458d-b98c-8403085c8056"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7633333333333333\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.7633333333333333\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3 Select the model (5)**"
      ],
      "metadata": {
        "id": "9rkISCMNq4--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solv = 'newton-cg'\n",
        "c = 0.00001\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train)\n",
        "predictions = learner.predict(X_test)\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "M-S4BbxDq5sA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "9c18fa1e-b149-48a1-c58d-4a3192bc0cc3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7633333333333333\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.7633333333333333\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 Reweighting and applying 5 Fold Cross Validation**"
      ],
      "metadata": {
        "id": "jtRLhAL-KScS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "                privileged_groups=privileged_groups)\n",
        "\n",
        "train_fair = RW.fit_transform(train)\n",
        "trainA_fair = RW.fit_transform(trainA.copy())\n",
        "trainB_fair = RW.fit_transform(trainB.copy())\n",
        "trainC_fair = RW.fit_transform(trainC.copy())\n",
        "trainD_fair = RW.fit_transform(trainD.copy())\n",
        "trainE_fair = RW.fit_transform(trainE.copy())\n",
        "\n",
        "train_weights = [trainA_fair.instance_weights, trainB_fair.instance_weights, trainC_fair.instance_weights, trainD_fair.instance_weights, trainE_fair.instance_weights]\n",
        "\n",
        "print(\"subgroup weights\", np.unique(trainA_fair.instance_weights), np.unique(trainB_fair.instance_weights), np.unique(trainC_fair.instance_weights), np.unique(trainD_fair.instance_weights), np.unique(trainE_fair.instance_weights))\n",
        "train_cross_fair = [trainA_fair, trainB_fair, trainC_fair, trainD_fair, trainE_fair]\n",
        "\n",
        "X_train_cross_fair = train_cross_fair\n",
        "Y_train_cross_fair = [0] * 5\n",
        "\n",
        "scale_orig = StandardScaler()   #setup the scaler object\n",
        "\n",
        "for i in range(len(train_cross_fair)):\n",
        "    Y_train_cross_fair[i] = train_cross_fair[i].labels.ravel()\n",
        "\n",
        "for i in range(len(train_cross)):\n",
        "    X_train_cross_fair[i] = scale_orig.fit_transform(train_cross_fair[i].features) #scale both features and labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0FlW-WvKYqB",
        "outputId": "a4ce3652-31dc-4474-eb29-cd7474a47bc6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subgroup weights [0.86558442 0.98079777 1.04620536 1.07392857] [0.67083333 0.950625   1.10675676 1.35909091] [0.79487179 0.96491228 1.07843137 1.14814815] [0.63868905 0.91442635 1.22426197 1.40647482] [0.59065934 0.93739496 1.17738095 1.44345238]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Cs = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "scores = []\n",
        "\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "\n",
        "with open('task2crossval2Results.csv', 'w') as csvfile:\n",
        "    field_names = ['Activation', 'C' , 'crossVal1', 'crossVal2', 'crossVal3', 'crossVal4', 'crossVal5','Accuracy', 'general', \n",
        "                   'Par_diff1', 'Par_diff2', 'Par_diff3', 'Par_diff4', 'Par_diff5', 'par_diff_avg',\n",
        "                   'ep_opp1', 'ep_opp2','ep_opp3', 'ep_opp4', 'ep_opp5', 'ep_opp_avg',\n",
        "                   'avg_odd1', 'avg_odd2', 'avg_odd3', 'avg_odd4', 'avg_odd5','avg_odds_avg' ,'TPR', 'FPR'] #Name of the columns for the .csv file\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
        "    writer.writeheader()\n",
        "\n",
        "\n",
        "\n",
        "    for solv in solvers:\n",
        "\n",
        "          for c in Cs:\n",
        "\n",
        "              data = {}\n",
        "              accuracies = []\n",
        "              cs = []\n",
        "              activations = []\n",
        "              par_diff = []\n",
        "              ep_opp = []\n",
        "              avg_odds = []\n",
        "              TPR = []\n",
        "              FPR = []\n",
        "\n",
        "              classifier = LogisticRegression(C=c, solver=solv)\n",
        "\n",
        "              for i in range(len(train_cross)):\n",
        "\n",
        "                  X_test_cross_fair = []\n",
        "                  Y_test_cross_fair = []\n",
        "                  indexs = []\n",
        "                  ind = []\n",
        "                  X = []\n",
        "                  Y = []\n",
        "\n",
        "                  for j in range(len(train_cross)):\n",
        "                      if i == j:\n",
        "                          X_test_cross_fair = X_train_cross_fair[j]\n",
        "                          Y_test_cross_fair = Y_train_cross_fair[j]\n",
        "                          ind.append(j)\n",
        "                      else:\n",
        "                          indexs.append(j)\n",
        "\n",
        "\n",
        "                  X = np.concatenate([X_train_cross_fair[indexs[0]], X_train_cross_fair[indexs[1]], X_train_cross_fair[indexs[2]]])\n",
        "                  Y = np.concatenate([Y_train_cross_fair[indexs[0]], Y_train_cross_fair[indexs[1]], Y_train_cross_fair[indexs[2]]])\n",
        "                  X = np.concatenate([X, X_train_cross_fair[indexs[3]]])\n",
        "                  Y = np.concatenate([Y, Y_train_cross_fair[indexs[3]]])\n",
        "                  bias_weights = np.concatenate([train_weights[indexs[0]], train_weights[indexs[1]], train_weights[indexs[2]]])\n",
        "                  bias_weights = np.concatenate([bias_weights, train_weights[indexs[3]]])\n",
        "\n",
        "\n",
        "                  #LR = classifier.fit(X, Y, sample_weight=train_cross_fair_weight.instance_weights) #,sample_weight=train.instance_weights fit all the data that is not in the test\n",
        "\n",
        "                  LR = classifier.fit(X, Y, sample_weight=bias_weights)\n",
        "\n",
        "\n",
        "\n",
        "                  predictions = LR.predict(X_test_cross_fair)\n",
        "                  \n",
        "                  acc = find_score(predictions, Y_test_cross_fair)\n",
        "\n",
        "                  #train_copy = trainA\n",
        "\n",
        "\n",
        "                  if i == 0:\n",
        "                    train_copy = trainA_fair\n",
        "\n",
        "                  elif i == 1:\n",
        "                    train_copy = trainB_fair\n",
        "                  \n",
        "                  elif i == 2:\n",
        "                    train_copy = trainC_fair\n",
        "\n",
        "                  elif i == 3:\n",
        "                    train_copy = trainD_fair\n",
        "                  \n",
        "                  elif i == 4:\n",
        "                    train_copy = trainE_fair\n",
        "\n",
        "                  else:\n",
        "                    print(\"Error\")\n",
        "\n",
        "                  test_pred = train_copy.copy()\n",
        "                  predictions.resize((len(predictions),1))\n",
        "                  test_pred.labels = predictions\n",
        "\n",
        "                  metric = ClassificationMetric(train_copy, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "                  metric_arrs = {}\n",
        "                  metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "                  metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "                  metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "                  #metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "                  #metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "                  metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "                  metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "                  print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, acc))\n",
        "                  print(metric_arrs)\n",
        "                  print('\\n')\n",
        "\n",
        "\n",
        "                  accuracies.append(acc)\n",
        "                  cs.append(c)\n",
        "                  activations.append(solv)\n",
        "                  par_diff.append((metric.statistical_parity_difference()))\n",
        "                  ep_opp.append((metric.equal_opportunity_difference()))\n",
        "                  avg_odds.append((metric.average_odds_difference()))\n",
        "                  TPR.append((metric.generalized_true_positive_rate()))\n",
        "                  FPR.append((metric.num_generalized_false_positives()))\n",
        "\n",
        "\n",
        "              print(\"Accuracies: \", accuracies)\n",
        "              print(\"Cs\", c)\n",
        "              print(\"Activation: \", activations)\n",
        "              print(\"Par_diff: \", par_diff)\n",
        "              print(\"ep_opp: \", ep_opp)\n",
        "              print(\"avg_odd: \", avg_odds)\n",
        "              print(\"TPR: \", TPR)\n",
        "              print(\"FPR: \", FPR)\n",
        "\n",
        "              pred_test = LR.predict(X_test)\n",
        "              test_acc = find_score(pred_test, y_test)\n",
        "              print(\"Generalisability --- \", test_acc)\n",
        "\n",
        "              acc = sum(accuracies)/len(accuracies)\n",
        "              par_diff_avg = sum(par_diff)/len(par_diff)\n",
        "              ep_opp_avg = sum(ep_opp)/len(ep_opp)\n",
        "              avg_odds_avg = sum(avg_odds)/len(avg_odds)\n",
        "\n",
        "\n",
        "              writer.writerow({'Activation': solv, 'C': c , 'crossVal1': accuracies[0], 'crossVal2': accuracies[1], 'crossVal3': accuracies[2], 'crossVal4': accuracies[3], 'crossVal5': accuracies[4],'Accuracy': acc, 'general': test_acc, \n",
        "                               'Par_diff1': par_diff[0], 'Par_diff2': par_diff[1], 'Par_diff3': par_diff[2], 'Par_diff4': par_diff[3], 'Par_diff5': par_diff[4], 'par_diff_avg': par_diff_avg,\n",
        "                   'ep_opp1': ep_opp[0], 'ep_opp2': ep_opp[1],'ep_opp3': ep_opp[2], 'ep_opp4': ep_opp[3], 'ep_opp5': ep_opp[4], 'ep_opp_avg': ep_opp_avg,\n",
        "                   'avg_odd1': avg_odds[0], 'avg_odd2': avg_odds[1], 'avg_odd3': avg_odds[2], 'avg_odd4': avg_odds[3], 'avg_odd5': avg_odds[4],'avg_odds_avg': avg_odds_avg , 'TPR': TPR[0], 'FPR': FPR[0]}) #save the data into .csv\n",
        "\n",
        "          \n",
        "files.download('task2crossval2Results.csv')   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7KQf3teNKY5p",
        "outputId": "8aaa07a9-ea6a-4235-f9a5-f2a5bee74ec4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.6928571428571428\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 1e-05\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 0.0001\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 0.001\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.01 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 0.01 | Activation: newton-cg | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 0.01 | Activation: newton-cg | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 0.01 | Activation: newton-cg | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.01 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 0.01\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.1 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 0.1 | Activation: newton-cg | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 0.1 | Activation: newton-cg | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 0.1 | Activation: newton-cg | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 0.1 | Activation: newton-cg | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 0.1\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 1 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 1 | Activation: newton-cg | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 1 | Activation: newton-cg | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 1 | Activation: newton-cg | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 1 | Activation: newton-cg | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 1\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 10\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 100 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 100 | Activation: newton-cg | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6524822695035462\n",
            "C : 100 | Activation: newton-cg | Accuracy: 0.6524822695035462\n",
            "{'stat_par_diff': -0.09184726522187814, 'eq_opp_diff': -0.07894736842105254, 'avg_odds_diff': -0.09829721362229099, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 100 | Activation: newton-cg | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 100 | Activation: newton-cg | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6524822695035462, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 100\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, -0.09184726522187814, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, -0.07894736842105254, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, -0.09829721362229099, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 1000 | Activation: newton-cg | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 1000 | Activation: newton-cg | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6524822695035462\n",
            "C : 1000 | Activation: newton-cg | Accuracy: 0.6524822695035462\n",
            "{'stat_par_diff': -0.09184726522187814, 'eq_opp_diff': -0.07894736842105254, 'avg_odds_diff': -0.09829721362229099, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 1000 | Activation: newton-cg | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 1000 | Activation: newton-cg | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6524822695035462, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 1000\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, -0.09184726522187814, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, -0.07894736842105254, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, -0.09829721362229099, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 1e-05\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.0001 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 0.0001 | Activation: lbfgs | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 0.0001 | Activation: lbfgs | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 0.0001 | Activation: lbfgs | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.0001 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 0.0001\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.001 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 0.001 | Activation: lbfgs | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 0.001 | Activation: lbfgs | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 0.001 | Activation: lbfgs | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.001 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 0.001\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.01 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 0.01 | Activation: lbfgs | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 0.01 | Activation: lbfgs | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 0.01 | Activation: lbfgs | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.01 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6928571428571428]\n",
            "Cs 0.01\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7633333333333333\n",
            "Generalisability ---  0.7633333333333333\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 0.1\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7466666666666667\n",
            "Generalisability ---  0.7466666666666667\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 1 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 1 | Activation: lbfgs | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 1 | Activation: lbfgs | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 1 | Activation: lbfgs | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 1 | Activation: lbfgs | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 1\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 10\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 100 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 100 | Activation: lbfgs | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6524822695035462\n",
            "C : 100 | Activation: lbfgs | Accuracy: 0.6524822695035462\n",
            "{'stat_par_diff': -0.09184726522187814, 'eq_opp_diff': -0.07894736842105254, 'avg_odds_diff': -0.09829721362229099, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 100 | Activation: lbfgs | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 100 | Activation: lbfgs | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6524822695035462, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 100\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, -0.09184726522187814, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, -0.07894736842105254, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, -0.09829721362229099, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 1000 | Activation: lbfgs | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 1000 | Activation: lbfgs | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6524822695035462\n",
            "C : 1000 | Activation: lbfgs | Accuracy: 0.6524822695035462\n",
            "{'stat_par_diff': -0.09184726522187814, 'eq_opp_diff': -0.07894736842105254, 'avg_odds_diff': -0.09829721362229099, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 1000 | Activation: lbfgs | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 1000 | Activation: lbfgs | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6524822695035462, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 1000\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, -0.09184726522187814, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, -0.07894736842105254, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, -0.09829721362229099, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6785714285714286\n",
            "C : 1e-05 | Activation: liblinear | Accuracy: 0.6785714285714286\n",
            "{'stat_par_diff': 0.2275463821892394, 'eq_opp_diff': 0.15714285714285725, 'avg_odds_diff': 0.2717532467532468, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6714285714285714\n",
            "C : 1e-05 | Activation: liblinear | Accuracy: 0.6714285714285714\n",
            "{'stat_par_diff': 0.074043304668305, 'eq_opp_diff': 0.0193181818181819, 'avg_odds_diff': 0.09749692874692867, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6312056737588653\n",
            "C : 1e-05 | Activation: liblinear | Accuracy: 0.6312056737588653\n",
            "{'stat_par_diff': 0.1365316256957123, 'eq_opp_diff': 0.16520467836257302, 'avg_odds_diff': 0.12219509936228201, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6187050359712231\n",
            "C : 1e-05 | Activation: liblinear | Accuracy: 0.6187050359712231\n",
            "{'stat_par_diff': 0.23513747985277855, 'eq_opp_diff': 0.23684210526315774, 'avg_odds_diff': 0.23432143577334133, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6357142857142857\n",
            "C : 1e-05 | Activation: liblinear | Accuracy: 0.6357142857142857\n",
            "{'stat_par_diff': 0.3668729799612153, 'eq_opp_diff': 0.33627450980392193, 'avg_odds_diff': 0.3860859728506789, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6785714285714286, 0.6714285714285714, 0.6312056737588653, 0.6187050359712231, 0.6357142857142857]\n",
            "Cs 1e-05\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.2275463821892394, 0.074043304668305, 0.1365316256957123, 0.23513747985277855, 0.3668729799612153]\n",
            "ep_opp:  [0.15714285714285725, 0.0193181818181819, 0.16520467836257302, 0.23684210526315774, 0.33627450980392193]\n",
            "avg_odd:  [0.2717532467532468, 0.09749692874692867, 0.12219509936228201, 0.23432143577334133, 0.3860859728506789]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.6766666666666666\n",
            "Generalisability ---  0.6766666666666666\n",
            "Model Accuracy 0.6785714285714286\n",
            "C : 0.0001 | Activation: liblinear | Accuracy: 0.6785714285714286\n",
            "{'stat_par_diff': 0.2275463821892394, 'eq_opp_diff': 0.15714285714285725, 'avg_odds_diff': 0.2717532467532468, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6714285714285714\n",
            "C : 0.0001 | Activation: liblinear | Accuracy: 0.6714285714285714\n",
            "{'stat_par_diff': 0.074043304668305, 'eq_opp_diff': 0.0193181818181819, 'avg_odds_diff': 0.09749692874692867, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6312056737588653\n",
            "C : 0.0001 | Activation: liblinear | Accuracy: 0.6312056737588653\n",
            "{'stat_par_diff': 0.1365316256957123, 'eq_opp_diff': 0.16520467836257302, 'avg_odds_diff': 0.12219509936228201, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6187050359712231\n",
            "C : 0.0001 | Activation: liblinear | Accuracy: 0.6187050359712231\n",
            "{'stat_par_diff': 0.23513747985277855, 'eq_opp_diff': 0.23684210526315774, 'avg_odds_diff': 0.23432143577334133, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6357142857142857\n",
            "C : 0.0001 | Activation: liblinear | Accuracy: 0.6357142857142857\n",
            "{'stat_par_diff': 0.3668729799612153, 'eq_opp_diff': 0.33627450980392193, 'avg_odds_diff': 0.3860859728506789, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6785714285714286, 0.6714285714285714, 0.6312056737588653, 0.6187050359712231, 0.6357142857142857]\n",
            "Cs 0.0001\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.2275463821892394, 0.074043304668305, 0.1365316256957123, 0.23513747985277855, 0.3668729799612153]\n",
            "ep_opp:  [0.15714285714285725, 0.0193181818181819, 0.16520467836257302, 0.23684210526315774, 0.33627450980392193]\n",
            "avg_odd:  [0.2717532467532468, 0.09749692874692867, 0.12219509936228201, 0.23432143577334133, 0.3860859728506789]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.6766666666666666\n",
            "Generalisability ---  0.6766666666666666\n",
            "Model Accuracy 0.6785714285714286\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.6785714285714286\n",
            "{'stat_par_diff': 0.2275463821892394, 'eq_opp_diff': 0.15714285714285725, 'avg_odds_diff': 0.2717532467532468, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6785714285714286\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.6785714285714286\n",
            "{'stat_par_diff': 0.08216830466830505, 'eq_opp_diff': 0.03181818181818197, 'avg_odds_diff': 0.1037469287469287, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6312056737588653\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.6312056737588653\n",
            "{'stat_par_diff': 0.1365316256957123, 'eq_opp_diff': 0.16520467836257302, 'avg_odds_diff': 0.12219509936228201, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6187050359712231\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.6187050359712231\n",
            "{'stat_par_diff': 0.23513747985277855, 'eq_opp_diff': 0.23684210526315774, 'avg_odds_diff': 0.23432143577334133, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6357142857142857\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.6357142857142857\n",
            "{'stat_par_diff': 0.3668729799612153, 'eq_opp_diff': 0.33627450980392193, 'avg_odds_diff': 0.3860859728506789, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6785714285714286, 0.6785714285714286, 0.6312056737588653, 0.6187050359712231, 0.6357142857142857]\n",
            "Cs 0.001\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.2275463821892394, 0.08216830466830505, 0.1365316256957123, 0.23513747985277855, 0.3668729799612153]\n",
            "ep_opp:  [0.15714285714285725, 0.03181818181818197, 0.16520467836257302, 0.23684210526315774, 0.33627450980392193]\n",
            "avg_odd:  [0.2717532467532468, 0.1037469287469287, 0.12219509936228201, 0.23432143577334133, 0.3860859728506789]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.6766666666666666\n",
            "Generalisability ---  0.6766666666666666\n",
            "Model Accuracy 0.6714285714285714\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.6714285714285714\n",
            "{'stat_par_diff': -0.15716894712430418, 'eq_opp_diff': -0.1298701298701298, 'avg_odds_diff': -0.1743100649350649, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.65\n",
            "{'stat_par_diff': -0.035168918918919156, 'eq_opp_diff': -0.02499999999999991, 'avg_odds_diff': -0.03952702702702704, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6312056737588653\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.6312056737588653\n",
            "{'stat_par_diff': -0.26676986584107343, 'eq_opp_diff': -0.22368421052631582, 'avg_odds_diff': -0.28831269349845207, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.5971223021582733\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.5971223021582733\n",
            "{'stat_par_diff': -0.3024324641919858, 'eq_opp_diff': -0.26315789473684226, 'avg_odds_diff': -0.3212341197822142, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6071428571428571\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.6071428571428571\n",
            "{'stat_par_diff': -0.26331932773109246, 'eq_opp_diff': -0.24705882352941144, 'avg_odds_diff': -0.27352941176470563, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6714285714285714, 0.65, 0.6312056737588653, 0.5971223021582733, 0.6071428571428571]\n",
            "Cs 0.01\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.15716894712430418, -0.035168918918919156, -0.26676986584107343, -0.3024324641919858, -0.26331932773109246]\n",
            "ep_opp:  [-0.1298701298701298, -0.02499999999999991, -0.22368421052631582, -0.26315789473684226, -0.24705882352941144]\n",
            "avg_odd:  [-0.1743100649350649, -0.03952702702702704, -0.28831269349845207, -0.3212341197822142, -0.27352941176470563]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7033333333333334\n",
            "Generalisability ---  0.7033333333333334\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 0.1 | Activation: liblinear | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 0.1 | Activation: liblinear | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 0.1 | Activation: liblinear | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 0.1 | Activation: liblinear | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 0.1 | Activation: liblinear | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 0.1\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 1 | Activation: liblinear | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 1 | Activation: liblinear | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 1 | Activation: liblinear | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 1 | Activation: liblinear | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 1 | Activation: liblinear | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 1\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 10 | Activation: liblinear | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 10 | Activation: liblinear | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6666666666666666\n",
            "C : 10 | Activation: liblinear | Accuracy: 0.6666666666666666\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 10 | Activation: liblinear | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 10 | Activation: liblinear | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6666666666666666, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 10\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 100 | Activation: liblinear | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 100 | Activation: liblinear | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6524822695035462\n",
            "C : 100 | Activation: liblinear | Accuracy: 0.6524822695035462\n",
            "{'stat_par_diff': -0.09184726522187814, 'eq_opp_diff': -0.07894736842105254, 'avg_odds_diff': -0.09829721362229099, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 100 | Activation: liblinear | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 100 | Activation: liblinear | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6524822695035462, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 100\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.0, 0.0, -0.09184726522187814, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, -0.07894736842105254, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, -0.09829721362229099, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n",
            "Model Accuracy 0.6928571428571428\n",
            "C : 1000 | Activation: liblinear | Accuracy: 0.6928571428571428\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.65\n",
            "C : 1000 | Activation: liblinear | Accuracy: 0.65\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6524822695035462\n",
            "C : 1000 | Activation: liblinear | Accuracy: 0.6524822695035462\n",
            "{'stat_par_diff': -0.09184726522187814, 'eq_opp_diff': -0.07894736842105254, 'avg_odds_diff': -0.09829721362229099, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6618705035971223\n",
            "C : 1000 | Activation: liblinear | Accuracy: 0.6618705035971223\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.6642857142857143\n",
            "C : 1000 | Activation: liblinear | Accuracy: 0.6642857142857143\n",
            "{'stat_par_diff': -0.08777310924369741, 'eq_opp_diff': -0.08235294117647063, 'avg_odds_diff': -0.09117647058823525, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.6928571428571428, 0.65, 0.6524822695035462, 0.6618705035971223, 0.6642857142857143]\n",
            "Cs 1000\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [0.0, 0.0, -0.09184726522187814, 0.0, -0.08777310924369741]\n",
            "ep_opp:  [0.0, 0.0, -0.07894736842105254, 0.0, -0.08235294117647063]\n",
            "avg_odd:  [0.0, 0.0, -0.09829721362229099, 0.0, -0.09117647058823525]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.74\n",
            "Generalisability ---  0.74\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_b9f3b74c-f1e2-4955-bc15-cf0769015d22\", \"task2crossval2Results.csv\", 8873)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting**\n",
        "\n",
        "Plotting the hyper parameters"
      ],
      "metadata": {
        "id": "BBvCtsUpjJ7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task2_df = pd.read_csv(\"task2crossval2Results.csv\")\n",
        "task2_df"
      ],
      "metadata": {
        "id": "GJmeXMpjjKso",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "outputId": "28036c98-212f-4bc8-acf8-7a782323fcc1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Activation           C  crossVal1  crossVal2  crossVal3  crossVal4  \\\n",
              "0   newton-cg     0.00001   0.692857   0.650000   0.666667   0.661871   \n",
              "1   newton-cg     0.00010   0.692857   0.650000   0.666667   0.661871   \n",
              "2   newton-cg     0.00100   0.692857   0.650000   0.666667   0.661871   \n",
              "3   newton-cg     0.01000   0.692857   0.650000   0.666667   0.661871   \n",
              "4   newton-cg     0.10000   0.692857   0.650000   0.666667   0.661871   \n",
              "5   newton-cg     1.00000   0.692857   0.650000   0.666667   0.661871   \n",
              "6   newton-cg    10.00000   0.692857   0.650000   0.666667   0.661871   \n",
              "7   newton-cg   100.00000   0.692857   0.650000   0.652482   0.661871   \n",
              "8   newton-cg  1000.00000   0.692857   0.650000   0.652482   0.661871   \n",
              "9       lbfgs     0.00001   0.692857   0.650000   0.666667   0.661871   \n",
              "10      lbfgs     0.00010   0.692857   0.650000   0.666667   0.661871   \n",
              "11      lbfgs     0.00100   0.692857   0.650000   0.666667   0.661871   \n",
              "12      lbfgs     0.01000   0.692857   0.650000   0.666667   0.661871   \n",
              "13      lbfgs     0.10000   0.692857   0.650000   0.666667   0.661871   \n",
              "14      lbfgs     1.00000   0.692857   0.650000   0.666667   0.661871   \n",
              "15      lbfgs    10.00000   0.692857   0.650000   0.666667   0.661871   \n",
              "16      lbfgs   100.00000   0.692857   0.650000   0.652482   0.661871   \n",
              "17      lbfgs  1000.00000   0.692857   0.650000   0.652482   0.661871   \n",
              "18  liblinear     0.00001   0.678571   0.671429   0.631206   0.618705   \n",
              "19  liblinear     0.00010   0.678571   0.671429   0.631206   0.618705   \n",
              "20  liblinear     0.00100   0.678571   0.678571   0.631206   0.618705   \n",
              "21  liblinear     0.01000   0.671429   0.650000   0.631206   0.597122   \n",
              "22  liblinear     0.10000   0.692857   0.650000   0.666667   0.661871   \n",
              "23  liblinear     1.00000   0.692857   0.650000   0.666667   0.661871   \n",
              "24  liblinear    10.00000   0.692857   0.650000   0.666667   0.661871   \n",
              "25  liblinear   100.00000   0.692857   0.650000   0.652482   0.661871   \n",
              "26  liblinear  1000.00000   0.692857   0.650000   0.652482   0.661871   \n",
              "\n",
              "    crossVal5  Accuracy   general  Par_diff1  ...   ep_opp5  ep_opp_avg  \\\n",
              "0    0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "1    0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "2    0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "3    0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "4    0.664286  0.667136  0.746667   0.000000  ... -0.082353   -0.016471   \n",
              "5    0.664286  0.667136  0.740000   0.000000  ... -0.082353   -0.016471   \n",
              "6    0.664286  0.667136  0.740000   0.000000  ... -0.082353   -0.016471   \n",
              "7    0.664286  0.664299  0.740000   0.000000  ... -0.082353   -0.032260   \n",
              "8    0.664286  0.664299  0.740000   0.000000  ... -0.082353   -0.032260   \n",
              "9    0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "10   0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "11   0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "12   0.692857  0.672850  0.763333   0.000000  ...  0.000000    0.000000   \n",
              "13   0.664286  0.667136  0.746667   0.000000  ... -0.082353   -0.016471   \n",
              "14   0.664286  0.667136  0.740000   0.000000  ... -0.082353   -0.016471   \n",
              "15   0.664286  0.667136  0.740000   0.000000  ... -0.082353   -0.016471   \n",
              "16   0.664286  0.664299  0.740000   0.000000  ... -0.082353   -0.032260   \n",
              "17   0.664286  0.664299  0.740000   0.000000  ... -0.082353   -0.032260   \n",
              "18   0.635714  0.647125  0.676667   0.227546  ...  0.336275    0.182956   \n",
              "19   0.635714  0.647125  0.676667   0.227546  ...  0.336275    0.182956   \n",
              "20   0.635714  0.648554  0.676667   0.227546  ...  0.336275    0.185456   \n",
              "21   0.607143  0.631380  0.703333  -0.157169  ... -0.247059   -0.177754   \n",
              "22   0.664286  0.667136  0.740000   0.000000  ... -0.082353   -0.016471   \n",
              "23   0.664286  0.667136  0.740000   0.000000  ... -0.082353   -0.016471   \n",
              "24   0.664286  0.667136  0.740000   0.000000  ... -0.082353   -0.016471   \n",
              "25   0.664286  0.664299  0.740000   0.000000  ... -0.082353   -0.032260   \n",
              "26   0.664286  0.664299  0.740000   0.000000  ... -0.082353   -0.032260   \n",
              "\n",
              "    avg_odd1  avg_odd2  avg_odd3  avg_odd4  avg_odd5  avg_odds_avg  TPR  FPR  \n",
              "0   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "1   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "2   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "3   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "4   0.000000  0.000000  0.000000  0.000000 -0.091176     -0.018235  1.0  0.0  \n",
              "5   0.000000  0.000000  0.000000  0.000000 -0.091176     -0.018235  1.0  0.0  \n",
              "6   0.000000  0.000000  0.000000  0.000000 -0.091176     -0.018235  1.0  0.0  \n",
              "7   0.000000  0.000000 -0.098297  0.000000 -0.091176     -0.037895  1.0  0.0  \n",
              "8   0.000000  0.000000 -0.098297  0.000000 -0.091176     -0.037895  1.0  0.0  \n",
              "9   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "10  0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "11  0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "12  0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "13  0.000000  0.000000  0.000000  0.000000 -0.091176     -0.018235  1.0  0.0  \n",
              "14  0.000000  0.000000  0.000000  0.000000 -0.091176     -0.018235  1.0  0.0  \n",
              "15  0.000000  0.000000  0.000000  0.000000 -0.091176     -0.018235  1.0  0.0  \n",
              "16  0.000000  0.000000 -0.098297  0.000000 -0.091176     -0.037895  1.0  0.0  \n",
              "17  0.000000  0.000000 -0.098297  0.000000 -0.091176     -0.037895  1.0  0.0  \n",
              "18  0.271753  0.097497  0.122195  0.234321  0.386086      0.222371  1.0  0.0  \n",
              "19  0.271753  0.097497  0.122195  0.234321  0.386086      0.222371  1.0  0.0  \n",
              "20  0.271753  0.103747  0.122195  0.234321  0.386086      0.223621  1.0  0.0  \n",
              "21 -0.174310 -0.039527 -0.288313 -0.321234 -0.273529     -0.219383  1.0  0.0  \n",
              "22  0.000000  0.000000  0.000000  0.000000 -0.091176     -0.018235  1.0  0.0  \n",
              "23  0.000000  0.000000  0.000000  0.000000 -0.091176     -0.018235  1.0  0.0  \n",
              "24  0.000000  0.000000  0.000000  0.000000 -0.091176     -0.018235  1.0  0.0  \n",
              "25  0.000000  0.000000 -0.098297  0.000000 -0.091176     -0.037895  1.0  0.0  \n",
              "26  0.000000  0.000000 -0.098297  0.000000 -0.091176     -0.037895  1.0  0.0  \n",
              "\n",
              "[27 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-db0ae0da-111f-4f2e-b4c7-6b8b0906ba76\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Activation</th>\n",
              "      <th>C</th>\n",
              "      <th>crossVal1</th>\n",
              "      <th>crossVal2</th>\n",
              "      <th>crossVal3</th>\n",
              "      <th>crossVal4</th>\n",
              "      <th>crossVal5</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>general</th>\n",
              "      <th>Par_diff1</th>\n",
              "      <th>...</th>\n",
              "      <th>ep_opp5</th>\n",
              "      <th>ep_opp_avg</th>\n",
              "      <th>avg_odd1</th>\n",
              "      <th>avg_odd2</th>\n",
              "      <th>avg_odd3</th>\n",
              "      <th>avg_odd4</th>\n",
              "      <th>avg_odd5</th>\n",
              "      <th>avg_odds_avg</th>\n",
              "      <th>TPR</th>\n",
              "      <th>FPR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.667136</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.016471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.018235</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.667136</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.016471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.018235</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.667136</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.016471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.018235</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.652482</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.664299</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.032260</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.098297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.037895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.652482</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.664299</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.032260</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.098297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.037895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.672850</td>\n",
              "      <td>0.763333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.667136</td>\n",
              "      <td>0.746667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.016471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.018235</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.667136</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.016471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.018235</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.667136</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.016471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.018235</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.652482</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.664299</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.032260</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.098297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.037895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.652482</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.664299</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.032260</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.098297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.037895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.631206</td>\n",
              "      <td>0.618705</td>\n",
              "      <td>0.635714</td>\n",
              "      <td>0.647125</td>\n",
              "      <td>0.676667</td>\n",
              "      <td>0.227546</td>\n",
              "      <td>...</td>\n",
              "      <td>0.336275</td>\n",
              "      <td>0.182956</td>\n",
              "      <td>0.271753</td>\n",
              "      <td>0.097497</td>\n",
              "      <td>0.122195</td>\n",
              "      <td>0.234321</td>\n",
              "      <td>0.386086</td>\n",
              "      <td>0.222371</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.631206</td>\n",
              "      <td>0.618705</td>\n",
              "      <td>0.635714</td>\n",
              "      <td>0.647125</td>\n",
              "      <td>0.676667</td>\n",
              "      <td>0.227546</td>\n",
              "      <td>...</td>\n",
              "      <td>0.336275</td>\n",
              "      <td>0.182956</td>\n",
              "      <td>0.271753</td>\n",
              "      <td>0.097497</td>\n",
              "      <td>0.122195</td>\n",
              "      <td>0.234321</td>\n",
              "      <td>0.386086</td>\n",
              "      <td>0.222371</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.678571</td>\n",
              "      <td>0.631206</td>\n",
              "      <td>0.618705</td>\n",
              "      <td>0.635714</td>\n",
              "      <td>0.648554</td>\n",
              "      <td>0.676667</td>\n",
              "      <td>0.227546</td>\n",
              "      <td>...</td>\n",
              "      <td>0.336275</td>\n",
              "      <td>0.185456</td>\n",
              "      <td>0.271753</td>\n",
              "      <td>0.103747</td>\n",
              "      <td>0.122195</td>\n",
              "      <td>0.234321</td>\n",
              "      <td>0.386086</td>\n",
              "      <td>0.223621</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.671429</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.631206</td>\n",
              "      <td>0.597122</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.631380</td>\n",
              "      <td>0.703333</td>\n",
              "      <td>-0.157169</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.247059</td>\n",
              "      <td>-0.177754</td>\n",
              "      <td>-0.174310</td>\n",
              "      <td>-0.039527</td>\n",
              "      <td>-0.288313</td>\n",
              "      <td>-0.321234</td>\n",
              "      <td>-0.273529</td>\n",
              "      <td>-0.219383</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.667136</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.016471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.018235</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.667136</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.016471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.018235</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.667136</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.016471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.018235</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.652482</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.664299</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.032260</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.098297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.037895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.692857</td>\n",
              "      <td>0.650000</td>\n",
              "      <td>0.652482</td>\n",
              "      <td>0.661871</td>\n",
              "      <td>0.664286</td>\n",
              "      <td>0.664299</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.082353</td>\n",
              "      <td>-0.032260</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.098297</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.091176</td>\n",
              "      <td>-0.037895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27 rows × 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db0ae0da-111f-4f2e-b4c7-6b8b0906ba76')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-db0ae0da-111f-4f2e-b4c7-6b8b0906ba76 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-db0ae0da-111f-4f2e-b4c7-6b8b0906ba76');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def formula(acc, gen, equ, odds, diff):\n",
        "\n",
        "    return (acc + gen)/ 1 - (equ + odds + diff) #The algirthm to select the model\n",
        "\n",
        "df = task2_df\n",
        "\n",
        "accuacy = df['Accuracy'] #get only the wanted information\n",
        "general = df['general']\n",
        "equility = df['ep_opp_avg']\n",
        "avg_odds = df['avg_odds_avg']\n",
        "par_diff = df['par_diff_avg']\n",
        "\n",
        "\n",
        "C = df['C']             #get the paramters of the model\n",
        "act = df['Activation']\n",
        "\n",
        "\n",
        "answers = 0\n",
        "\n",
        "for i in range(len(accuacy)):\n",
        "    \n",
        "    ans = formula(accuacy[i], general[i], equility[i], avg_odds[i], par_diff[i]) #loop through varibales and find answer\n",
        "    \n",
        "    if ans > answers:             #if it is th best the save the score and variables\n",
        "        answers = ans\n",
        "        bestC = C[i]\n",
        "        bestActivation = act[i]\n",
        "        \n",
        "print(bestC, bestActivation, answers) #print the best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uz86A2LQ1LR",
        "outputId": "33d32ded-0e58-49ec-d75c-64a96a521772"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01 liblinear 1.9368219941265545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 Reweighted Accuracy Model**"
      ],
      "metadata": {
        "id": "NfYqN-DEW-aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solv = 'newton-cg'\n",
        "c = 10\n",
        "\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train ,sample_weight=train_fair.instance_weights)\n",
        "predictions = learner.predict(X_test)\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, acc))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "YvVqrX0wXEZ-",
        "outputId": "8af175da-783d-4f6b-c76a-113ea420dfb2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.7633333333333333\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.6642991260487051\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 Reweighted Fairness Model**"
      ],
      "metadata": {
        "id": "rqcIIY1HXE4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solv = 'newton-cg'\n",
        "c = 0.0001\n",
        "\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train ,sample_weight=train_fair.instance_weights)\n",
        "predictions = learner.predict(X_test)\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, acc))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "m_IUlQnwXRAv",
        "outputId": "40fb61a7-1f77-4c3d-cd58-8d829197d95f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.7633333333333333\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.6642991260487051\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3 (EXTRA) Fairness Model 6**"
      ],
      "metadata": {
        "id": "CqT08aPhKZjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solv = 'liblinear'\n",
        "c = 0.001\n",
        "\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train ,sample_weight=train_fair.instance_weights)\n",
        "predictions = learner.predict(X_test)\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, acc))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "My9oOup7EZmQ",
        "outputId": "98a88ec2-dad8-4957-e4ba-32e57a9da694"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.6933333333333334\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.6642991260487051\n",
            "{'stat_par_diff': -0.1432728517630948, 'eq_opp_diff': -0.12925170068027214, 'avg_odds_diff': -0.13558075750193976, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Future work **"
      ],
      "metadata": {
        "id": "bpbPV0NREZ9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "privileged_groups = [{'age': 1}]                                           #set the numbers for the privilage and unprivilage for the race\n",
        "unprivileged_groups = [{'age': 0}]\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "\n",
        "\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "learner = LogisticRegression(solver='newton-cg', random_state=1, C=0.01)\n",
        "\n",
        "\n",
        "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "                          unprivileged_groups = unprivileged_groups,\n",
        "                          scope_name='plain_classifier',\n",
        "                          debias=False,\n",
        "                          sess=sess)\n",
        "\n",
        "lis = np.arange(0.0, 1.0, 0.1)\n",
        "\n",
        "accur = []\n",
        "\n",
        "\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "                          unprivileged_groups = unprivileged_groups,\n",
        "                          scope_name='debiased_classifier',\n",
        "                          debias=True,\n",
        "                          sess=sess,\n",
        "                          adversary_loss_weight = 0.1)\n",
        "\n",
        "debiased_model = plain_model.fit(train)\n",
        "\n",
        "preds = debiased_model.predict(test)\n",
        "\n",
        "predictionsAAAAA = preds.labels\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictionsAAAAA, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0][0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0][0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "accur.append(accuracy)\n",
        "sess.close()\n",
        "\n",
        "print(accur)\n"
      ],
      "metadata": {
        "id": "L9JiPl63snKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b07783-4335-42f9-cc9b-08330f5b697e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "epoch 0; iter: 0; batch classifier loss: 0.648088\n",
            "epoch 1; iter: 0; batch classifier loss: 0.623139\n",
            "epoch 2; iter: 0; batch classifier loss: 0.627923\n",
            "epoch 3; iter: 0; batch classifier loss: 0.601497\n",
            "epoch 4; iter: 0; batch classifier loss: 0.566719\n",
            "epoch 5; iter: 0; batch classifier loss: 0.591419\n",
            "epoch 6; iter: 0; batch classifier loss: 0.604912\n",
            "epoch 7; iter: 0; batch classifier loss: 0.621593\n",
            "epoch 8; iter: 0; batch classifier loss: 0.624118\n",
            "epoch 9; iter: 0; batch classifier loss: 0.581142\n",
            "epoch 10; iter: 0; batch classifier loss: 0.596767\n",
            "epoch 11; iter: 0; batch classifier loss: 0.539824\n",
            "epoch 12; iter: 0; batch classifier loss: 0.590534\n",
            "epoch 13; iter: 0; batch classifier loss: 0.579750\n",
            "epoch 14; iter: 0; batch classifier loss: 0.546790\n",
            "epoch 15; iter: 0; batch classifier loss: 0.576283\n",
            "epoch 16; iter: 0; batch classifier loss: 0.613352\n",
            "epoch 17; iter: 0; batch classifier loss: 0.549627\n",
            "epoch 18; iter: 0; batch classifier loss: 0.551605\n",
            "epoch 19; iter: 0; batch classifier loss: 0.578924\n",
            "epoch 20; iter: 0; batch classifier loss: 0.548188\n",
            "epoch 21; iter: 0; batch classifier loss: 0.590449\n",
            "epoch 22; iter: 0; batch classifier loss: 0.579192\n",
            "epoch 23; iter: 0; batch classifier loss: 0.587974\n",
            "epoch 24; iter: 0; batch classifier loss: 0.604708\n",
            "epoch 25; iter: 0; batch classifier loss: 0.602980\n",
            "epoch 26; iter: 0; batch classifier loss: 0.578938\n",
            "epoch 27; iter: 0; batch classifier loss: 0.568521\n",
            "epoch 28; iter: 0; batch classifier loss: 0.571992\n",
            "epoch 29; iter: 0; batch classifier loss: 0.568985\n",
            "epoch 30; iter: 0; batch classifier loss: 0.570538\n",
            "epoch 31; iter: 0; batch classifier loss: 0.592783\n",
            "epoch 32; iter: 0; batch classifier loss: 0.566704\n",
            "epoch 33; iter: 0; batch classifier loss: 0.616422\n",
            "epoch 34; iter: 0; batch classifier loss: 0.574874\n",
            "epoch 35; iter: 0; batch classifier loss: 0.574550\n",
            "epoch 36; iter: 0; batch classifier loss: 0.551858\n",
            "epoch 37; iter: 0; batch classifier loss: 0.582304\n",
            "epoch 38; iter: 0; batch classifier loss: 0.535020\n",
            "epoch 39; iter: 0; batch classifier loss: 0.583837\n",
            "epoch 40; iter: 0; batch classifier loss: 0.591172\n",
            "epoch 41; iter: 0; batch classifier loss: 0.526498\n",
            "epoch 42; iter: 0; batch classifier loss: 0.593948\n",
            "epoch 43; iter: 0; batch classifier loss: 0.575788\n",
            "epoch 44; iter: 0; batch classifier loss: 0.504585\n",
            "epoch 45; iter: 0; batch classifier loss: 0.544665\n",
            "epoch 46; iter: 0; batch classifier loss: 0.558911\n",
            "epoch 47; iter: 0; batch classifier loss: 0.575960\n",
            "epoch 48; iter: 0; batch classifier loss: 0.547248\n",
            "epoch 49; iter: 0; batch classifier loss: 0.600619\n",
            "Model Accuracy 0.7466666666666667\n",
            "[0.7466666666666667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictionsAAAAA, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0][0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0][0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy() \n",
        "predictions.resize((len(predictionsAAAAA),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "xpDuZUGGP7wl",
        "outputId": "a9d78ddf-dacf-4f26-fc9f-d8f72d38d4b7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.7466666666666667\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.7466666666666667\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}