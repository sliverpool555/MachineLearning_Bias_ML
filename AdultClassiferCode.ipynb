{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3naoHFZJAs7",
        "outputId": "1991a793-823c-4954-df19-d752fd5effe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aif360[LFR]\n",
            "  Downloading aif360-0.4.0-py3-none-any.whl (175 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▉                              | 10 kB 15.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 20 kB 15.0 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 30 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 40 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 71 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 81 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 174 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 175 kB 5.4 MB/s \n",
            "\u001b[33mWARNING: aif360 0.4.0 does not provide the extra 'lfr'\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (3.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.0.2)\n",
            "Requirement already satisfied: scipy<1.6.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.4.1)\n",
            "Collecting tempeh\n",
            "  Downloading tempeh-0.1.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from aif360[LFR]) (1.3.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360[LFR]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->aif360[LFR]) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->aif360[LFR]) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360[LFR]) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->aif360[LFR]) (1.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[LFR]) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[LFR]) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->aif360[LFR]) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->aif360[LFR]) (4.2.0)\n",
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360[LFR]) (2.23.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from tempeh->aif360[LFR]) (3.6.4)\n",
            "Collecting shap\n",
            "  Downloading shap-0.40.0-cp37-cp37m-manylinux2010_x86_64.whl (564 kB)\n",
            "\u001b[K     |████████████████████████████████| 564 kB 37.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler->tempeh->aif360[LFR]) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (57.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (8.12.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (1.11.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (0.7.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->tempeh->aif360[LFR]) (21.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->tempeh->aif360[LFR]) (1.24.3)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (4.64.0)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (21.3)\n",
            "Collecting slicer==0.0.7\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (1.3.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from shap->tempeh->aif360[LFR]) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->shap->tempeh->aif360[LFR]) (0.34.0)\n",
            "Building wheels for collected packages: memory-profiler\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=c21cac46439cd3336ad0d164986568e1c2d7df7c31dc9517378aec2d0486f95d\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "Successfully built memory-profiler\n",
            "Installing collected packages: slicer, shap, memory-profiler, tempeh, aif360\n",
            "Successfully installed aif360-0.4.0 memory-profiler-0.60.0 shap-0.40.0 slicer-0.0.7 tempeh-0.1.12\n"
          ]
        }
      ],
      "source": [
        "!pip install 'aif360[LFR]'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /usr/local/lib/python3.7/dist-packages/aif360/data/raw/adult"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpD9C8iUJNir",
        "outputId": "e271bca3-9c34-405c-d594-d771d1e8244b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/aif360/data/raw/adult\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGkQjadsJQE6",
        "outputId": "7f8f3e90-002d-4a34-c345-6c75df355063"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-13 10:51:49--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3974305 (3.8M) [application/x-httpd-php]\n",
            "Saving to: ‘adult.data’\n",
            "\n",
            "adult.data          100%[===================>]   3.79M  7.08MB/s    in 0.5s    \n",
            "\n",
            "2022-05-13 10:51:50 (7.08 MB/s) - ‘adult.data’ saved [3974305/3974305]\n",
            "\n",
            "--2022-05-13 10:51:50--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5229 (5.1K) [application/x-httpd-php]\n",
            "Saving to: ‘adult.names’\n",
            "\n",
            "adult.names         100%[===================>]   5.11K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-05-13 10:51:50 (88.0 MB/s) - ‘adult.names’ saved [5229/5229]\n",
            "\n",
            "--2022-05-13 10:51:50--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2003153 (1.9M) [application/x-httpd-php]\n",
            "Saving to: ‘adult.test’\n",
            "\n",
            "adult.test          100%[===================>]   1.91M  4.18MB/s    in 0.5s    \n",
            "\n",
            "2022-05-13 10:51:51 (4.18 MB/s) - ‘adult.test’ saved [2003153/2003153]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing all libraries**"
      ],
      "metadata": {
        "id": "ZE4oS2UUJkZZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler  #MinMaxScaler\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "g6zzbx7wJShZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aequitas==0.42.0\n",
        "!pip install fairlearn==0.4.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjBmtckNHLQ9",
        "outputId": "b0d9c2e0-ab43-491b-d892-23d2ab6580f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aequitas==0.42.0\n",
            "  Downloading aequitas-0.42.0-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting millify==0.1.1\n",
            "  Downloading millify-0.1.1.tar.gz (1.2 kB)\n",
            "Requirement already satisfied: seaborn>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from aequitas==0.42.0) (0.11.2)\n",
            "Collecting altair==4.1.0\n",
            "  Downloading altair-4.1.0-py3-none-any.whl (727 kB)\n",
            "\u001b[K     |████████████████████████████████| 727 kB 53.3 MB/s \n",
            "\u001b[?25hCollecting tabulate==0.8.2\n",
            "  Downloading tabulate-0.8.2.tar.gz (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting Flask-Bootstrap==3.3.7.1\n",
            "  Downloading Flask-Bootstrap-3.3.7.1.tar.gz (456 kB)\n",
            "\u001b[K     |████████████████████████████████| 456 kB 30.8 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.3 MB/s \n",
            "\u001b[?25hCollecting ohio>=0.2.0\n",
            "  Downloading ohio-0.5.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: SQLAlchemy>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aequitas==0.42.0) (1.4.36)\n",
            "Collecting markdown2==2.3.5\n",
            "  Downloading markdown2-2.3.5.zip (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 21.0 MB/s \n",
            "\u001b[?25hCollecting Flask==0.12.2\n",
            "  Downloading Flask-0.12.2-py2.py3-none-any.whl (83 kB)\n",
            "\u001b[K     |████████████████████████████████| 83 kB 735 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from aequitas==0.42.0) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=3.0.3 in /usr/local/lib/python3.7/dist-packages (from aequitas==0.42.0) (3.2.2)\n",
            "Collecting xhtml2pdf==0.2.2\n",
            "  Downloading xhtml2pdf-0.2.2.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair==4.1.0->aequitas==0.42.0) (0.11.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair==4.1.0->aequitas==0.42.0) (4.3.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair==4.1.0->aequitas==0.42.0) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair==4.1.0->aequitas==0.42.0) (2.11.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from altair==4.1.0->aequitas==0.42.0) (1.21.6)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.7/dist-packages (from Flask==0.12.2->aequitas==0.42.0) (1.0.1)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.7/dist-packages (from Flask==0.12.2->aequitas==0.42.0) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.7/dist-packages (from Flask==0.12.2->aequitas==0.42.0) (1.1.0)\n",
            "Collecting dominate\n",
            "  Downloading dominate-2.6.0-py2.py3-none-any.whl (29 kB)\n",
            "Collecting visitor\n",
            "  Downloading visitor-0.1.3.tar.gz (3.3 kB)\n",
            "Requirement already satisfied: html5lib>=1.0 in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf==0.2.2->aequitas==0.42.0) (1.0.1)\n",
            "Requirement already satisfied: httplib2 in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf==0.2.2->aequitas==0.42.0) (0.17.4)\n",
            "Collecting pyPdf2\n",
            "  Downloading PyPDF2-1.27.12-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf==0.2.2->aequitas==0.42.0) (7.1.2)\n",
            "Collecting reportlab>=3.0\n",
            "  Downloading reportlab-3.6.9-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.7 MB 32.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from xhtml2pdf==0.2.2->aequitas==0.42.0) (1.15.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from html5lib>=1.0->xhtml2pdf==0.2.2->aequitas==0.42.0) (0.5.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair==4.1.0->aequitas==0.42.0) (2.0.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->aequitas==0.42.0) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->aequitas==0.42.0) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->aequitas==0.42.0) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.0.3->aequitas==0.42.0) (0.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.3->aequitas==0.42.0) (4.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.1->aequitas==0.42.0) (2022.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn>=0.9.0->aequitas==0.42.0) (1.4.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy>=1.1.1->aequitas==0.42.0) (1.1.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from SQLAlchemy>=1.1.1->aequitas==0.42.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->SQLAlchemy>=1.1.1->aequitas==0.42.0) (3.8.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1.0->aequitas==0.42.0) (21.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1.0->aequitas==0.42.0) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->altair==4.1.0->aequitas==0.42.0) (5.7.1)\n",
            "Building wheels for collected packages: Flask-Bootstrap, markdown2, millify, tabulate, xhtml2pdf, visitor\n",
            "  Building wheel for Flask-Bootstrap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Flask-Bootstrap: filename=Flask_Bootstrap-3.3.7.1-py3-none-any.whl size=460123 sha256=459de122c163d62f65c1209cfac606f949bd615c2215e3f58071fc99574950b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/a2/d6/50d039c9b59b4caca6d7b53839c8100354a52ab7553d2456eb\n",
            "  Building wheel for markdown2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for markdown2: filename=markdown2-2.3.5-py3-none-any.whl size=33327 sha256=2444220617e364333062a2dec670b8b8c0ac4a46a49143136c99ae9c30416206\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/b9/ae/4050b5eeeedc7cba8ed5a0203189c89c0fa980f683822bfa31\n",
            "  Building wheel for millify (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for millify: filename=millify-0.1.1-py3-none-any.whl size=1866 sha256=9bb925da9d1310a812341d0b3e7a83b9a3bd034ba1058d9da26d345c43f8141f\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/26/25/c2a8bb99a5cf348903e6ac35a29878e221cc9daeb698545148\n",
            "  Building wheel for tabulate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tabulate: filename=tabulate-0.8.2-py3-none-any.whl size=23550 sha256=211ec23934b6944fd693a0a1b2e3512cb1c8e07dcd43d5841d589f49cbbad98a\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/63/72/4156fe55e8e06830d7aed3d20a6d1aacc753536843ab7330f6\n",
            "  Building wheel for xhtml2pdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for xhtml2pdf: filename=xhtml2pdf-0.2.2-py3-none-any.whl size=230265 sha256=215efa26cbd94d9a268a566e7f021a20a285fd022b0cb8458a104bc3efbc1323\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/e6/3a/9851102d40dd8e643a4ff3ce5d69988f95d1d9b7448e37a916\n",
            "  Building wheel for visitor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visitor: filename=visitor-0.1.3-py3-none-any.whl size=3946 sha256=db4dc8414db83d057d67737bcae5b264c6da9e04522a1f329837488f8f23b4f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/34/11/053f47218984c9a31a00f911ed98dda036b867481dcc527a12\n",
            "Successfully built Flask-Bootstrap markdown2 millify tabulate xhtml2pdf visitor\n",
            "Installing collected packages: visitor, reportlab, pyPdf2, Flask, dominate, xhtml2pdf, tabulate, pyyaml, ohio, millify, markdown2, Flask-Bootstrap, altair, aequitas\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 1.1.4\n",
            "    Uninstalling Flask-1.1.4:\n",
            "      Successfully uninstalled Flask-1.1.4\n",
            "  Attempting uninstall: tabulate\n",
            "    Found existing installation: tabulate 0.8.9\n",
            "    Uninstalling tabulate-0.8.9:\n",
            "      Successfully uninstalled tabulate-0.8.9\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: altair\n",
            "    Found existing installation: altair 4.2.0\n",
            "    Uninstalling altair-4.2.0:\n",
            "      Successfully uninstalled altair-4.2.0\n",
            "Successfully installed Flask-0.12.2 Flask-Bootstrap-3.3.7.1 aequitas-0.42.0 altair-4.1.0 dominate-2.6.0 markdown2-2.3.5 millify-0.1.1 ohio-0.5.0 pyPdf2-1.27.12 pyyaml-6.0 reportlab-3.6.9 tabulate-0.8.2 visitor-0.1.3 xhtml2pdf-0.2.2\n",
            "Collecting fairlearn==0.4.6\n",
            "  Downloading fairlearn-0.4.6-py3-none-any.whl (21.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.2 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6) (1.0.2)\n",
            "Requirement already satisfied: ipywidgets>=7.5.0 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6) (7.7.0)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from fairlearn==0.4.6) (1.21.6)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (5.1.1)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (4.10.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (3.6.0)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (5.5.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (0.2.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (1.1.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.5.0->fairlearn==0.4.6) (5.3.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.3.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.8.1)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (2.15.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.10.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (21.4.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (4.11.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn==0.4.6) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.1->fairlearn==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.2.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn==0.4.6) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->fairlearn==0.4.6) (1.1.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (2.11.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn==0.4.6) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (2.0.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (5.0.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.5.0->fairlearn==0.4.6) (0.5.1)\n",
            "Installing collected packages: fairlearn\n",
            "Successfully installed fairlearn-0.4.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from aif360.datasets import AdultDataset\n",
        "from aif360.algorithms.preprocessing.optim_preproc_helpers.data_preproc_functions import load_preproc_data_adult\n",
        "from aif360.metrics import ClassificationMetric\n",
        "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
        "#from aif360.algorithms.preprocessing import *\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing\n"
      ],
      "metadata": {
        "id": "iIa3PbYfJqW4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the fairness Matrixs"
      ],
      "metadata": {
        "id": "ELguKCpVJy1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "races = ['White', 'Asian.Pac.Islander', 'Amer.Indian.Eskimo', 'Other', 'Black'] #Set the name of the races\n",
        "\n",
        "privileged_groups = [{'race': 1}]                                           #set the numbers for the privilage and unprivilage for the race\n",
        "unprivileged_groups = [{'race': 0}, {'race': 2}, {'race': 3}, {'race': 4}]\n",
        "\n",
        "\n",
        "\n",
        "dataset_orig = load_preproc_data_adult(['race'])                            #import the data"
      ],
      "metadata": {
        "id": "8TGF_bJ5Jurh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Managing the dataset into train and test"
      ],
      "metadata": {
        "id": "o1l-yak0J5qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train, test = dataset_orig.split([0.7], shuffle=True) #split the test and train\n",
        "print(\"training data size\", train.features.shape)     #find the dimentions of the data\n",
        "print(\"dataset feature names\", train.feature_names)\n",
        "\n",
        "#Normalize the dataset, both train and test. This should always be done in any machine learning pipeline!\n",
        "scale_orig = StandardScaler()                         #create sclaer object\n",
        "X_train = scale_orig.fit_transform(train.features)    #use the scalerto normalise the data\n",
        "y_train = train.labels.ravel()                        #Standardise the class labels\n",
        "\n",
        "X_test = scale_orig.transform(test.features) \n",
        "y_test = test.labels.ravel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oev9CI5lJ9rJ",
        "outputId": "6a323ce8-fc66-4f6f-a0af-4a0425fc485a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training data size (34189, 18)\n",
            "dataset feature names ['race', 'sex', 'Age (decade)=10', 'Age (decade)=20', 'Age (decade)=30', 'Age (decade)=40', 'Age (decade)=50', 'Age (decade)=60', 'Age (decade)=>=70', 'Education Years=6', 'Education Years=7', 'Education Years=8', 'Education Years=9', 'Education Years=10', 'Education Years=11', 'Education Years=12', 'Education Years=<6', 'Education Years=>12']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions used in all the tasks"
      ],
      "metadata": {
        "id": "U7-GcZwBK6sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_score(predictions, labels):\n",
        "    \n",
        "    correct_ans = [] \n",
        "    \n",
        "    for i in labels:\n",
        "      correct_ans.append(i)     #append the label to the list\n",
        "    \n",
        "    lists = zip(predictions, correct_ans)   #put both lists into 1 to make it easier to manage\n",
        "    \n",
        "    preds = []\n",
        "    corrects = []\n",
        "    \n",
        "    score = 0\n",
        "    \n",
        "    for i in lists:\n",
        "        if int(round(i[0])) == int(round(i[1])):  #If both elements in the same row equal the same \n",
        "            score = score + 1                     #Add 1 to score\n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "        var = round(i[0])       #round the i to nearest int\n",
        "        preds.append(int(var))  #update preds\n",
        "        corrects.append(i[1])\n",
        "      \n",
        "    \n",
        "    accuracy = score/len(labels)      #find the accuracy as a percentage\n",
        "    print(\"Model Accuracy\", accuracy)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "RXmJCZFeLAU6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 Finding an 5 Fold Cross validation for Accuracy and Fairness**"
      ],
      "metadata": {
        "id": "GMoOXQQSKEF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(train.features.shape)\n",
        "print()\n",
        "trainVar, trainA = train.split([0.8], shuffle=True)   #spilt training data into 5 fold through the split function\n",
        "\n",
        "print(trainA.features.shape)\n",
        "\n",
        "trainVar, trainB = trainVar.split([0.75], shuffle=True)\n",
        "\n",
        "print(trainB.features.shape)\n",
        "\n",
        "trainVar, trainC = trainVar.split([0.666666], shuffle=True)\n",
        "\n",
        "print(trainC.features.shape)\n",
        "\n",
        "trainD, trainE = trainVar.split([0.5], shuffle=True)\n",
        "\n",
        "print(trainD.features.shape)\n",
        "print(trainE.features.shape)\n",
        "\n",
        "train_cross = [trainA, trainB, trainC, trainD, trainE]  #put all the 5 fold data into a train list\n",
        "X_train_cross = train_cross     #set the features to the train\n",
        "Y_train_cross = [0] * 5         #create a list which has 5 spaces to start the lists\n",
        "\n",
        "for i in range(len(train_cross)):\n",
        "    Y_train_cross[i] = train_cross[i].labels.ravel() #Find the labels from the train data\n",
        "\n",
        "scale_orig = StandardScaler()   #setup the scaler object\n",
        "\n",
        "for i in range(len(train_cross)):\n",
        "    X_train_cross[i] = scale_orig.fit_transform(train_cross[i].features) #scale the features in 5 fold\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuZAm616KNCJ",
        "outputId": "fb0609b9-6d50-4776-e31c-42be213d65d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(34189, 18)\n",
            "\n",
            "(6838, 18)\n",
            "(6838, 18)\n",
            "(6838, 18)\n",
            "(6837, 18)\n",
            "(6838, 18)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seaborn import matrix\n",
        "Cs = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000] #Put all hyperparamters for C in lists\n",
        "scores = []\n",
        "\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']     #list of the activation functions\n",
        "\n",
        "datas = {}\n",
        "\n",
        "with open('task1crossvalResults.csv', 'w') as csvfile:\n",
        "    field_names = ['Activation', 'C' , 'crossVal1', 'crossVal2', 'crossVal3', 'crossVal4', 'crossVal5','Accuracy', 'general', \n",
        "                   'Par_diff1', 'Par_diff2', 'Par_diff3', 'Par_diff4', 'Par_diff5', 'par_diff_avg',\n",
        "                   'ep_opp1', 'ep_opp2','ep_opp3', 'ep_opp4', 'ep_opp5', 'ep_opp_avg',\n",
        "                   'avg_odd1', 'avg_odd2', 'avg_odd3', 'avg_odd4', 'avg_odd5','avg_odds_avg' ,'TPR', 'FPR'] #Name of the columns for the .csv file\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=field_names)  #write the names of columns in .csv\n",
        "    writer.writeheader()    #write the headers\n",
        "\n",
        "\n",
        "    for solv in solvers:  #loop through hyperparemeters\n",
        "          for c in Cs:\n",
        "              classifier = LogisticRegression(C=c, solver=solv) #make the logistic regression\n",
        "\n",
        "              data = {}\n",
        "              accuracies = []   #create lists for storing the epoch\n",
        "              cs = []\n",
        "              activations = []\n",
        "              par_diff = []\n",
        "              ep_opp = []\n",
        "              avg_odds = []\n",
        "              TPR = []\n",
        "              FPR = []\n",
        "\n",
        "\n",
        "              for i in range(len(train_cross)): #loop through to the lenght of train_cross\n",
        "\n",
        "                  X_test_cross = [] #create lists to store data for the cross validation\n",
        "                  Y_test_cross = []\n",
        "                  indexs = []\n",
        "                  ind = []\n",
        "                  X = []\n",
        "                  Y = []\n",
        "\n",
        "                  for j in range(len(train_cross)): #loop through the cross validation\n",
        "                      if i == j:                          #If the i == the j index then the X_train[i] is the test\n",
        "                          X_test_cross = X_train_cross[j] #set the X_train and Y _train\n",
        "                          Y_test_cross = Y_train_cross[j]\n",
        "                          ind.append(j)\n",
        "                      else:\n",
        "                          indexs.append(j) #all the non test is now in indexs\n",
        "\n",
        "\n",
        "                  X = np.concatenate([X_train_cross[indexs[0]], X_train_cross[indexs[1]], X_train_cross[indexs[2]]]) #joining all the features together that are not in the test part of the fold\n",
        "                  Y = np.concatenate([Y_train_cross[indexs[0]], Y_train_cross[indexs[1]], Y_train_cross[indexs[2]]])\n",
        "                  X = np.concatenate([X, X_train_cross[indexs[3]]]) #the concatenate is \n",
        "                  Y = np.concatenate([Y, Y_train_cross[indexs[3]]])\n",
        "\n",
        "                  LR = classifier.fit(X, Y) #fit all the data that is not in the test\n",
        "\n",
        "                  predictions = LR.predict(X_test_cross)    #use model to make prediction\n",
        "                  \n",
        "                  acc = find_score(predictions, Y_test_cross) #find the accuracy from the test in k-fold\n",
        "\n",
        "                  train_copy = trainA #As a backup redunacy so the system doesnt crash (Hasnt but there is an else)\n",
        "\n",
        "\n",
        "\n",
        "                  if i == 0:              #if the iteration is 1 set the copy to the train\n",
        "                    train_copy = trainA\n",
        "\n",
        "                  elif i == 1:\n",
        "                    train_copy = trainB\n",
        "                  \n",
        "                  elif i == 2:\n",
        "                    train_copy = trainC\n",
        "\n",
        "                  elif i == 3:\n",
        "                    train_copy = trainD\n",
        "                  \n",
        "                  elif i == 4:\n",
        "                    train_copy = trainE\n",
        "\n",
        "                  else:\n",
        "                    print(\"Error\")\n",
        "\n",
        "                  test_pred = train_copy.copy()               #set the test pred to the train to store features\n",
        "                  predictions.resize((len(predictions),1))    #resize the predictions\n",
        "                  test_pred.labels = predictions              #set the labels to the predictions\n",
        "\n",
        "\n",
        "\n",
        "                  metric = ClassificationMetric(train_copy, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups) #create fairness matrix\n",
        "                  metric_arrs = {}\n",
        "                  metric_arrs['C'] = c              #store results in the dictionary\n",
        "                  metric_arrs['Activation'] = solv\n",
        "                  metric_arrs['Accuracy'] = acc\n",
        "                  metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "                  metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "                  metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "                  #metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "                  #metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "                  metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "                  metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "                  #print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, acc))\n",
        "                  #print(metric_arrs)\n",
        "                  #print('\\n')\n",
        "\n",
        "\n",
        "                  accuracies.append(acc)  #store all the data for one epoch\n",
        "                  cs.append(c)\n",
        "                  activations.append(solv)\n",
        "                  par_diff.append((metric.statistical_parity_difference()))\n",
        "                  ep_opp.append((metric.equal_opportunity_difference()))\n",
        "                  avg_odds.append((metric.average_odds_difference()))\n",
        "                  TPR.append((metric.generalized_true_positive_rate()))\n",
        "                  FPR.append((metric.num_generalized_false_positives()))\n",
        "\n",
        "\n",
        "              pred_test = LR.predict(X_test)\n",
        "              test_acc = find_score(pred_test, y_test)\n",
        "              print(\"Generalisability --- \", test_acc)\n",
        "\n",
        "              print(\"Accuracies: \", accuracies)\n",
        "              print(\"Cs\", c)\n",
        "              print(\"Activation: \", activations)\n",
        "              print(\"Par_diff: \", par_diff)\n",
        "              print(\"ep_opp: \", ep_opp)\n",
        "              print(\"avg_odd: \", avg_odds)\n",
        "              print(\"TPR: \", TPR)\n",
        "              print(\"FPR: \", FPR)\n",
        "\n",
        "              acc = sum(accuracies)/len(accuracies) #find the average for all variables for analysis\n",
        "              par_diff_avg = sum(par_diff)/len(par_diff)\n",
        "              ep_opp_avg = sum(ep_opp)/len(ep_opp)\n",
        "              avg_odds_avg = sum(avg_odds)/len(avg_odds)\n",
        "\n",
        "              writer.writerow({'Activation': solv, 'C': c , 'crossVal1': accuracies[0], 'crossVal2': accuracies[1], 'crossVal3': accuracies[2], 'crossVal4': accuracies[3], 'crossVal5': accuracies[4],'Accuracy': acc, 'general': test_acc, \n",
        "                               'Par_diff1': par_diff[0], 'Par_diff2': par_diff[1], 'Par_diff3': par_diff[2], 'Par_diff4': par_diff[3], 'Par_diff5': par_diff[4], 'par_diff_avg': par_diff_avg,\n",
        "                   'ep_opp1': ep_opp[0], 'ep_opp2': ep_opp[1],'ep_opp3': ep_opp[2], 'ep_opp4': ep_opp[3], 'ep_opp5': ep_opp[4], 'ep_opp_avg': ep_opp_avg,\n",
        "                   'avg_odd1': avg_odds[0], 'avg_odd2': avg_odds[1], 'avg_odd3': avg_odds[2], 'avg_odd4': avg_odds[3], 'avg_odd5': avg_odds[4],'avg_odds_avg': avg_odds_avg , 'TPR': TPR[0], 'FPR': FPR[0]}) #save the data into .csv\n",
        "\n",
        "          \n",
        "files.download('task1crossvalResults.csv')   #save the data into a .csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CuE4D0sKKSHZ",
        "outputId": "99c7316d-070b-49c1-9aef-e840517175ef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.7535829189821586\n",
            "Model Accuracy 0.7576776835331969\n",
            "Model Accuracy 0.7581164083065224\n",
            "Model Accuracy 0.7629077080590903\n",
            "Model Accuracy 0.7623574144486692\n",
            "Model Accuracy 0.7648945608407834\n",
            "Generalisability ---  0.7648945608407834\n",
            "Accuracies:  [0.7535829189821586, 0.7576776835331969, 0.7581164083065224, 0.7629077080590903, 0.7623574144486692]\n",
            "Cs 1e-05\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7535829189821586\n",
            "Model Accuracy 0.7576776835331969\n",
            "Model Accuracy 0.7581164083065224\n",
            "Model Accuracy 0.7629077080590903\n",
            "Model Accuracy 0.7623574144486692\n",
            "Model Accuracy 0.7648945608407834\n",
            "Generalisability ---  0.7648945608407834\n",
            "Accuracies:  [0.7535829189821586, 0.7576776835331969, 0.7581164083065224, 0.7629077080590903, 0.7623574144486692]\n",
            "Cs 0.0001\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8079847908745247\n",
            "Model Accuracy 0.8034513015501609\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8031300277899663\n",
            "Model Accuracy 0.804621234279029\n",
            "Model Accuracy 0.8024295366136628\n",
            "Generalisability ---  0.8024295366136628\n",
            "Accuracies:  [0.8079847908745247, 0.8034513015501609, 0.8033050599590523, 0.8031300277899663, 0.804621234279029]\n",
            "Cs 0.001\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.10876867885285149, -0.09490572169155934, -0.09254516187210127, -0.08113467785794193, -0.09961731643147573]\n",
            "ep_opp:  [-0.2266925876334672, -0.17591168764228343, -0.15267365033720176, -0.08565043440413816, -0.18280808947011112]\n",
            "avg_odd:  [-0.13468983689412758, -0.1082743411115379, -0.09721510792503055, -0.06697266677550172, -0.11608432725847534]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8034513015501609\n",
            "Model Accuracy 0.8035975431412694\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8019518187401897\n",
            "Generalisability ---  0.8019518187401897\n",
            "Accuracies:  [0.8071073413278737, 0.8034513015501609, 0.8035975431412694, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 0.01\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.10945685629933624, -0.0869455226865842, -0.08667432038482142, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.1501052360293802, -0.1270326246961761, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.0930181741286157, -0.08323986069343225, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8022930457926705\n",
            "Generalisability ---  0.8022930457926705\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 0.1\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8022930457926705\n",
            "Generalisability ---  0.8022930457926705\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 1\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8022930457926705\n",
            "Generalisability ---  0.8022930457926705\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 10\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8019518187401897\n",
            "Generalisability ---  0.8019518187401897\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 100\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8019518187401897\n",
            "Generalisability ---  0.8019518187401897\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 1000\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7535829189821586\n",
            "Model Accuracy 0.7576776835331969\n",
            "Model Accuracy 0.7581164083065224\n",
            "Model Accuracy 0.7629077080590903\n",
            "Model Accuracy 0.7623574144486692\n",
            "Model Accuracy 0.7648945608407834\n",
            "Generalisability ---  0.7648945608407834\n",
            "Accuracies:  [0.7535829189821586, 0.7576776835331969, 0.7581164083065224, 0.7629077080590903, 0.7623574144486692]\n",
            "Cs 1e-05\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7535829189821586\n",
            "Model Accuracy 0.7576776835331969\n",
            "Model Accuracy 0.7581164083065224\n",
            "Model Accuracy 0.7629077080590903\n",
            "Model Accuracy 0.7623574144486692\n",
            "Model Accuracy 0.7648945608407834\n",
            "Generalisability ---  0.7648945608407834\n",
            "Accuracies:  [0.7535829189821586, 0.7576776835331969, 0.7581164083065224, 0.7629077080590903, 0.7623574144486692]\n",
            "Cs 0.0001\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8079847908745247\n",
            "Model Accuracy 0.8034513015501609\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8031300277899663\n",
            "Model Accuracy 0.804621234279029\n",
            "Model Accuracy 0.8024295366136628\n",
            "Generalisability ---  0.8024295366136628\n",
            "Accuracies:  [0.8079847908745247, 0.8034513015501609, 0.8033050599590523, 0.8031300277899663, 0.804621234279029]\n",
            "Cs 0.001\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.10876867885285149, -0.09490572169155934, -0.09254516187210127, -0.08113467785794193, -0.09961731643147573]\n",
            "ep_opp:  [-0.2266925876334672, -0.17591168764228343, -0.15267365033720176, -0.08565043440413816, -0.18280808947011112]\n",
            "avg_odd:  [-0.13468983689412758, -0.1082743411115379, -0.09721510792503055, -0.06697266677550172, -0.11608432725847534]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8034513015501609\n",
            "Model Accuracy 0.8035975431412694\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8019518187401897\n",
            "Generalisability ---  0.8019518187401897\n",
            "Accuracies:  [0.8071073413278737, 0.8034513015501609, 0.8035975431412694, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 0.01\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.10945685629933624, -0.0869455226865842, -0.08667432038482142, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.1501052360293802, -0.1270326246961761, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.0930181741286157, -0.08323986069343225, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8022930457926705\n",
            "Generalisability ---  0.8022930457926705\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 0.1\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8022930457926705\n",
            "Generalisability ---  0.8022930457926705\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 1\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8022930457926705\n",
            "Generalisability ---  0.8022930457926705\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 10\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8019518187401897\n",
            "Generalisability ---  0.8019518187401897\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 100\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8019518187401897\n",
            "Generalisability ---  0.8019518187401897\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 1000\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8008189529102077\n",
            "Model Accuracy 0.7952617724480843\n",
            "Model Accuracy 0.7959929804036268\n",
            "Model Accuracy 0.7959631417288284\n",
            "Model Accuracy 0.7954080140391927\n",
            "Model Accuracy 0.7939671057121409\n",
            "Generalisability ---  0.7939671057121409\n",
            "Accuracies:  [0.8008189529102077, 0.7952617724480843, 0.7959929804036268, 0.7959631417288284, 0.7954080140391927]\n",
            "Cs 1e-05\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.08358948051676125, -0.048764407006626376, -0.05497621784294357, -0.07650369837321207, -0.04998344157636192]\n",
            "ep_opp:  [-0.0927661619802932, -0.00623684549632747, -0.013368251686008692, -0.03225387950516434, -0.008682602636938597]\n",
            "avg_odd:  [-0.06406612417134289, -0.009725917281419201, -0.016055178004481037, -0.04118548673163504, -0.014707830312449273]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8003802281368821\n",
            "Model Accuracy 0.7977478794969289\n",
            "Model Accuracy 0.7971629131324949\n",
            "Model Accuracy 0.797718297498903\n",
            "Model Accuracy 0.7974553963147119\n",
            "Model Accuracy 0.7963556950795059\n",
            "Generalisability ---  0.7963556950795059\n",
            "Accuracies:  [0.8003802281368821, 0.7977478794969289, 0.7971629131324949, 0.797718297498903, 0.7974553963147119]\n",
            "Cs 0.0001\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.0772754873426998, -0.06493027746574857, -0.0774528461871832, -0.06794791466342424, -0.07035490221330928]\n",
            "ep_opp:  [-0.07971984756411576, -0.04586572741720718, -0.07760775050494678, -0.019213385339069577, -0.037294100832529375]\n",
            "avg_odd:  [-0.055578557441645665, -0.03583505991380005, -0.056135855779768234, -0.031132073475804437, -0.03881147254147113]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8073998245100906\n",
            "Model Accuracy 0.8034513015501609\n",
            "Model Accuracy 0.8035975431412694\n",
            "Model Accuracy 0.8034225537516455\n",
            "Model Accuracy 0.8040362679145949\n",
            "Model Accuracy 0.8019518187401897\n",
            "Generalisability ---  0.8019518187401897\n",
            "Accuracies:  [0.8073998245100906, 0.8034513015501609, 0.8035975431412694, 0.8034225537516455, 0.8040362679145949]\n",
            "Cs 0.001\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.10809166858602567, -0.0869455226865842, -0.08667432038482142, -0.07710648047627024, -0.046018619027468566]\n",
            "ep_opp:  [-0.21875321866309608, -0.1501052360293802, -0.1270326246961761, -0.06735775147730888, -0.004028656161789379]\n",
            "avg_odd:  [-0.13089614067116845, -0.0930181741286157, -0.08323986069343225, -0.057223189003281286, -0.01150421011979634]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8034513015501609\n",
            "Model Accuracy 0.8035975431412694\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8022930457926705\n",
            "Generalisability ---  0.8022930457926705\n",
            "Accuracies:  [0.8071073413278737, 0.8034513015501609, 0.8035975431412694, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 0.01\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.10945685629933624, -0.0869455226865842, -0.08667432038482142, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.1501052360293802, -0.1270326246961761, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.0930181741286157, -0.08323986069343225, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8022930457926705\n",
            "Generalisability ---  0.8022930457926705\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 0.1\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8022930457926705\n",
            "Generalisability ---  0.8022930457926705\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 1\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8022930457926705\n",
            "Generalisability ---  0.8022930457926705\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 10\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8019518187401897\n",
            "Generalisability ---  0.8019518187401897\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 100\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8071073413278737\n",
            "Model Accuracy 0.8033050599590523\n",
            "Model Accuracy 0.8038900263234864\n",
            "Model Accuracy 0.8038613426941641\n",
            "Model Accuracy 0.804913717461246\n",
            "Model Accuracy 0.8019518187401897\n",
            "Generalisability ---  0.8019518187401897\n",
            "Accuracies:  [0.8071073413278737, 0.8033050599590523, 0.8038900263234864, 0.8038613426941641, 0.804913717461246]\n",
            "Cs 1000\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.10945685629933624, -0.08917422146937179, -0.08908147306707725, -0.08309552907312172, -0.10706860706860707]\n",
            "ep_opp:  [-0.2207101658255227, -0.15409990979768912, -0.1323730786347609, -0.08039824564340359, -0.19591685425376068]\n",
            "avg_odd:  [-0.1324523817587372, -0.09582363846601422, -0.0866048537581392, -0.06556700572389418, -0.12508751700106824]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_00689337-3947-4403-b8b2-78d9655fb124\", \"task1crossvalResults.csv\", 13336)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting the Hyperparamters for TASK1** "
      ],
      "metadata": {
        "id": "WNBZAzYgrnKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task1_df = pd.read_csv(\"task1crossvalResults.csv\") #read and print the data\n",
        "task1_df"
      ],
      "metadata": {
        "id": "MTmGYml6sATl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "outputId": "f793dbae-578f-412c-d880-040b2d4d91a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Activation           C  crossVal1  crossVal2  crossVal3  crossVal4  \\\n",
              "0   newton-cg     0.00001   0.753583   0.757678   0.758116   0.762908   \n",
              "1   newton-cg     0.00010   0.753583   0.757678   0.758116   0.762908   \n",
              "2   newton-cg     0.00100   0.807985   0.803451   0.803305   0.803130   \n",
              "3   newton-cg     0.01000   0.807107   0.803451   0.803598   0.803861   \n",
              "4   newton-cg     0.10000   0.807107   0.803305   0.803890   0.803861   \n",
              "5   newton-cg     1.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "6   newton-cg    10.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "7   newton-cg   100.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "8   newton-cg  1000.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "9       lbfgs     0.00001   0.753583   0.757678   0.758116   0.762908   \n",
              "10      lbfgs     0.00010   0.753583   0.757678   0.758116   0.762908   \n",
              "11      lbfgs     0.00100   0.807985   0.803451   0.803305   0.803130   \n",
              "12      lbfgs     0.01000   0.807107   0.803451   0.803598   0.803861   \n",
              "13      lbfgs     0.10000   0.807107   0.803305   0.803890   0.803861   \n",
              "14      lbfgs     1.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "15      lbfgs    10.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "16      lbfgs   100.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "17      lbfgs  1000.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "18  liblinear     0.00001   0.800819   0.795262   0.795993   0.795963   \n",
              "19  liblinear     0.00010   0.800380   0.797748   0.797163   0.797718   \n",
              "20  liblinear     0.00100   0.807400   0.803451   0.803598   0.803423   \n",
              "21  liblinear     0.01000   0.807107   0.803451   0.803598   0.803861   \n",
              "22  liblinear     0.10000   0.807107   0.803305   0.803890   0.803861   \n",
              "23  liblinear     1.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "24  liblinear    10.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "25  liblinear   100.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "26  liblinear  1000.00000   0.807107   0.803305   0.803890   0.803861   \n",
              "\n",
              "    crossVal5  Accuracy   general  Par_diff1  ...   ep_opp5  ep_opp_avg  \\\n",
              "0    0.762357  0.758928  0.764895   0.000000  ...  0.000000    0.000000   \n",
              "1    0.762357  0.758928  0.764895   0.000000  ...  0.000000    0.000000   \n",
              "2    0.804621  0.804498  0.802430  -0.108769  ... -0.182808   -0.164747   \n",
              "3    0.804914  0.804586  0.801952  -0.109457  ... -0.195917   -0.154833   \n",
              "4    0.804914  0.804615  0.802293  -0.109457  ... -0.195917   -0.156700   \n",
              "5    0.804914  0.804615  0.802293  -0.109457  ... -0.195917   -0.156700   \n",
              "6    0.804914  0.804615  0.802293  -0.109457  ... -0.195917   -0.156700   \n",
              "7    0.804914  0.804615  0.801952  -0.109457  ... -0.195917   -0.156700   \n",
              "8    0.804914  0.804615  0.801952  -0.109457  ... -0.195917   -0.156700   \n",
              "9    0.762357  0.758928  0.764895   0.000000  ...  0.000000    0.000000   \n",
              "10   0.762357  0.758928  0.764895   0.000000  ...  0.000000    0.000000   \n",
              "11   0.804621  0.804498  0.802430  -0.108769  ... -0.182808   -0.164747   \n",
              "12   0.804914  0.804586  0.801952  -0.109457  ... -0.195917   -0.154833   \n",
              "13   0.804914  0.804615  0.802293  -0.109457  ... -0.195917   -0.156700   \n",
              "14   0.804914  0.804615  0.802293  -0.109457  ... -0.195917   -0.156700   \n",
              "15   0.804914  0.804615  0.802293  -0.109457  ... -0.195917   -0.156700   \n",
              "16   0.804914  0.804615  0.801952  -0.109457  ... -0.195917   -0.156700   \n",
              "17   0.804914  0.804615  0.801952  -0.109457  ... -0.195917   -0.156700   \n",
              "18   0.795408  0.796689  0.793967  -0.083589  ... -0.008683   -0.030662   \n",
              "19   0.797455  0.798093  0.796356  -0.077275  ... -0.037294   -0.051940   \n",
              "20   0.804036  0.804381  0.801952  -0.108092  ... -0.004029   -0.113455   \n",
              "21   0.804914  0.804586  0.802293  -0.109457  ... -0.195917   -0.154833   \n",
              "22   0.804914  0.804615  0.802293  -0.109457  ... -0.195917   -0.156700   \n",
              "23   0.804914  0.804615  0.802293  -0.109457  ... -0.195917   -0.156700   \n",
              "24   0.804914  0.804615  0.802293  -0.109457  ... -0.195917   -0.156700   \n",
              "25   0.804914  0.804615  0.801952  -0.109457  ... -0.195917   -0.156700   \n",
              "26   0.804914  0.804615  0.801952  -0.109457  ... -0.195917   -0.156700   \n",
              "\n",
              "    avg_odd1  avg_odd2  avg_odd3  avg_odd4  avg_odd5  avg_odds_avg  TPR  FPR  \n",
              "0   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "1   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "2  -0.134690 -0.108274 -0.097215 -0.066973 -0.116084     -0.104647  1.0  0.0  \n",
              "3  -0.132452 -0.093018 -0.083240 -0.065567 -0.125088     -0.099873  1.0  0.0  \n",
              "4  -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "5  -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "6  -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "7  -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "8  -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "9   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "10  0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "11 -0.134690 -0.108274 -0.097215 -0.066973 -0.116084     -0.104647  1.0  0.0  \n",
              "12 -0.132452 -0.093018 -0.083240 -0.065567 -0.125088     -0.099873  1.0  0.0  \n",
              "13 -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "14 -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "15 -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "16 -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "17 -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "18 -0.064066 -0.009726 -0.016055 -0.041185 -0.014708     -0.029148  1.0  0.0  \n",
              "19 -0.055579 -0.035835 -0.056136 -0.031132 -0.038811     -0.043499  1.0  0.0  \n",
              "20 -0.130896 -0.093018 -0.083240 -0.057223 -0.011504     -0.075176  1.0  0.0  \n",
              "21 -0.132452 -0.093018 -0.083240 -0.065567 -0.125088     -0.099873  1.0  0.0  \n",
              "22 -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "23 -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "24 -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "25 -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "26 -0.132452 -0.095824 -0.086605 -0.065567 -0.125088     -0.101107  1.0  0.0  \n",
              "\n",
              "[27 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-cdae801d-d665-4080-b2a8-a40339327471\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Activation</th>\n",
              "      <th>C</th>\n",
              "      <th>crossVal1</th>\n",
              "      <th>crossVal2</th>\n",
              "      <th>crossVal3</th>\n",
              "      <th>crossVal4</th>\n",
              "      <th>crossVal5</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>general</th>\n",
              "      <th>Par_diff1</th>\n",
              "      <th>...</th>\n",
              "      <th>ep_opp5</th>\n",
              "      <th>ep_opp_avg</th>\n",
              "      <th>avg_odd1</th>\n",
              "      <th>avg_odd2</th>\n",
              "      <th>avg_odd3</th>\n",
              "      <th>avg_odd4</th>\n",
              "      <th>avg_odd5</th>\n",
              "      <th>avg_odds_avg</th>\n",
              "      <th>TPR</th>\n",
              "      <th>FPR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.753583</td>\n",
              "      <td>0.757678</td>\n",
              "      <td>0.758116</td>\n",
              "      <td>0.762908</td>\n",
              "      <td>0.762357</td>\n",
              "      <td>0.758928</td>\n",
              "      <td>0.764895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.753583</td>\n",
              "      <td>0.757678</td>\n",
              "      <td>0.758116</td>\n",
              "      <td>0.762908</td>\n",
              "      <td>0.762357</td>\n",
              "      <td>0.758928</td>\n",
              "      <td>0.764895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.807985</td>\n",
              "      <td>0.803451</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803130</td>\n",
              "      <td>0.804621</td>\n",
              "      <td>0.804498</td>\n",
              "      <td>0.802430</td>\n",
              "      <td>-0.108769</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.182808</td>\n",
              "      <td>-0.164747</td>\n",
              "      <td>-0.134690</td>\n",
              "      <td>-0.108274</td>\n",
              "      <td>-0.097215</td>\n",
              "      <td>-0.066973</td>\n",
              "      <td>-0.116084</td>\n",
              "      <td>-0.104647</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803451</td>\n",
              "      <td>0.803598</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804586</td>\n",
              "      <td>0.801952</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.154833</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.093018</td>\n",
              "      <td>-0.083240</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.099873</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.802293</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.802293</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.802293</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.801952</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.801952</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.753583</td>\n",
              "      <td>0.757678</td>\n",
              "      <td>0.758116</td>\n",
              "      <td>0.762908</td>\n",
              "      <td>0.762357</td>\n",
              "      <td>0.758928</td>\n",
              "      <td>0.764895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.753583</td>\n",
              "      <td>0.757678</td>\n",
              "      <td>0.758116</td>\n",
              "      <td>0.762908</td>\n",
              "      <td>0.762357</td>\n",
              "      <td>0.758928</td>\n",
              "      <td>0.764895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.807985</td>\n",
              "      <td>0.803451</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803130</td>\n",
              "      <td>0.804621</td>\n",
              "      <td>0.804498</td>\n",
              "      <td>0.802430</td>\n",
              "      <td>-0.108769</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.182808</td>\n",
              "      <td>-0.164747</td>\n",
              "      <td>-0.134690</td>\n",
              "      <td>-0.108274</td>\n",
              "      <td>-0.097215</td>\n",
              "      <td>-0.066973</td>\n",
              "      <td>-0.116084</td>\n",
              "      <td>-0.104647</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803451</td>\n",
              "      <td>0.803598</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804586</td>\n",
              "      <td>0.801952</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.154833</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.093018</td>\n",
              "      <td>-0.083240</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.099873</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.802293</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.802293</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.802293</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.801952</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.801952</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.800819</td>\n",
              "      <td>0.795262</td>\n",
              "      <td>0.795993</td>\n",
              "      <td>0.795963</td>\n",
              "      <td>0.795408</td>\n",
              "      <td>0.796689</td>\n",
              "      <td>0.793967</td>\n",
              "      <td>-0.083589</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.008683</td>\n",
              "      <td>-0.030662</td>\n",
              "      <td>-0.064066</td>\n",
              "      <td>-0.009726</td>\n",
              "      <td>-0.016055</td>\n",
              "      <td>-0.041185</td>\n",
              "      <td>-0.014708</td>\n",
              "      <td>-0.029148</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.800380</td>\n",
              "      <td>0.797748</td>\n",
              "      <td>0.797163</td>\n",
              "      <td>0.797718</td>\n",
              "      <td>0.797455</td>\n",
              "      <td>0.798093</td>\n",
              "      <td>0.796356</td>\n",
              "      <td>-0.077275</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.037294</td>\n",
              "      <td>-0.051940</td>\n",
              "      <td>-0.055579</td>\n",
              "      <td>-0.035835</td>\n",
              "      <td>-0.056136</td>\n",
              "      <td>-0.031132</td>\n",
              "      <td>-0.038811</td>\n",
              "      <td>-0.043499</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.807400</td>\n",
              "      <td>0.803451</td>\n",
              "      <td>0.803598</td>\n",
              "      <td>0.803423</td>\n",
              "      <td>0.804036</td>\n",
              "      <td>0.804381</td>\n",
              "      <td>0.801952</td>\n",
              "      <td>-0.108092</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004029</td>\n",
              "      <td>-0.113455</td>\n",
              "      <td>-0.130896</td>\n",
              "      <td>-0.093018</td>\n",
              "      <td>-0.083240</td>\n",
              "      <td>-0.057223</td>\n",
              "      <td>-0.011504</td>\n",
              "      <td>-0.075176</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803451</td>\n",
              "      <td>0.803598</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804586</td>\n",
              "      <td>0.802293</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.154833</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.093018</td>\n",
              "      <td>-0.083240</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.099873</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.802293</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.802293</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.802293</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.801952</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.807107</td>\n",
              "      <td>0.803305</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803861</td>\n",
              "      <td>0.804914</td>\n",
              "      <td>0.804615</td>\n",
              "      <td>0.801952</td>\n",
              "      <td>-0.109457</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.195917</td>\n",
              "      <td>-0.156700</td>\n",
              "      <td>-0.132452</td>\n",
              "      <td>-0.095824</td>\n",
              "      <td>-0.086605</td>\n",
              "      <td>-0.065567</td>\n",
              "      <td>-0.125088</td>\n",
              "      <td>-0.101107</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27 rows × 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdae801d-d665-4080-b2a8-a40339327471')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cdae801d-d665-4080-b2a8-a40339327471 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cdae801d-d665-4080-b2a8-a40339327471');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def formula(acc, gen, equ, odds, diff):\n",
        "\n",
        "    return (acc + gen)/ 1 - (equ + odds + diff) #The algirthm to select the model\n",
        "\n",
        "df = task1_df\n",
        "\n",
        "accuacy = df['Accuracy'] #get only the wanted information\n",
        "general = df['general']\n",
        "equility = df['ep_opp_avg']\n",
        "avg_odds = df['avg_odds_avg']\n",
        "par_diff = df['par_diff_avg']\n",
        "\n",
        "\n",
        "C = df['C']             #get the paramters of the model\n",
        "act = df['Activation']\n",
        "\n",
        "\n",
        "answers = 0\n",
        "\n",
        "for i in range(len(accuacy)):\n",
        "    \n",
        "    ans = formula(accuacy[i], general[i], equility[i], avg_odds[i], par_diff[i]) #loop through varibales and find answer\n",
        "    \n",
        "    if ans > answers:             #if it is th best the save the score and variables\n",
        "        answers = ans\n",
        "        bestC = C[i]\n",
        "        bestActivation = act[i]\n",
        "        \n",
        "print(bestC, bestActivation, answers) #print the best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYvFisNiRv-B",
        "outputId": "200c5fdb-aff3-4b64-9e19-5a465ac61d4d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001 newton-cg 1.97171687673577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 Most Accurate Model**\n",
        "\n",
        "Using the test data which was split from train before the cross validation."
      ],
      "metadata": {
        "id": "8quPShZiO0kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solv = 'lbfgs' #set the paramters for this model\n",
        "c = 0.1\n",
        "\n",
        "\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)    #create the classifier\n",
        "learner.fit(X_train,y_train)                                      #fit the classifier\n",
        "predictions = learner.predict(X_test)                             #make predictions\n",
        "\n",
        "\n",
        "conclude = zip(predictions, y_test)                               #Join both  lists together\n",
        "\n",
        "score = 0                                                         \n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)             #normalise the correct answer by putting into a normal python list\n",
        "\n",
        "lists = zip(predictions, correct_ans)   #zip the predictions and correct answer\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:                   #loop through to see if the prediction is correct\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1         #if correct add one to score\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var)) #round the predictions\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)      #Workout the score\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)        #make the matrix using sckit learn\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False) #use sea born to make it look good\n",
        "\n",
        "test_pred = test.copy() \n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "anV2l-5qOz4X",
        "outputId": "3c7468a6-bfb8-4e7a-89c0-42906cef1560"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.8027707636661434\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.8027707636661434\n",
            "{'stat_par_diff': -0.10360858653908399, 'eq_opp_diff': -0.18414551156873185, 'avg_odds_diff': -0.11698098738262834, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 Most Fair Model**"
      ],
      "metadata": {
        "id": "9NtW14kyO5t8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "solv = 'newton-cg'\n",
        "c = 0.0001\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train)\n",
        "predictions = learner.predict(X_test)\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "0-2dO9dgO8rl",
        "outputId": "174744f9-ab3b-4a41-f021-d469711923a3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.7648945608407834\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.7648945608407834\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3 Select the model (5)**"
      ],
      "metadata": {
        "id": "9rkISCMNq4--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solv = 'liblinear'\n",
        "c = 0.01\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train)\n",
        "predictions = learner.predict(X_test)\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "M-S4BbxDq5sA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "069edeb7-d7c7-4b2f-e5bc-4d3243dfdc99"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.8027707636661434\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.8027707636661434\n",
            "{'stat_par_diff': -0.10360858653908399, 'eq_opp_diff': -0.18414551156873185, 'avg_odds_diff': -0.11698098738262834, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 Reweighting and applying 5 Fold Cross Validation**"
      ],
      "metadata": {
        "id": "jtRLhAL-KScS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "                privileged_groups=privileged_groups)        #create the weights for the bias\n",
        "\n",
        "\n",
        "\n",
        "train_fair = RW.fit_transform(train)            #find the rewrights for the whole train and tests datasets\n",
        "test_fair = RW.fit_transform(test)\n",
        "trainA_fair = RW.fit_transform(trainA.copy())   #Reweight all the 5 fold train and tests\n",
        "trainB_fair = RW.fit_transform(trainB.copy())\n",
        "trainC_fair = RW.fit_transform(trainC.copy())\n",
        "trainD_fair = RW.fit_transform(trainD.copy())\n",
        "trainE_fair = RW.fit_transform(trainE.copy())\n",
        "\n",
        "\n",
        "train_weights = [trainA_fair.instance_weights, trainB_fair.instance_weights, trainC_fair.instance_weights, trainD_fair.instance_weights, trainE_fair.instance_weights] #Put the weights into the lists\n",
        "\n",
        "print(\"subgroup weights\", np.unique(trainA_fair.instance_weights), np.unique(trainB_fair.instance_weights), np.unique(trainC_fair.instance_weights), np.unique(trainD_fair.instance_weights), np.unique(trainE_fair.instance_weights))\n",
        "train_cross_fair = [trainA_fair, trainB_fair, trainC_fair, trainD_fair, trainE_fair] #put all fair data into the into one list to make it easy to access\n",
        "\n",
        "X_train_cross_fair = train_cross_fair #set the fair list\n",
        "Y_train_cross_fair = [0] * 5\n",
        "\n",
        "scale_orig = StandardScaler()   #setup the scaler object\n",
        "\n",
        "for i in range(len(train_cross_fair)):\n",
        "    Y_train_cross_fair[i] = train_cross_fair[i].labels.ravel()  #set the y_train labels\n",
        "\n",
        "for i in range(len(train_cross)):\n",
        "    X_train_cross_fair[i] = scale_orig.fit_transform(train_cross_fair[i].features) #scale both features and labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0FlW-WvKYqB",
        "outputId": "c72f235f-e9a7-49b9-8e1b-0db3fc0aad75"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subgroup weights [0.89225677 0.94194657 1.02056758 1.58549938] [0.89584244 0.94105597 1.02044191 1.57118663] [0.89468241 0.93911547 1.02112205 1.58464763] [0.91383276 0.95097279 1.0162828  1.43556491] [0.90541708 0.94798902 1.01739999 1.50402742]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 Reweighting**"
      ],
      "metadata": {
        "id": "Y504aJflY8Qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "                privileged_groups=privileged_groups)        #create the weights for the bias\n",
        "\n",
        "\n",
        "\n",
        "train_fair = RW.fit_transform(train)            #find the rewrights for the whole train and tests datasets\n",
        "test_fair = RW.fit_transform(test)\n",
        "trainA_fair = RW.fit_transform(trainA.copy())   #Reweight all the 5 fold train and tests\n",
        "trainB_fair = RW.fit_transform(trainB.copy())\n",
        "trainC_fair = RW.fit_transform(trainC.copy())\n",
        "trainD_fair = RW.fit_transform(trainD.copy())\n",
        "trainE_fair = RW.fit_transform(trainE.copy())\n",
        "\n",
        "\n",
        "train_weights = [trainA_fair.instance_weights, trainB_fair.instance_weights, trainC_fair.instance_weights, trainD_fair.instance_weights, trainE_fair.instance_weights] #Put the weights into the lists\n",
        "\n",
        "print(\"subgroup weights\", np.unique(trainA_fair.instance_weights), np.unique(trainB_fair.instance_weights), np.unique(trainC_fair.instance_weights), np.unique(trainD_fair.instance_weights), np.unique(trainE_fair.instance_weights))\n",
        "train_cross_fair = [trainA_fair, trainB_fair, trainC_fair, trainD_fair, trainE_fair] #put all fair data into the into one list to make it easy to access\n",
        "\n",
        "X_train_cross_fair = train_cross_fair #set the fair list\n",
        "Y_train_cross_fair = [0] * 5\n",
        "\n",
        "scale_orig = StandardScaler()   #setup the scaler object\n",
        "\n",
        "for i in range(len(train_cross_fair)):\n",
        "    Y_train_cross_fair[i] = train_cross_fair[i].labels.ravel()  #set the y_train labels\n",
        "\n",
        "for i in range(len(train_cross)):\n",
        "    X_train_cross_fair[i] = scale_orig.fit_transform(train_cross_fair[i].features) #scale both features and labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v24OxhZgY7W-",
        "outputId": "da2977c5-ceab-4ab3-883e-6839d2c62b8f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "subgroup weights [0.89225677 0.94194657 1.02056758 1.58549938] [0.89584244 0.94105597 1.02044191 1.57118663] [0.89468241 0.93911547 1.02112205 1.58464763] [0.91383276 0.95097279 1.0162828  1.43556491] [0.90541708 0.94798902 1.01739999 1.50402742]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Cs = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]    #set the hyperparameters\n",
        "scores = []\n",
        "\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']       #set the actionation functions\n",
        "\n",
        "with open('task2crossvalResults.csv', 'w') as csvfile:\n",
        "    field_names = ['Activation', 'C' , 'crossVal1', 'crossVal2', 'crossVal3', 'crossVal4', 'crossVal5','Accuracy', 'general',\n",
        "                   'Par_diff1', 'Par_diff2', 'Par_diff3', 'Par_diff4', 'Par_diff5', 'par_diff_avg',\n",
        "                   'ep_opp1', 'ep_opp2','ep_opp3', 'ep_opp4', 'ep_opp5', 'ep_opp_avg',\n",
        "                   'avg_odd1', 'avg_odd2', 'avg_odd3', 'avg_odd4', 'avg_odd5','avg_odds_avg' ,'TPR', 'FPR'] #set the field names for the .csv\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
        "    writer.writeheader()\n",
        "\n",
        "\n",
        "\n",
        "    for solv in solvers: #loop through the solves\n",
        "\n",
        "          for c in Cs:   #loop through c\n",
        "\n",
        "              data = {}\n",
        "              accuracies = [] #set the inital lists\n",
        "              cs = []\n",
        "              activations = []\n",
        "              par_diff = []\n",
        "              ep_opp = []\n",
        "              avg_odds = []\n",
        "              TPR = []\n",
        "              FPR = []\n",
        "\n",
        "              classifier = LogisticRegression(C=c, solver=solv) #create the classifiers\n",
        "\n",
        "              for i in range(len(train_cross)): #loop through the cross val\n",
        "\n",
        "                  X_test_cross_fair = [] #reset the epoch lists\n",
        "                  Y_test_cross_fair = []\n",
        "                  indexs = []\n",
        "                  ind = []\n",
        "                  X = []\n",
        "                  Y = []\n",
        "\n",
        "                  for j in range(len(train_cross)): #loop through\n",
        "                      if i == j:\n",
        "                          X_test_cross_fair = X_train_cross_fair[j] #if the i and j are the same this index of training is the testing\n",
        "                          Y_test_cross_fair = Y_train_cross_fair[j]\n",
        "                          ind.append(j)\n",
        "                      else:\n",
        "                          indexs.append(j)\n",
        "\n",
        "\n",
        "                  X = np.concatenate([X_train_cross_fair[indexs[0]], X_train_cross_fair[indexs[1]], X_train_cross_fair[indexs[2]]]) #join all the training and not test\n",
        "                  Y = np.concatenate([Y_train_cross_fair[indexs[0]], Y_train_cross_fair[indexs[1]], Y_train_cross_fair[indexs[2]]])\n",
        "                  X = np.concatenate([X, X_train_cross_fair[indexs[3]]])\n",
        "                  Y = np.concatenate([Y, Y_train_cross_fair[indexs[3]]])\n",
        "                  bias_weights = np.concatenate([train_weights[indexs[0]], train_weights[indexs[1]], train_weights[indexs[2]]])\n",
        "                  bias_weights = np.concatenate([bias_weights, train_weights[indexs[3]]])\n",
        "\n",
        "\n",
        "                  #LR = classifier.fit(X, Y, sample_weight=train_cross_fair_weight.instance_weights) #,sample_weight=train.instance_weights fit all the data that is not in the test\n",
        "\n",
        "                  LR = classifier.fit(X, Y, sample_weight=bias_weights)   #Fit the data the biased X and Y\n",
        "\n",
        "\n",
        "\n",
        "                  predictions = LR.predict(X_test_cross_fair) #find the predictions from model\n",
        "                  \n",
        "                  acc = find_score(predictions, Y_test_cross_fair) #find the accuracy\n",
        "\n",
        "                  #train_copy = trainA\n",
        "\n",
        "                  #Set the traincopy to the train index data\n",
        "                  if i == 0:\n",
        "                    train_copy = trainA_fair\n",
        "\n",
        "                  elif i == 1:\n",
        "                    train_copy = trainB_fair\n",
        "                  \n",
        "                  elif i == 2:\n",
        "                    train_copy = trainC_fair\n",
        "\n",
        "                  elif i == 3:\n",
        "                    train_copy = trainD_fair\n",
        "                  \n",
        "                  elif i == 4:\n",
        "                    train_copy = trainE_fair\n",
        "\n",
        "                  else:\n",
        "                    print(\"Error\")\n",
        "\n",
        "                  test_pred = train_copy.copy()             #create the same\n",
        "                  predictions.resize((len(predictions),1))  #resize prediction\n",
        "                  test_pred.labels = predictions            #set the labels to the \n",
        "\n",
        "                  metric = ClassificationMetric(train_copy, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "                  metric_arrs = {}\n",
        "                  metric_arrs['stat_par_diff']=(metric.statistical_parity_difference()) #add all the data into the matrix\n",
        "                  metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "                  metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "                  #metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "                  #metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "                  metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "                  metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "                  print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, acc))\n",
        "                  print(metric_arrs)\n",
        "                  print('\\n')\n",
        "\n",
        "\n",
        "                  accuracies.append(acc) #store all the data from the epoch\n",
        "                  cs.append(c)\n",
        "                  activations.append(solv)\n",
        "                  par_diff.append((metric.statistical_parity_difference()))\n",
        "                  ep_opp.append((metric.equal_opportunity_difference()))\n",
        "                  avg_odds.append((metric.average_odds_difference()))\n",
        "                  TPR.append((metric.generalized_true_positive_rate()))\n",
        "                  FPR.append((metric.num_generalized_false_positives()))\n",
        "\n",
        "\n",
        "              print(\"Accuracies: \", accuracies)\n",
        "              print(\"Cs\", c)\n",
        "              print(\"Activation: \", activations)\n",
        "              print(\"Par_diff: \", par_diff)\n",
        "              print(\"ep_opp: \", ep_opp)\n",
        "              print(\"avg_odd: \", avg_odds)\n",
        "              print(\"TPR: \", TPR)\n",
        "              print(\"FPR: \", FPR)\n",
        "\n",
        "              pred_test = LR.predict(X_test)\n",
        "              test_acc = find_score(pred_test, y_test)\n",
        "              print(\"Generalisability --- \", test_acc)\n",
        "\n",
        "              acc = sum(accuracies)/len(accuracies)       #find the averages within the data\n",
        "              par_diff_avg = sum(par_diff)/len(par_diff)\n",
        "              ep_opp_avg = sum(ep_opp)/len(ep_opp)\n",
        "              avg_odds_avg = sum(avg_odds)/len(avg_odds)\n",
        "\n",
        "              writer.writerow({'Activation': solv, 'C': c , 'crossVal1': accuracies[0], 'crossVal2': accuracies[1], 'crossVal3': accuracies[2], 'crossVal4': accuracies[3], 'crossVal5': accuracies[4],'Accuracy': acc, 'general': test_acc,\n",
        "                               'Par_diff1': par_diff[0], 'Par_diff2': par_diff[1], 'Par_diff3': par_diff[2], 'Par_diff4': par_diff[3], 'Par_diff5': par_diff[4], 'par_diff_avg': par_diff_avg,\n",
        "                   'ep_opp1': ep_opp[0], 'ep_opp2': ep_opp[1],'ep_opp3': ep_opp[2], 'ep_opp4': ep_opp[3], 'ep_opp5': ep_opp[4], 'ep_opp_avg': ep_opp_avg,\n",
        "                   'avg_odd1': avg_odds[0], 'avg_odd2': avg_odds[1], 'avg_odd3': avg_odds[2], 'avg_odd4': avg_odds[3], 'avg_odd5': avg_odds[4],'avg_odds_avg': avg_odds_avg , 'TPR': TPR[0], 'FPR': FPR[0]}) #write the data in the .csv\n",
        "\n",
        "          \n",
        "files.download('task2crossvalResults.csv')   "
      ],
      "metadata": {
        "id": "7KQf3teNKY5p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b67224d0-cd79-4960-e088-b657e674c644"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.7535829189821586\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.7535829189821586\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7576776835331969\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.7576776835331969\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7581164083065224\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.7581164083065224\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7629077080590903\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.7629077080590903\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7623574144486692\n",
            "C : 1e-05 | Activation: newton-cg | Accuracy: 0.7623574144486692\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.7535829189821586, 0.7576776835331969, 0.7581164083065224, 0.7629077080590903, 0.7623574144486692]\n",
            "Cs 1e-05\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7648945608407834\n",
            "Generalisability ---  0.7648945608407834\n",
            "Model Accuracy 0.7535829189821586\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.7535829189821586\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7576776835331969\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.7576776835331969\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7581164083065224\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.7581164083065224\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7629077080590903\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.7629077080590903\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7623574144486692\n",
            "C : 0.0001 | Activation: newton-cg | Accuracy: 0.7623574144486692\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.7535829189821586, 0.7576776835331969, 0.7581164083065224, 0.7629077080590903, 0.7623574144486692]\n",
            "Cs 0.0001\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7648945608407834\n",
            "Generalisability ---  0.7648945608407834\n",
            "Model Accuracy 0.8078385492834163\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.8078385492834163\n",
            "{'stat_par_diff': -0.018712545845276984, 'eq_opp_diff': -0.04906100868609875, 'avg_odds_diff': -0.02892489487807049, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8025738520035098\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.8025738520035098\n",
            "{'stat_par_diff': 0.0006643897209295535, 'eq_opp_diff': 0.004733473648039166, 'avg_odds_diff': 0.0020482395336234097, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8021351272301843\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.8021351272301843\n",
            "{'stat_par_diff': -0.015488983515189875, 'eq_opp_diff': -0.030878778542329843, 'avg_odds_diff': -0.02072875747794101, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8023987128857686\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.8023987128857686\n",
            "{'stat_par_diff': -0.007597006913597215, 'eq_opp_diff': 0.03020322413244736, 'avg_odds_diff': 0.005429433904130022, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8047674758701374\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.8047674758701374\n",
            "{'stat_par_diff': -0.011885200211798869, 'eq_opp_diff': 0.001402436845678423, 'avg_odds_diff': -0.007312397433913896, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8078385492834163, 0.8025738520035098, 0.8021351272301843, 0.8023987128857686, 0.8047674758701374]\n",
            "Cs 0.001\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.018712545845276984, 0.0006643897209295535, -0.015488983515189875, -0.007597006913597215, -0.011885200211798869]\n",
            "ep_opp:  [-0.04906100868609875, 0.004733473648039166, -0.030878778542329843, 0.03020322413244736, 0.001402436845678423]\n",
            "avg_odd:  [-0.02892489487807049, 0.0020482395336234097, -0.02072875747794101, 0.005429433904130022, -0.007312397433913896]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8021565549716781\n",
            "Generalisability ---  0.8021565549716781\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 0.01 | Activation: newton-cg | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 0.01 | Activation: newton-cg | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8015501608657503\n",
            "C : 0.01 | Activation: newton-cg | Accuracy: 0.8015501608657503\n",
            "{'stat_par_diff': -0.0071353732011316395, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010854297834428234, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 0.01 | Activation: newton-cg | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8044749926879204\n",
            "C : 0.01 | Activation: newton-cg | Accuracy: 0.8044749926879204\n",
            "{'stat_par_diff': -0.007498216169521649, 'eq_opp_diff': 0.007981384214099496, 'avg_odds_diff': -0.002171071897851505, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8015501608657503, 0.8028375018282873, 0.8044749926879204]\n",
            "Cs 0.01\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.0071353732011316395, -0.005373036793145491, -0.007498216169521649]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, 0.007981384214099496]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010854297834428234, 0.008960890791294522, -0.002171071897851505]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8018835733296936\n",
            "Generalisability ---  0.8018835733296936\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 0.1 | Activation: newton-cg | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 0.1 | Activation: newton-cg | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 0.1 | Activation: newton-cg | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.802252449904929\n",
            "C : 0.1 | Activation: newton-cg | Accuracy: 0.802252449904929\n",
            "{'stat_par_diff': -0.0016919380811595552, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.011373436026517674, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8037437847323778\n",
            "C : 0.1 | Activation: newton-cg | Accuracy: 0.8037437847323778\n",
            "{'stat_par_diff': -0.011039760093507572, 'eq_opp_diff': 0.0025502912066317496, 'avg_odds_diff': -0.006362884583733908, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.802252449904929, 0.8037437847323778]\n",
            "Cs 0.1\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.0016919380811595552, -0.011039760093507572]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, 0.0025502912066317496]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.011373436026517674, -0.006362884583733908]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8018835733296936\n",
            "Generalisability ---  0.8018835733296936\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 1 | Activation: newton-cg | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 1 | Activation: newton-cg | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8011114360924246\n",
            "C : 1 | Activation: newton-cg | Accuracy: 0.8011114360924246\n",
            "{'stat_par_diff': -0.002083144939822479, 'eq_opp_diff': -0.01164800931156057, 'avg_odds_diff': -0.005339700807129123, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.802252449904929\n",
            "C : 1 | Activation: newton-cg | Accuracy: 0.802252449904929\n",
            "{'stat_par_diff': -0.0016919380811595552, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.011373436026517674, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8037437847323778\n",
            "C : 1 | Activation: newton-cg | Accuracy: 0.8037437847323778\n",
            "{'stat_par_diff': -0.011039760093507572, 'eq_opp_diff': 0.0025502912066317496, 'avg_odds_diff': -0.006362884583733908, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8011114360924246, 0.802252449904929, 0.8037437847323778]\n",
            "Cs 1\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.002083144939822479, -0.0016919380811595552, -0.011039760093507572]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.01164800931156057, 0.03622126989972735, 0.0025502912066317496]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.005339700807129123, 0.011373436026517674, -0.006362884583733908]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8040362679145949\n",
            "C : 10 | Activation: newton-cg | Accuracy: 0.8040362679145949\n",
            "{'stat_par_diff': -0.013034811681751052, 'eq_opp_diff': -0.0022019151749025356, 'avg_odds_diff': -0.009306782459942799, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.8028375018282873, 0.8040362679145949]\n",
            "Cs 10\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.005373036793145491, -0.013034811681751052]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, -0.0022019151749025356]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.008960890791294522, -0.009306782459942799]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 100 | Activation: newton-cg | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 100 | Activation: newton-cg | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 100 | Activation: newton-cg | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 100 | Activation: newton-cg | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8040362679145949\n",
            "C : 100 | Activation: newton-cg | Accuracy: 0.8040362679145949\n",
            "{'stat_par_diff': -0.013034811681751052, 'eq_opp_diff': -0.0022019151749025356, 'avg_odds_diff': -0.009306782459942799, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.8028375018282873, 0.8040362679145949]\n",
            "Cs 100\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.005373036793145491, -0.013034811681751052]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, -0.0022019151749025356]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.008960890791294522, -0.009306782459942799]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 1000 | Activation: newton-cg | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 1000 | Activation: newton-cg | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 1000 | Activation: newton-cg | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 1000 | Activation: newton-cg | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8040362679145949\n",
            "C : 1000 | Activation: newton-cg | Accuracy: 0.8040362679145949\n",
            "{'stat_par_diff': -0.013034811681751052, 'eq_opp_diff': -0.0022019151749025356, 'avg_odds_diff': -0.009306782459942799, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.8028375018282873, 0.8040362679145949]\n",
            "Cs 1000\n",
            "Activation:  ['newton-cg', 'newton-cg', 'newton-cg', 'newton-cg', 'newton-cg']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.005373036793145491, -0.013034811681751052]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, -0.0022019151749025356]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.008960890791294522, -0.009306782459942799]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.7535829189821586\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.7535829189821586\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7576776835331969\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.7576776835331969\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7581164083065224\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.7581164083065224\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7629077080590903\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.7629077080590903\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7623574144486692\n",
            "C : 1e-05 | Activation: lbfgs | Accuracy: 0.7623574144486692\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.7535829189821586, 0.7576776835331969, 0.7581164083065224, 0.7629077080590903, 0.7623574144486692]\n",
            "Cs 1e-05\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7648945608407834\n",
            "Generalisability ---  0.7648945608407834\n",
            "Model Accuracy 0.7535829189821586\n",
            "C : 0.0001 | Activation: lbfgs | Accuracy: 0.7535829189821586\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7576776835331969\n",
            "C : 0.0001 | Activation: lbfgs | Accuracy: 0.7576776835331969\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7581164083065224\n",
            "C : 0.0001 | Activation: lbfgs | Accuracy: 0.7581164083065224\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7629077080590903\n",
            "C : 0.0001 | Activation: lbfgs | Accuracy: 0.7629077080590903\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7623574144486692\n",
            "C : 0.0001 | Activation: lbfgs | Accuracy: 0.7623574144486692\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.7535829189821586, 0.7576776835331969, 0.7581164083065224, 0.7629077080590903, 0.7623574144486692]\n",
            "Cs 0.0001\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "ep_opp:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "avg_odd:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7648945608407834\n",
            "Generalisability ---  0.7648945608407834\n",
            "Model Accuracy 0.8078385492834163\n",
            "C : 0.001 | Activation: lbfgs | Accuracy: 0.8078385492834163\n",
            "{'stat_par_diff': -0.018712545845276984, 'eq_opp_diff': -0.04906100868609875, 'avg_odds_diff': -0.02892489487807049, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8025738520035098\n",
            "C : 0.001 | Activation: lbfgs | Accuracy: 0.8025738520035098\n",
            "{'stat_par_diff': 0.0006643897209295535, 'eq_opp_diff': 0.004733473648039166, 'avg_odds_diff': 0.0020482395336234097, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8021351272301843\n",
            "C : 0.001 | Activation: lbfgs | Accuracy: 0.8021351272301843\n",
            "{'stat_par_diff': -0.015488983515189875, 'eq_opp_diff': -0.030878778542329843, 'avg_odds_diff': -0.02072875747794101, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8023987128857686\n",
            "C : 0.001 | Activation: lbfgs | Accuracy: 0.8023987128857686\n",
            "{'stat_par_diff': -0.007597006913597215, 'eq_opp_diff': 0.03020322413244736, 'avg_odds_diff': 0.005429433904130022, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8047674758701374\n",
            "C : 0.001 | Activation: lbfgs | Accuracy: 0.8047674758701374\n",
            "{'stat_par_diff': -0.011885200211798869, 'eq_opp_diff': 0.001402436845678423, 'avg_odds_diff': -0.007312397433913896, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8078385492834163, 0.8025738520035098, 0.8021351272301843, 0.8023987128857686, 0.8047674758701374]\n",
            "Cs 0.001\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.018712545845276984, 0.0006643897209295535, -0.015488983515189875, -0.007597006913597215, -0.011885200211798869]\n",
            "ep_opp:  [-0.04906100868609875, 0.004733473648039166, -0.030878778542329843, 0.03020322413244736, 0.001402436845678423]\n",
            "avg_odd:  [-0.02892489487807049, 0.0020482395336234097, -0.02072875747794101, 0.005429433904130022, -0.007312397433913896]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8021565549716781\n",
            "Generalisability ---  0.8021565549716781\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 0.01 | Activation: lbfgs | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 0.01 | Activation: lbfgs | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8015501608657503\n",
            "C : 0.01 | Activation: lbfgs | Accuracy: 0.8015501608657503\n",
            "{'stat_par_diff': -0.0071353732011316395, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010854297834428234, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 0.01 | Activation: lbfgs | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8044749926879204\n",
            "C : 0.01 | Activation: lbfgs | Accuracy: 0.8044749926879204\n",
            "{'stat_par_diff': -0.007498216169521649, 'eq_opp_diff': 0.007981384214099496, 'avg_odds_diff': -0.002171071897851505, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8015501608657503, 0.8028375018282873, 0.8044749926879204]\n",
            "Cs 0.01\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.0071353732011316395, -0.005373036793145491, -0.007498216169521649]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, 0.007981384214099496]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010854297834428234, 0.008960890791294522, -0.002171071897851505]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8018835733296936\n",
            "Generalisability ---  0.8018835733296936\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.802252449904929\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.802252449904929\n",
            "{'stat_par_diff': -0.0016919380811595552, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.011373436026517674, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8037437847323778\n",
            "C : 0.1 | Activation: lbfgs | Accuracy: 0.8037437847323778\n",
            "{'stat_par_diff': -0.011039760093507572, 'eq_opp_diff': 0.0025502912066317496, 'avg_odds_diff': -0.006362884583733908, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.802252449904929, 0.8037437847323778]\n",
            "Cs 0.1\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.0016919380811595552, -0.011039760093507572]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, 0.0025502912066317496]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.011373436026517674, -0.006362884583733908]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8018835733296936\n",
            "Generalisability ---  0.8018835733296936\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 1 | Activation: lbfgs | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 1 | Activation: lbfgs | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8011114360924246\n",
            "C : 1 | Activation: lbfgs | Accuracy: 0.8011114360924246\n",
            "{'stat_par_diff': -0.002083144939822479, 'eq_opp_diff': -0.01164800931156057, 'avg_odds_diff': -0.005339700807129123, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.802252449904929\n",
            "C : 1 | Activation: lbfgs | Accuracy: 0.802252449904929\n",
            "{'stat_par_diff': -0.0016919380811595552, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.011373436026517674, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8037437847323778\n",
            "C : 1 | Activation: lbfgs | Accuracy: 0.8037437847323778\n",
            "{'stat_par_diff': -0.011039760093507572, 'eq_opp_diff': 0.0025502912066317496, 'avg_odds_diff': -0.006362884583733908, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8011114360924246, 0.802252449904929, 0.8037437847323778]\n",
            "Cs 1\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.002083144939822479, -0.0016919380811595552, -0.011039760093507572]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.01164800931156057, 0.03622126989972735, 0.0025502912066317496]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.005339700807129123, 0.011373436026517674, -0.006362884583733908]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8011114360924246\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.8011114360924246\n",
            "{'stat_par_diff': -0.002083144939822479, 'eq_opp_diff': -0.01164800931156057, 'avg_odds_diff': -0.005339700807129123, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8040362679145949\n",
            "C : 10 | Activation: lbfgs | Accuracy: 0.8040362679145949\n",
            "{'stat_par_diff': -0.013034811681751052, 'eq_opp_diff': -0.0022019151749025356, 'avg_odds_diff': -0.009306782459942799, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8011114360924246, 0.8028375018282873, 0.8040362679145949]\n",
            "Cs 10\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.002083144939822479, -0.005373036793145491, -0.013034811681751052]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.01164800931156057, 0.03622126989972735, -0.0022019151749025356]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.005339700807129123, 0.008960890791294522, -0.009306782459942799]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 100 | Activation: lbfgs | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 100 | Activation: lbfgs | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 100 | Activation: lbfgs | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 100 | Activation: lbfgs | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8040362679145949\n",
            "C : 100 | Activation: lbfgs | Accuracy: 0.8040362679145949\n",
            "{'stat_par_diff': -0.013034811681751052, 'eq_opp_diff': -0.0022019151749025356, 'avg_odds_diff': -0.009306782459942799, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.8028375018282873, 0.8040362679145949]\n",
            "Cs 100\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.005373036793145491, -0.013034811681751052]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, -0.0022019151749025356]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.008960890791294522, -0.009306782459942799]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 1000 | Activation: lbfgs | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 1000 | Activation: lbfgs | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 1000 | Activation: lbfgs | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 1000 | Activation: lbfgs | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8040362679145949\n",
            "C : 1000 | Activation: lbfgs | Accuracy: 0.8040362679145949\n",
            "{'stat_par_diff': -0.013034811681751052, 'eq_opp_diff': -0.0022019151749025356, 'avg_odds_diff': -0.009306782459942799, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.8028375018282873, 0.8040362679145949]\n",
            "Cs 1000\n",
            "Activation:  ['lbfgs', 'lbfgs', 'lbfgs', 'lbfgs', 'lbfgs']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.005373036793145491, -0.013034811681751052]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, -0.0022019151749025356]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.008960890791294522, -0.009306782459942799]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.798332845861363\n",
            "C : 1e-05 | Activation: liblinear | Accuracy: 0.798332845861363\n",
            "{'stat_par_diff': -0.00522739800219979, 'eq_opp_diff': -0.0139303738799052, 'avg_odds_diff': -0.008155975563220785, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7964317051769523\n",
            "C : 1e-05 | Activation: liblinear | Accuracy: 0.7964317051769523\n",
            "{'stat_par_diff': 0.004359076240345178, 'eq_opp_diff': 0.02510201451827676, 'avg_odds_diff': 0.01141351693629486, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7954080140391927\n",
            "C : 1e-05 | Activation: liblinear | Accuracy: 0.7954080140391927\n",
            "{'stat_par_diff': 0.0018431023906251842, 'eq_opp_diff': 0.01854609564889953, 'avg_odds_diff': 0.0075299818468084895, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7962556676905075\n",
            "C : 1e-05 | Activation: liblinear | Accuracy: 0.7962556676905075\n",
            "{'stat_par_diff': -0.016366559733273306, 'eq_opp_diff': 0.011274419538979286, 'avg_odds_diff': -0.006841126404693129, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7954080140391927\n",
            "C : 1e-05 | Activation: liblinear | Accuracy: 0.7954080140391927\n",
            "{'stat_par_diff': -0.011403391738145835, 'eq_opp_diff': 0.002179583377996952, 'avg_odds_diff': -0.0067289514238622755, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.798332845861363, 0.7964317051769523, 0.7954080140391927, 0.7962556676905075, 0.7954080140391927]\n",
            "Cs 1e-05\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.00522739800219979, 0.004359076240345178, 0.0018431023906251842, -0.016366559733273306, -0.011403391738145835]\n",
            "ep_opp:  [-0.0139303738799052, 0.02510201451827676, 0.01854609564889953, 0.011274419538979286, 0.002179583377996952]\n",
            "avg_odd:  [-0.008155975563220785, 0.01141351693629486, 0.0075299818468084895, -0.006841126404693129, -0.0067289514238622755]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7951955231010714\n",
            "Generalisability ---  0.7951955231010714\n",
            "Model Accuracy 0.798332845861363\n",
            "C : 0.0001 | Activation: liblinear | Accuracy: 0.798332845861363\n",
            "{'stat_par_diff': -0.00522739800219979, 'eq_opp_diff': -0.0139303738799052, 'avg_odds_diff': -0.008155975563220785, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7954080140391927\n",
            "C : 0.0001 | Activation: liblinear | Accuracy: 0.7954080140391927\n",
            "{'stat_par_diff': 0.007842339916379115, 'eq_opp_diff': 0.03442291997766417, 'avg_odds_diff': 0.016882097119232622, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7954080140391927\n",
            "C : 0.0001 | Activation: liblinear | Accuracy: 0.7954080140391927\n",
            "{'stat_par_diff': 0.0018431023906251842, 'eq_opp_diff': 0.01854609564889953, 'avg_odds_diff': 0.0075299818468084895, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7962556676905075\n",
            "C : 0.0001 | Activation: liblinear | Accuracy: 0.7962556676905075\n",
            "{'stat_par_diff': -0.016366559733273306, 'eq_opp_diff': 0.011274419538979286, 'avg_odds_diff': -0.006841126404693129, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.7954080140391927\n",
            "C : 0.0001 | Activation: liblinear | Accuracy: 0.7954080140391927\n",
            "{'stat_par_diff': -0.011403391738145835, 'eq_opp_diff': 0.002179583377996952, 'avg_odds_diff': -0.0067289514238622755, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.798332845861363, 0.7954080140391927, 0.7954080140391927, 0.7962556676905075, 0.7954080140391927]\n",
            "Cs 0.0001\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.00522739800219979, 0.007842339916379115, 0.0018431023906251842, -0.016366559733273306, -0.011403391738145835]\n",
            "ep_opp:  [-0.0139303738799052, 0.03442291997766417, 0.01854609564889953, 0.011274419538979286, 0.002179583377996952]\n",
            "avg_odd:  [-0.008155975563220785, 0.016882097119232622, 0.0075299818468084895, -0.006841126404693129, -0.0067289514238622755]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.7951955231010714\n",
            "Generalisability ---  0.7951955231010714\n",
            "Model Accuracy 0.8075460661011992\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.8075460661011992\n",
            "{'stat_par_diff': -0.014354396871333108, 'eq_opp_diff': -0.04248206131767768, 'avg_odds_diff': -0.02381944056432, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8025738520035098\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.8025738520035098\n",
            "{'stat_par_diff': 0.0006643897209295535, 'eq_opp_diff': 0.004733473648039166, 'avg_odds_diff': 0.0020482395336234097, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8018426440479672\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.8018426440479672\n",
            "{'stat_par_diff': -0.011312178358160757, 'eq_opp_diff': -0.024468522132073456, 'avg_odds_diff': -0.015791527656184638, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8023987128857686\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.8023987128857686\n",
            "{'stat_par_diff': -0.005231047528643951, 'eq_opp_diff': 0.03630078510805712, 'avg_odds_diff': 0.009081350700740693, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8038900263234864\n",
            "C : 0.001 | Activation: liblinear | Accuracy: 0.8038900263234864\n",
            "{'stat_par_diff': -0.0037334881969356137, 'eq_opp_diff': 0.007981384214099496, 'avg_odds_diff': 0.0002980639046176285, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8075460661011992, 0.8025738520035098, 0.8018426440479672, 0.8023987128857686, 0.8038900263234864]\n",
            "Cs 0.001\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.014354396871333108, 0.0006643897209295535, -0.011312178358160757, -0.005231047528643951, -0.0037334881969356137]\n",
            "ep_opp:  [-0.04248206131767768, 0.004733473648039166, -0.024468522132073456, 0.03630078510805712, 0.007981384214099496]\n",
            "avg_odd:  [-0.02381944056432, 0.0020482395336234097, -0.015791527656184638, 0.009081350700740693, 0.0002980639046176285]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.802088309561182\n",
            "Generalisability ---  0.802088309561182\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.802252449904929\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.802252449904929\n",
            "{'stat_par_diff': -0.0016919380811595552, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.011373436026517674, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8037437847323778\n",
            "C : 0.01 | Activation: liblinear | Accuracy: 0.8037437847323778\n",
            "{'stat_par_diff': -0.011039760093507572, 'eq_opp_diff': 0.0025502912066317496, 'avg_odds_diff': -0.006362884583733908, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.802252449904929, 0.8037437847323778]\n",
            "Cs 0.01\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.0016919380811595552, -0.011039760093507572]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, 0.0025502912066317496]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.011373436026517674, -0.006362884583733908]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8018835733296936\n",
            "Generalisability ---  0.8018835733296936\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 0.1 | Activation: liblinear | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 0.1 | Activation: liblinear | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 0.1 | Activation: liblinear | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.802252449904929\n",
            "C : 0.1 | Activation: liblinear | Accuracy: 0.802252449904929\n",
            "{'stat_par_diff': -0.0016919380811595552, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.011373436026517674, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8037437847323778\n",
            "C : 0.1 | Activation: liblinear | Accuracy: 0.8037437847323778\n",
            "{'stat_par_diff': -0.011039760093507572, 'eq_opp_diff': 0.0025502912066317496, 'avg_odds_diff': -0.006362884583733908, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.802252449904929, 0.8037437847323778]\n",
            "Cs 0.1\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.0016919380811595552, -0.011039760093507572]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, 0.0025502912066317496]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.011373436026517674, -0.006362884583733908]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8018835733296936\n",
            "Generalisability ---  0.8018835733296936\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 1 | Activation: liblinear | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 1 | Activation: liblinear | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8011114360924246\n",
            "C : 1 | Activation: liblinear | Accuracy: 0.8011114360924246\n",
            "{'stat_par_diff': -0.002083144939822479, 'eq_opp_diff': -0.01164800931156057, 'avg_odds_diff': -0.005339700807129123, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.802252449904929\n",
            "C : 1 | Activation: liblinear | Accuracy: 0.802252449904929\n",
            "{'stat_par_diff': -0.0016919380811595552, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.011373436026517674, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8037437847323778\n",
            "C : 1 | Activation: liblinear | Accuracy: 0.8037437847323778\n",
            "{'stat_par_diff': -0.011039760093507572, 'eq_opp_diff': 0.0025502912066317496, 'avg_odds_diff': -0.006362884583733908, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8011114360924246, 0.802252449904929, 0.8037437847323778]\n",
            "Cs 1\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.002083144939822479, -0.0016919380811595552, -0.011039760093507572]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.01164800931156057, 0.03622126989972735, 0.0025502912066317496]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.005339700807129123, 0.011373436026517674, -0.006362884583733908]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 10 | Activation: liblinear | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 10 | Activation: liblinear | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 10 | Activation: liblinear | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 10 | Activation: liblinear | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8040362679145949\n",
            "C : 10 | Activation: liblinear | Accuracy: 0.8040362679145949\n",
            "{'stat_par_diff': -0.013034811681751052, 'eq_opp_diff': -0.0022019151749025356, 'avg_odds_diff': -0.009306782459942799, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.8028375018282873, 0.8040362679145949]\n",
            "Cs 10\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.005373036793145491, -0.013034811681751052]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, -0.0022019151749025356]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.008960890791294522, -0.009306782459942799]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 100 | Activation: liblinear | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 100 | Activation: liblinear | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 100 | Activation: liblinear | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 100 | Activation: liblinear | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8040362679145949\n",
            "C : 100 | Activation: liblinear | Accuracy: 0.8040362679145949\n",
            "{'stat_par_diff': -0.013034811681751052, 'eq_opp_diff': -0.0022019151749025356, 'avg_odds_diff': -0.009306782459942799, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.8028375018282873, 0.8040362679145949]\n",
            "Cs 100\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.005373036793145491, -0.013034811681751052]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, -0.0022019151749025356]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.008960890791294522, -0.009306782459942799]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n",
            "Model Accuracy 0.8068148581456567\n",
            "C : 1000 | Activation: liblinear | Accuracy: 0.8068148581456567\n",
            "{'stat_par_diff': -0.012970429559421626, 'eq_opp_diff': -0.044439008480104236, 'avg_odds_diff': -0.023559701022348778, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8030125767768354\n",
            "C : 1000 | Activation: liblinear | Accuracy: 0.8030125767768354\n",
            "{'stat_par_diff': 0.007809254365595736, 'eq_opp_diff': 0.030539925260942402, 'avg_odds_diff': 0.015539700634192667, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8014039192746417\n",
            "C : 1000 | Activation: liblinear | Accuracy: 0.8014039192746417\n",
            "{'stat_par_diff': -0.006259950096851596, 'eq_opp_diff': -0.018058265721817013, 'avg_odds_diff': -0.010276930628885524, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8028375018282873\n",
            "C : 1000 | Activation: liblinear | Accuracy: 0.8028375018282873\n",
            "{'stat_par_diff': -0.005373036793145491, 'eq_opp_diff': 0.03622126989972735, 'avg_odds_diff': 0.008960890791294522, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Model Accuracy 0.8040362679145949\n",
            "C : 1000 | Activation: liblinear | Accuracy: 0.8040362679145949\n",
            "{'stat_par_diff': -0.013034811681751052, 'eq_opp_diff': -0.0022019151749025356, 'avg_odds_diff': -0.009306782459942799, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n",
            "Accuracies:  [0.8068148581456567, 0.8030125767768354, 0.8014039192746417, 0.8028375018282873, 0.8040362679145949]\n",
            "Cs 1000\n",
            "Activation:  ['liblinear', 'liblinear', 'liblinear', 'liblinear', 'liblinear']\n",
            "Par_diff:  [-0.012970429559421626, 0.007809254365595736, -0.006259950096851596, -0.005373036793145491, -0.013034811681751052]\n",
            "ep_opp:  [-0.044439008480104236, 0.030539925260942402, -0.018058265721817013, 0.03622126989972735, -0.0022019151749025356]\n",
            "avg_odd:  [-0.023559701022348778, 0.015539700634192667, -0.010276930628885524, 0.008960890791294522, -0.009306782459942799]\n",
            "TPR:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "FPR:  [0.0, 0.0, 0.0, 0.0, 0.0]\n",
            "Model Accuracy 0.8014058554562206\n",
            "Generalisability ---  0.8014058554562206\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_eb0e73c8-b812-415c-87d5-6919fc30f7c7\", \"task2crossvalResults.csv\", 13707)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plotting**\n",
        "\n",
        "Plotting the hyper parameters"
      ],
      "metadata": {
        "id": "BBvCtsUpjJ7v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task2_df = pd.read_csv(\"task2crossvalResults.csv\")\n",
        "task2_df"
      ],
      "metadata": {
        "id": "GJmeXMpjjKso",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "outputId": "8e7a0e11-1315-438e-a707-a31a982f398d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Activation           C  crossVal1  crossVal2  crossVal3  crossVal4  \\\n",
              "0   newton-cg     0.00001   0.753583   0.757678   0.758116   0.762908   \n",
              "1   newton-cg     0.00010   0.753583   0.757678   0.758116   0.762908   \n",
              "2   newton-cg     0.00100   0.807839   0.802574   0.802135   0.802399   \n",
              "3   newton-cg     0.01000   0.806815   0.803013   0.801550   0.802838   \n",
              "4   newton-cg     0.10000   0.806815   0.803013   0.801404   0.802252   \n",
              "5   newton-cg     1.00000   0.806815   0.803013   0.801111   0.802252   \n",
              "6   newton-cg    10.00000   0.806815   0.803013   0.801404   0.802838   \n",
              "7   newton-cg   100.00000   0.806815   0.803013   0.801404   0.802838   \n",
              "8   newton-cg  1000.00000   0.806815   0.803013   0.801404   0.802838   \n",
              "9       lbfgs     0.00001   0.753583   0.757678   0.758116   0.762908   \n",
              "10      lbfgs     0.00010   0.753583   0.757678   0.758116   0.762908   \n",
              "11      lbfgs     0.00100   0.807839   0.802574   0.802135   0.802399   \n",
              "12      lbfgs     0.01000   0.806815   0.803013   0.801550   0.802838   \n",
              "13      lbfgs     0.10000   0.806815   0.803013   0.801404   0.802252   \n",
              "14      lbfgs     1.00000   0.806815   0.803013   0.801111   0.802252   \n",
              "15      lbfgs    10.00000   0.806815   0.803013   0.801111   0.802838   \n",
              "16      lbfgs   100.00000   0.806815   0.803013   0.801404   0.802838   \n",
              "17      lbfgs  1000.00000   0.806815   0.803013   0.801404   0.802838   \n",
              "18  liblinear     0.00001   0.798333   0.796432   0.795408   0.796256   \n",
              "19  liblinear     0.00010   0.798333   0.795408   0.795408   0.796256   \n",
              "20  liblinear     0.00100   0.807546   0.802574   0.801843   0.802399   \n",
              "21  liblinear     0.01000   0.806815   0.803013   0.801404   0.802252   \n",
              "22  liblinear     0.10000   0.806815   0.803013   0.801404   0.802252   \n",
              "23  liblinear     1.00000   0.806815   0.803013   0.801111   0.802252   \n",
              "24  liblinear    10.00000   0.806815   0.803013   0.801404   0.802838   \n",
              "25  liblinear   100.00000   0.806815   0.803013   0.801404   0.802838   \n",
              "26  liblinear  1000.00000   0.806815   0.803013   0.801404   0.802838   \n",
              "\n",
              "    crossVal5  Accuracy   general  Par_diff1  ...   ep_opp5  ep_opp_avg  \\\n",
              "0    0.762357  0.758928  0.764895   0.000000  ...  0.000000    0.000000   \n",
              "1    0.762357  0.758928  0.764895   0.000000  ...  0.000000    0.000000   \n",
              "2    0.804767  0.803943  0.802157  -0.018713  ...  0.001402   -0.008720   \n",
              "3    0.804475  0.803738  0.801884  -0.012970  ...  0.007981    0.002449   \n",
              "4    0.803744  0.803446  0.801884  -0.012970  ...  0.002550    0.001363   \n",
              "5    0.803744  0.803387  0.801406  -0.012970  ...  0.002550    0.002645   \n",
              "6    0.804036  0.803621  0.801406  -0.012970  ... -0.002202    0.000412   \n",
              "7    0.804036  0.803621  0.801406  -0.012970  ... -0.002202    0.000412   \n",
              "8    0.804036  0.803621  0.801406  -0.012970  ... -0.002202    0.000412   \n",
              "9    0.762357  0.758928  0.764895   0.000000  ...  0.000000    0.000000   \n",
              "10   0.762357  0.758928  0.764895   0.000000  ...  0.000000    0.000000   \n",
              "11   0.804767  0.803943  0.802157  -0.018713  ...  0.001402   -0.008720   \n",
              "12   0.804475  0.803738  0.801884  -0.012970  ...  0.007981    0.002449   \n",
              "13   0.803744  0.803446  0.801884  -0.012970  ...  0.002550    0.001363   \n",
              "14   0.803744  0.803387  0.801406  -0.012970  ...  0.002550    0.002645   \n",
              "15   0.804036  0.803563  0.801406  -0.012970  ... -0.002202    0.001694   \n",
              "16   0.804036  0.803621  0.801406  -0.012970  ... -0.002202    0.000412   \n",
              "17   0.804036  0.803621  0.801406  -0.012970  ... -0.002202    0.000412   \n",
              "18   0.795408  0.796367  0.795196  -0.005227  ...  0.002180    0.008634   \n",
              "19   0.795408  0.796163  0.795196  -0.005227  ...  0.002180    0.010499   \n",
              "20   0.803890  0.803650  0.802088  -0.014354  ...  0.007981   -0.003587   \n",
              "21   0.803744  0.803446  0.801884  -0.012970  ...  0.002550    0.001363   \n",
              "22   0.803744  0.803446  0.801884  -0.012970  ...  0.002550    0.001363   \n",
              "23   0.803744  0.803387  0.801406  -0.012970  ...  0.002550    0.002645   \n",
              "24   0.804036  0.803621  0.801406  -0.012970  ... -0.002202    0.000412   \n",
              "25   0.804036  0.803621  0.801406  -0.012970  ... -0.002202    0.000412   \n",
              "26   0.804036  0.803621  0.801406  -0.012970  ... -0.002202    0.000412   \n",
              "\n",
              "    avg_odd1  avg_odd2  avg_odd3  avg_odd4  avg_odd5  avg_odds_avg  TPR  FPR  \n",
              "0   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "1   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "2  -0.028925  0.002048 -0.020729  0.005429 -0.007312     -0.009898  1.0  0.0  \n",
              "3  -0.023560  0.015540 -0.010854  0.008961 -0.002171     -0.002417  1.0  0.0  \n",
              "4  -0.023560  0.015540 -0.010277  0.011373 -0.006363     -0.002657  1.0  0.0  \n",
              "5  -0.023560  0.015540 -0.005340  0.011373 -0.006363     -0.001670  1.0  0.0  \n",
              "6  -0.023560  0.015540 -0.010277  0.008961 -0.009307     -0.003729  1.0  0.0  \n",
              "7  -0.023560  0.015540 -0.010277  0.008961 -0.009307     -0.003729  1.0  0.0  \n",
              "8  -0.023560  0.015540 -0.010277  0.008961 -0.009307     -0.003729  1.0  0.0  \n",
              "9   0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "10  0.000000  0.000000  0.000000  0.000000  0.000000      0.000000  1.0  0.0  \n",
              "11 -0.028925  0.002048 -0.020729  0.005429 -0.007312     -0.009898  1.0  0.0  \n",
              "12 -0.023560  0.015540 -0.010854  0.008961 -0.002171     -0.002417  1.0  0.0  \n",
              "13 -0.023560  0.015540 -0.010277  0.011373 -0.006363     -0.002657  1.0  0.0  \n",
              "14 -0.023560  0.015540 -0.005340  0.011373 -0.006363     -0.001670  1.0  0.0  \n",
              "15 -0.023560  0.015540 -0.005340  0.008961 -0.009307     -0.002741  1.0  0.0  \n",
              "16 -0.023560  0.015540 -0.010277  0.008961 -0.009307     -0.003729  1.0  0.0  \n",
              "17 -0.023560  0.015540 -0.010277  0.008961 -0.009307     -0.003729  1.0  0.0  \n",
              "18 -0.008156  0.011414  0.007530 -0.006841 -0.006729     -0.000557  1.0  0.0  \n",
              "19 -0.008156  0.016882  0.007530 -0.006841 -0.006729      0.000537  1.0  0.0  \n",
              "20 -0.023819  0.002048 -0.015792  0.009081  0.000298     -0.005637  1.0  0.0  \n",
              "21 -0.023560  0.015540 -0.010277  0.011373 -0.006363     -0.002657  1.0  0.0  \n",
              "22 -0.023560  0.015540 -0.010277  0.011373 -0.006363     -0.002657  1.0  0.0  \n",
              "23 -0.023560  0.015540 -0.005340  0.011373 -0.006363     -0.001670  1.0  0.0  \n",
              "24 -0.023560  0.015540 -0.010277  0.008961 -0.009307     -0.003729  1.0  0.0  \n",
              "25 -0.023560  0.015540 -0.010277  0.008961 -0.009307     -0.003729  1.0  0.0  \n",
              "26 -0.023560  0.015540 -0.010277  0.008961 -0.009307     -0.003729  1.0  0.0  \n",
              "\n",
              "[27 rows x 29 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3854341-06ac-4414-9cc9-74450de6024a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Activation</th>\n",
              "      <th>C</th>\n",
              "      <th>crossVal1</th>\n",
              "      <th>crossVal2</th>\n",
              "      <th>crossVal3</th>\n",
              "      <th>crossVal4</th>\n",
              "      <th>crossVal5</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>general</th>\n",
              "      <th>Par_diff1</th>\n",
              "      <th>...</th>\n",
              "      <th>ep_opp5</th>\n",
              "      <th>ep_opp_avg</th>\n",
              "      <th>avg_odd1</th>\n",
              "      <th>avg_odd2</th>\n",
              "      <th>avg_odd3</th>\n",
              "      <th>avg_odd4</th>\n",
              "      <th>avg_odd5</th>\n",
              "      <th>avg_odds_avg</th>\n",
              "      <th>TPR</th>\n",
              "      <th>FPR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.753583</td>\n",
              "      <td>0.757678</td>\n",
              "      <td>0.758116</td>\n",
              "      <td>0.762908</td>\n",
              "      <td>0.762357</td>\n",
              "      <td>0.758928</td>\n",
              "      <td>0.764895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.753583</td>\n",
              "      <td>0.757678</td>\n",
              "      <td>0.758116</td>\n",
              "      <td>0.762908</td>\n",
              "      <td>0.762357</td>\n",
              "      <td>0.758928</td>\n",
              "      <td>0.764895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.807839</td>\n",
              "      <td>0.802574</td>\n",
              "      <td>0.802135</td>\n",
              "      <td>0.802399</td>\n",
              "      <td>0.804767</td>\n",
              "      <td>0.803943</td>\n",
              "      <td>0.802157</td>\n",
              "      <td>-0.018713</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>-0.008720</td>\n",
              "      <td>-0.028925</td>\n",
              "      <td>0.002048</td>\n",
              "      <td>-0.020729</td>\n",
              "      <td>0.005429</td>\n",
              "      <td>-0.007312</td>\n",
              "      <td>-0.009898</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801550</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804475</td>\n",
              "      <td>0.803738</td>\n",
              "      <td>0.801884</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007981</td>\n",
              "      <td>0.002449</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010854</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.002171</td>\n",
              "      <td>-0.002417</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802252</td>\n",
              "      <td>0.803744</td>\n",
              "      <td>0.803446</td>\n",
              "      <td>0.801884</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.001363</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.011373</td>\n",
              "      <td>-0.006363</td>\n",
              "      <td>-0.002657</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801111</td>\n",
              "      <td>0.802252</td>\n",
              "      <td>0.803744</td>\n",
              "      <td>0.803387</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.002645</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.005340</td>\n",
              "      <td>0.011373</td>\n",
              "      <td>-0.006363</td>\n",
              "      <td>-0.001670</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804036</td>\n",
              "      <td>0.803621</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002202</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.009307</td>\n",
              "      <td>-0.003729</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804036</td>\n",
              "      <td>0.803621</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002202</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.009307</td>\n",
              "      <td>-0.003729</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>newton-cg</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804036</td>\n",
              "      <td>0.803621</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002202</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.009307</td>\n",
              "      <td>-0.003729</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.753583</td>\n",
              "      <td>0.757678</td>\n",
              "      <td>0.758116</td>\n",
              "      <td>0.762908</td>\n",
              "      <td>0.762357</td>\n",
              "      <td>0.758928</td>\n",
              "      <td>0.764895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.753583</td>\n",
              "      <td>0.757678</td>\n",
              "      <td>0.758116</td>\n",
              "      <td>0.762908</td>\n",
              "      <td>0.762357</td>\n",
              "      <td>0.758928</td>\n",
              "      <td>0.764895</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.807839</td>\n",
              "      <td>0.802574</td>\n",
              "      <td>0.802135</td>\n",
              "      <td>0.802399</td>\n",
              "      <td>0.804767</td>\n",
              "      <td>0.803943</td>\n",
              "      <td>0.802157</td>\n",
              "      <td>-0.018713</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001402</td>\n",
              "      <td>-0.008720</td>\n",
              "      <td>-0.028925</td>\n",
              "      <td>0.002048</td>\n",
              "      <td>-0.020729</td>\n",
              "      <td>0.005429</td>\n",
              "      <td>-0.007312</td>\n",
              "      <td>-0.009898</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801550</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804475</td>\n",
              "      <td>0.803738</td>\n",
              "      <td>0.801884</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007981</td>\n",
              "      <td>0.002449</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010854</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.002171</td>\n",
              "      <td>-0.002417</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802252</td>\n",
              "      <td>0.803744</td>\n",
              "      <td>0.803446</td>\n",
              "      <td>0.801884</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.001363</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.011373</td>\n",
              "      <td>-0.006363</td>\n",
              "      <td>-0.002657</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801111</td>\n",
              "      <td>0.802252</td>\n",
              "      <td>0.803744</td>\n",
              "      <td>0.803387</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.002645</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.005340</td>\n",
              "      <td>0.011373</td>\n",
              "      <td>-0.006363</td>\n",
              "      <td>-0.001670</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801111</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804036</td>\n",
              "      <td>0.803563</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002202</td>\n",
              "      <td>0.001694</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.005340</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.009307</td>\n",
              "      <td>-0.002741</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804036</td>\n",
              "      <td>0.803621</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002202</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.009307</td>\n",
              "      <td>-0.003729</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>lbfgs</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804036</td>\n",
              "      <td>0.803621</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002202</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.009307</td>\n",
              "      <td>-0.003729</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00001</td>\n",
              "      <td>0.798333</td>\n",
              "      <td>0.796432</td>\n",
              "      <td>0.795408</td>\n",
              "      <td>0.796256</td>\n",
              "      <td>0.795408</td>\n",
              "      <td>0.796367</td>\n",
              "      <td>0.795196</td>\n",
              "      <td>-0.005227</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002180</td>\n",
              "      <td>0.008634</td>\n",
              "      <td>-0.008156</td>\n",
              "      <td>0.011414</td>\n",
              "      <td>0.007530</td>\n",
              "      <td>-0.006841</td>\n",
              "      <td>-0.006729</td>\n",
              "      <td>-0.000557</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00010</td>\n",
              "      <td>0.798333</td>\n",
              "      <td>0.795408</td>\n",
              "      <td>0.795408</td>\n",
              "      <td>0.796256</td>\n",
              "      <td>0.795408</td>\n",
              "      <td>0.796163</td>\n",
              "      <td>0.795196</td>\n",
              "      <td>-0.005227</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002180</td>\n",
              "      <td>0.010499</td>\n",
              "      <td>-0.008156</td>\n",
              "      <td>0.016882</td>\n",
              "      <td>0.007530</td>\n",
              "      <td>-0.006841</td>\n",
              "      <td>-0.006729</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.00100</td>\n",
              "      <td>0.807546</td>\n",
              "      <td>0.802574</td>\n",
              "      <td>0.801843</td>\n",
              "      <td>0.802399</td>\n",
              "      <td>0.803890</td>\n",
              "      <td>0.803650</td>\n",
              "      <td>0.802088</td>\n",
              "      <td>-0.014354</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007981</td>\n",
              "      <td>-0.003587</td>\n",
              "      <td>-0.023819</td>\n",
              "      <td>0.002048</td>\n",
              "      <td>-0.015792</td>\n",
              "      <td>0.009081</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>-0.005637</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.01000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802252</td>\n",
              "      <td>0.803744</td>\n",
              "      <td>0.803446</td>\n",
              "      <td>0.801884</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.001363</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.011373</td>\n",
              "      <td>-0.006363</td>\n",
              "      <td>-0.002657</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>0.10000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802252</td>\n",
              "      <td>0.803744</td>\n",
              "      <td>0.803446</td>\n",
              "      <td>0.801884</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.001363</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.011373</td>\n",
              "      <td>-0.006363</td>\n",
              "      <td>-0.002657</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801111</td>\n",
              "      <td>0.802252</td>\n",
              "      <td>0.803744</td>\n",
              "      <td>0.803387</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>0.002645</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.005340</td>\n",
              "      <td>0.011373</td>\n",
              "      <td>-0.006363</td>\n",
              "      <td>-0.001670</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>10.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804036</td>\n",
              "      <td>0.803621</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002202</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.009307</td>\n",
              "      <td>-0.003729</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>100.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804036</td>\n",
              "      <td>0.803621</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002202</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.009307</td>\n",
              "      <td>-0.003729</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>liblinear</td>\n",
              "      <td>1000.00000</td>\n",
              "      <td>0.806815</td>\n",
              "      <td>0.803013</td>\n",
              "      <td>0.801404</td>\n",
              "      <td>0.802838</td>\n",
              "      <td>0.804036</td>\n",
              "      <td>0.803621</td>\n",
              "      <td>0.801406</td>\n",
              "      <td>-0.012970</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.002202</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>-0.023560</td>\n",
              "      <td>0.015540</td>\n",
              "      <td>-0.010277</td>\n",
              "      <td>0.008961</td>\n",
              "      <td>-0.009307</td>\n",
              "      <td>-0.003729</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27 rows × 29 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3854341-06ac-4414-9cc9-74450de6024a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b3854341-06ac-4414-9cc9-74450de6024a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b3854341-06ac-4414-9cc9-74450de6024a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 Reweighted Fair Model**"
      ],
      "metadata": {
        "id": "NfYqN-DEW-aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "solv = 'newton-cg'\n",
        "c = 0.001\n",
        "\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train ,sample_weight=train_fair.instance_weights)\n",
        "predictions = learner.predict(test_fair.features, )\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "YvVqrX0wXEZ-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "d15149ac-e6f5-4547-a2c3-5fc551928c9d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.7648945608407834\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.7648945608407834\n",
            "{'stat_par_diff': 0.0, 'eq_opp_diff': 0.0, 'avg_odds_diff': 0.0, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2 Reweighted Accuracy Model**"
      ],
      "metadata": {
        "id": "rqcIIY1HXE4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "solv = 'lbfgs'\n",
        "c = 1.0\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train ,sample_weight=train_fair.instance_weights)\n",
        "predictions = learner.predict(X_test)\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "m_IUlQnwXRAv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "a9b8da28-b51b-4e46-ee30-375880cfd176"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.8018835733296936\n",
            "C : 1.0 | Activation: lbfgs | Accuracy: 0.8018835733296936\n",
            "{'stat_par_diff': -0.05506828011784737, 'eq_opp_diff': -0.02923672575722991, 'avg_odds_diff': -0.024319104585000455, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3 (EXTRA) Fairness Model 6**"
      ],
      "metadata": {
        "id": "CqT08aPhKZjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formula(acc, gen, equ, odds, diff):\n",
        "\n",
        "    return (acc + gen)/ 1 - (equ + odds + diff) #The algirthm to select the model\n",
        "\n",
        "df = task2_df\n",
        "\n",
        "accuacy = df['Accuracy'] #get only the wanted information\n",
        "general = df['general']\n",
        "equility = df['ep_opp_avg']\n",
        "avg_odds = df['avg_odds_avg']\n",
        "par_diff = df['par_diff_avg']\n",
        "\n",
        "\n",
        "C = df['C']             #get the paramters of the model\n",
        "act = df['Activation']\n",
        "\n",
        "\n",
        "answers = 0\n",
        "\n",
        "for i in range(len(accuacy)):\n",
        "    \n",
        "    ans = formula(accuacy[i], general[i], equility[i], avg_odds[i], par_diff[i]) #loop through varibales and find answer\n",
        "    \n",
        "    if ans > answers:             #if it is th best the save the score and variables\n",
        "        answers = ans\n",
        "        bestC = C[i]\n",
        "        bestActivation = act[i]\n",
        "        \n",
        "print(bestC, bestActivation, answers) #print the best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGae_gCdRp6D",
        "outputId": "f9265608-6d9f-4048-ec87-c38f7e4902ea"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001 newton-cg 1.635320973570155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "solv = 'newton-cg'\n",
        "c = 0.001\n",
        "\n",
        "learner = LogisticRegression(solver=solv, random_state=1, C=c)  \n",
        "learner.fit(X_train,y_train ,sample_weight=train_fair.instance_weights)\n",
        "predictions = learner.predict(X_test)\n",
        "\n",
        "conclude = zip(predictions, y_test)\n",
        "\n",
        "score = 0\n",
        "\n",
        "correct_ans = []\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictions, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy()\n",
        "predictions.resize((len(predictions),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "vcJhtyx_DbcY",
        "outputId": "1dd6f403-6f8f-434e-89d4-59de2961cca8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.8021565549716781\n",
            "C : 0.001 | Activation: newton-cg | Accuracy: 0.8021565549716781\n",
            "{'stat_par_diff': -0.06072865747633795, 'eq_opp_diff': -0.04252576230208044, 'avg_odds_diff': -0.033162633302725335, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Future work**"
      ],
      "metadata": {
        "id": "gJf-NQuSDcAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "privileged_groups = [{'race': 1}]                                           #set the numbers for the privilage and unprivilage for the race\n",
        "unprivileged_groups = [{'race': 0}]\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "\n",
        "\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "learner = LogisticRegression(solver='newton-cg', random_state=1, C=0.01)\n",
        "\n",
        "\n",
        "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "                          unprivileged_groups = unprivileged_groups,\n",
        "                          scope_name='plain_classifier',\n",
        "                          debias=False,\n",
        "                          sess=sess)\n",
        "\n",
        "lis = np.arange(0.0, 1.0, 0.1)\n",
        "\n",
        "accur = []\n",
        "\n",
        "\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "                          unprivileged_groups = unprivileged_groups,\n",
        "                          scope_name='debiased_classifier',\n",
        "                          debias=True,\n",
        "                          sess=sess,\n",
        "                          adversary_loss_weight = 0.1)\n",
        "\n",
        "debiased_model = plain_model.fit(train)\n",
        "\n",
        "preds = debiased_model.predict(test)\n",
        "\n",
        "predictionsAAAAA = preds.labels\n",
        "\n",
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictionsAAAAA, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0][0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0][0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "accur.append(accuracy)\n",
        "sess.close()\n",
        "\n",
        "print(accur)\n",
        "\n"
      ],
      "metadata": {
        "id": "L9JiPl63snKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1daa58b8-6fb1-4d91-e7a1-aaf1a35115cf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "epoch 0; iter: 0; batch classifier loss: 0.752920\n",
            "epoch 0; iter: 200; batch classifier loss: 0.353102\n",
            "epoch 1; iter: 0; batch classifier loss: 0.452746\n",
            "epoch 1; iter: 200; batch classifier loss: 0.352679\n",
            "epoch 2; iter: 0; batch classifier loss: 0.407403\n",
            "epoch 2; iter: 200; batch classifier loss: 0.504181\n",
            "epoch 3; iter: 0; batch classifier loss: 0.380855\n",
            "epoch 3; iter: 200; batch classifier loss: 0.405097\n",
            "epoch 4; iter: 0; batch classifier loss: 0.508605\n",
            "epoch 4; iter: 200; batch classifier loss: 0.476077\n",
            "epoch 5; iter: 0; batch classifier loss: 0.327546\n",
            "epoch 5; iter: 200; batch classifier loss: 0.430850\n",
            "epoch 6; iter: 0; batch classifier loss: 0.471416\n",
            "epoch 6; iter: 200; batch classifier loss: 0.414595\n",
            "epoch 7; iter: 0; batch classifier loss: 0.418324\n",
            "epoch 7; iter: 200; batch classifier loss: 0.385639\n",
            "epoch 8; iter: 0; batch classifier loss: 0.428684\n",
            "epoch 8; iter: 200; batch classifier loss: 0.344325\n",
            "epoch 9; iter: 0; batch classifier loss: 0.463907\n",
            "epoch 9; iter: 200; batch classifier loss: 0.432238\n",
            "epoch 10; iter: 0; batch classifier loss: 0.350888\n",
            "epoch 10; iter: 200; batch classifier loss: 0.351864\n",
            "epoch 11; iter: 0; batch classifier loss: 0.370624\n",
            "epoch 11; iter: 200; batch classifier loss: 0.422641\n",
            "epoch 12; iter: 0; batch classifier loss: 0.458267\n",
            "epoch 12; iter: 200; batch classifier loss: 0.380719\n",
            "epoch 13; iter: 0; batch classifier loss: 0.463908\n",
            "epoch 13; iter: 200; batch classifier loss: 0.357513\n",
            "epoch 14; iter: 0; batch classifier loss: 0.460661\n",
            "epoch 14; iter: 200; batch classifier loss: 0.416244\n",
            "epoch 15; iter: 0; batch classifier loss: 0.422190\n",
            "epoch 15; iter: 200; batch classifier loss: 0.454002\n",
            "epoch 16; iter: 0; batch classifier loss: 0.352307\n",
            "epoch 16; iter: 200; batch classifier loss: 0.450952\n",
            "epoch 17; iter: 0; batch classifier loss: 0.424757\n",
            "epoch 17; iter: 200; batch classifier loss: 0.430935\n",
            "epoch 18; iter: 0; batch classifier loss: 0.465965\n",
            "epoch 18; iter: 200; batch classifier loss: 0.443055\n",
            "epoch 19; iter: 0; batch classifier loss: 0.414844\n",
            "epoch 19; iter: 200; batch classifier loss: 0.423981\n",
            "epoch 20; iter: 0; batch classifier loss: 0.456842\n",
            "epoch 20; iter: 200; batch classifier loss: 0.330504\n",
            "epoch 21; iter: 0; batch classifier loss: 0.355922\n",
            "epoch 21; iter: 200; batch classifier loss: 0.387915\n",
            "epoch 22; iter: 0; batch classifier loss: 0.484709\n",
            "epoch 22; iter: 200; batch classifier loss: 0.468574\n",
            "epoch 23; iter: 0; batch classifier loss: 0.394562\n",
            "epoch 23; iter: 200; batch classifier loss: 0.388551\n",
            "epoch 24; iter: 0; batch classifier loss: 0.398506\n",
            "epoch 24; iter: 200; batch classifier loss: 0.374286\n",
            "epoch 25; iter: 0; batch classifier loss: 0.423917\n",
            "epoch 25; iter: 200; batch classifier loss: 0.448947\n",
            "epoch 26; iter: 0; batch classifier loss: 0.373247\n",
            "epoch 26; iter: 200; batch classifier loss: 0.433107\n",
            "epoch 27; iter: 0; batch classifier loss: 0.392301\n",
            "epoch 27; iter: 200; batch classifier loss: 0.361454\n",
            "epoch 28; iter: 0; batch classifier loss: 0.383522\n",
            "epoch 28; iter: 200; batch classifier loss: 0.413825\n",
            "epoch 29; iter: 0; batch classifier loss: 0.417150\n",
            "epoch 29; iter: 200; batch classifier loss: 0.508576\n",
            "epoch 30; iter: 0; batch classifier loss: 0.415585\n",
            "epoch 30; iter: 200; batch classifier loss: 0.359522\n",
            "epoch 31; iter: 0; batch classifier loss: 0.438393\n",
            "epoch 31; iter: 200; batch classifier loss: 0.407461\n",
            "epoch 32; iter: 0; batch classifier loss: 0.407791\n",
            "epoch 32; iter: 200; batch classifier loss: 0.399452\n",
            "epoch 33; iter: 0; batch classifier loss: 0.400420\n",
            "epoch 33; iter: 200; batch classifier loss: 0.455818\n",
            "epoch 34; iter: 0; batch classifier loss: 0.396292\n",
            "epoch 34; iter: 200; batch classifier loss: 0.384418\n",
            "epoch 35; iter: 0; batch classifier loss: 0.432144\n",
            "epoch 35; iter: 200; batch classifier loss: 0.432717\n",
            "epoch 36; iter: 0; batch classifier loss: 0.414712\n",
            "epoch 36; iter: 200; batch classifier loss: 0.366218\n",
            "epoch 37; iter: 0; batch classifier loss: 0.453058\n",
            "epoch 37; iter: 200; batch classifier loss: 0.419400\n",
            "epoch 38; iter: 0; batch classifier loss: 0.348106\n",
            "epoch 38; iter: 200; batch classifier loss: 0.393904\n",
            "epoch 39; iter: 0; batch classifier loss: 0.434799\n",
            "epoch 39; iter: 200; batch classifier loss: 0.488631\n",
            "epoch 40; iter: 0; batch classifier loss: 0.419486\n",
            "epoch 40; iter: 200; batch classifier loss: 0.455689\n",
            "epoch 41; iter: 0; batch classifier loss: 0.406306\n",
            "epoch 41; iter: 200; batch classifier loss: 0.366026\n",
            "epoch 42; iter: 0; batch classifier loss: 0.440716\n",
            "epoch 42; iter: 200; batch classifier loss: 0.412877\n",
            "epoch 43; iter: 0; batch classifier loss: 0.426736\n",
            "epoch 43; iter: 200; batch classifier loss: 0.382165\n",
            "epoch 44; iter: 0; batch classifier loss: 0.468207\n",
            "epoch 44; iter: 200; batch classifier loss: 0.359380\n",
            "epoch 45; iter: 0; batch classifier loss: 0.390489\n",
            "epoch 45; iter: 200; batch classifier loss: 0.320716\n",
            "epoch 46; iter: 0; batch classifier loss: 0.414388\n",
            "epoch 46; iter: 200; batch classifier loss: 0.348907\n",
            "epoch 47; iter: 0; batch classifier loss: 0.368273\n",
            "epoch 47; iter: 200; batch classifier loss: 0.386330\n",
            "epoch 48; iter: 0; batch classifier loss: 0.317362\n",
            "epoch 48; iter: 200; batch classifier loss: 0.378997\n",
            "epoch 49; iter: 0; batch classifier loss: 0.408290\n",
            "epoch 49; iter: 200; batch classifier loss: 0.454090\n",
            "Model Accuracy 0.8019518187401897\n",
            "[0.8019518187401897]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in y_test:\n",
        "  correct_ans.append(i)\n",
        "\n",
        "lists = zip(predictionsAAAAA, correct_ans)\n",
        "\n",
        "preds = []\n",
        "corrects = []\n",
        "\n",
        "score = 0\n",
        "\n",
        "for i in lists:\n",
        "    if int(round(i[0][0])) == int(round(i[1])):\n",
        "        score = score + 1\n",
        "    else:\n",
        "        pass\n",
        "    \n",
        "    var = round(i[0][0])\n",
        "    preds.append(int(var))\n",
        "    corrects.append(i[1])\n",
        "  \n",
        "\n",
        "accuracy = score/len(y_test)\n",
        "print(\"Model Accuracy\", accuracy)\n",
        "\n",
        "conf_mat = confusion_matrix(corrects, preds)\n",
        "sns.heatmap(conf_mat, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)\n",
        "\n",
        "test_pred = test.copy() \n",
        "predictions.resize((len(predictionsAAAAA),1))\n",
        "test_pred.labels = predictions\n",
        "\n",
        "metric = ClassificationMetric(test, test_pred, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "metric_arrs = {}\n",
        "metric_arrs['stat_par_diff']=(metric.statistical_parity_difference())\n",
        "metric_arrs['eq_opp_diff']=(metric.equal_opportunity_difference())\n",
        "metric_arrs['avg_odds_diff']=(metric.average_odds_difference())\n",
        "#metric_arrs['between_group']=(metric.between_group_coefficient_of_variation())\n",
        "#metric_arrs['generalized']=(metric.between_group_generalized_entropy_index())\n",
        "metric_arrs['True_positive_rate']=(metric.generalized_true_positive_rate())\n",
        "metric_arrs['False_positive_rate'] = (metric.num_generalized_false_positives())\n",
        "\n",
        "print(\"C : {} | Activation: {} | Accuracy: {}\".format(c, solv, accuracy))\n",
        "print(metric_arrs)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "N8rFsX62bDPa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "outputId": "7cb331f1-236f-4ff9-de36-cbabf5d63063"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy 0.8019518187401897\n",
            "C : 1.0 | Activation: lbfgs | Accuracy: 0.8019518187401897\n",
            "{'stat_par_diff': -0.05506828011784737, 'eq_opp_diff': -0.02923672575722991, 'avg_odds_diff': -0.024319104585000455, 'True_positive_rate': 1.0, 'False_positive_rate': 0.0}\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD4CAYAAADB0SsLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAANVElEQVR4nO3afZjNdf7H8dfnzBhmKJMx7omJ3HWzbrqRZBO52YplKzablb2kVstupGJpkqV+bESNZdHKMunKEjIY47YQ7bZk0RJDGGbMyM0M4+a7fzSdn8mYiZxz8p7n47rOZc7nnO/3+/5e43l9z804z/MEwCZfqAcAEDgEDhhG4IBhBA4YRuCAYeGBPkBko758TH8VydowIdQj4DKUCpcraJ0rOGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGFfvAJw57TKnLRmrjey/61667NkoLEvpq87yhWpDQV9HXRObbpkmDGjq2YZx+3von/rV5E57WgVWv6f1xfQo8zpjnfqH0j8YE5iQgSdq960s90rmj/3bX7Y01Y/rb2r5tm371y0fVpdODeubpPjp+/Hi+7Q7s3687mzbS36ZNCdHkgVPsA39n/jp1/O2b+dYG9GyjFZ9s180dX9aKT7ZrQM/7/Y/5fE6v9Ouo5HXb8m3z+vRk9RoyvcBjNG5QQ9HXRF354ZFPzVpxmj1nnmbPmadZ781RqVKRatW6jeKHDla/3z+r9+fOV6vWrfX21L/m2270a6N0d4sWIZo6sIp94B/9c6cyv87Ot/bAT2/RjPnrJUkz5q/Xg/fe4n/s6a4tNXfZv5WeeSzfNis++ULHTpy6YP8+n9Of+nfS4HFzAzA9Lmb9urWqXr26qlSpqtTU3WrS9DZJUrNmzbVs6RL/81KWJatqtaq6oXadUI0aUEUG7pyr55wb5Jx7I+82yDlXPxjDhUqFmGuUlnFUkpSWcVQVYq6RJFWJLauHWt2qSe+t/t77eurRllq4crN/fwiOpEUL1a7DA5KkG2rX0fKUZZKkJYuTlJZ2QJKUfeKEpk2ZrD5P9Q3ZnIFWaODOuUGSEiU5SZ/k3ZykWc655wvZrrdzbqNzbuOZjC1Xct6Q8Lxv/v2/gV00ZNw8ed8uFKFybFl1btNIbyWuDOB0+K7TublauTxF97dtJ0mKHz5C7ybOVNeHOys7+4RKlIiQJCW8NUHdH++hqNKlQzluQIUX8XgvSQ09zzt9/qJz7s+StkgaVdBGnudNkjRJkiIb9f1+NfyIHDp8TJXKX6u0jKOqVP5a/8vxxg1qaPqonpKkmOgyant3Q505c07zV2wqcD+31q2muOqx2vLBMElSVKkS+nzeMN3UMT44J1JMrVmzSvUaNFRM+fKSpFpxN+gvk6dKknbv3qVVK1dIkjZv+reSlyzW2DGjdezYUTnnU0RESXV7rHuoRr/iigr8nKQqklK/s1457zGTFq7crO4P3qHR05aq+4N3aEFewPUfeMn/nEnx3bVo9ecXjVuSktZsUa02///pfPpHY4g7CBZ9uFDtO/zMf//w4cOKiYnRuXPnNPkvCXr40a6SpLffmel/TsKb4xUVFWUqbqnowPtLWuac+6+kvXlrNSTVlmTijcvfRv5aLZrUUfnoMtqRNFzDJ36o0dOWasarT6hHp2bacyBT3Z+bWuR+kqf01421KqpMZEntSBquPvEzlbx2axDOAOfLzs7Wuo8/1h+HvexfS/pwgRJnfRPzfa3bqNPPu4RqvKBzRb2fdM75JN0uqWre0j5JGzzPO/t9DnA1vkQvzrI2TAj1CLgMpcLlClov6gouz/POSVp3xScCEHDF/ntwwDICBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDHOe5wX0ADvTcwJ7AFxR0VElQj0CLkNM6XBX0DpXcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMAwAgcMI3DAMAIHDCNwwDACBwwjcMCw8FAP8GOSfjBNY14ZoqysTDlJ7R7qok6PPKbVKUv096kTtTd1l16fPEM31mvo3+bdd6ZoyYK58vl86tN/kJrccZck6R/vvqPF8/8h55xqxtXR71+MV0TJkiE6M7tGvDREH61eqevKldPf35snSZr01htavWK5fD6n6HIxGhI/QrGxFbRqRYomvzVePp9TWFi4+g0YpFsbNdGnG9brjTGv+veZunuX4keOVst77wvVaV0xzvO8gB5gZ3pOYA9wBWVmpCvzcIZq162v7OwT+t0T3TR05OuSc/L5fBr/2nD16vsHf+B7du3Uqy+9oLGTZ+hwRrpe7P+kJs+ap6zMDA18uqcmzpijkiVL6U9/HKjbmt2tNh06hvgMixYdVSLUI1ySf326UVFRUXp56Av+wE8cP67SZcpIkmbPmqHdX+7Uc4OHKTv7hCIjo+Sc044vtmvI888qcc6CfPs7+vURPdyxveYtSlGpyMign8/liikd7gpa5yX6ecqVj1XtuvUlSVFRpVWjZpwyMg6pRs04VatR84Lnr12zQve0bqsSERGqVKWqqlSrri+2fi5JOnv2rHJPndLZM2d06tRJxZSPDeapFBuNmjTVtWXL5lv7Nm5JOpmTI+e++b8fFVXa/3NOTo6cLmwiJXmJmjVvcVXFXRheol/EwQP7tPOLbarX4OaLPudw+iHVa3iL/3752Io6nH5I9W+6VZ27Pq4eXdopomQpNb7tTjW+/a5gjI08EyeMU9LCD1S6TBlNmDTNv74yJVkJE8YqK/OwRo9LuGC75MWL1K17j2COGlCXfQV3zvUs5LHezrmNzrmNidOnXO4hQiYnO1sjBg9Q734DFVW6TNEbfMexo0e1bs0KTZu9UDPmLtHJkzlKWbwwAJPiYvr07ae5i5apbfsH9H7iTP96y1atlThngUaNGa/JCePzbZORnq4vd/xXdzRrHuxxA+aHvESPv9gDnudN8jyvqed5Tbs+3usHHCL4zpw5rRFDntVP7++g5i0L/5AlJraC0g+l+e9npB9UTGwFfbZxnSpVrqqy15VTeHgJNb/nPm3d/FmgR0cB7m//My1PWXrBeqMmTbV/31c6kpXlX1u2NEn33HufwktcXZ9DFKbQwJ1zmy5y2yypYpBmDBrP8zR2ZLyqX19Lnbv+qsjn39m8pVYlL9bp3Fyl7d+n/Xv36Mb6Nym2YmVt27JJJ0/myPM8ffbpelWvGReEM4Ak7d2T6v959crlur5mLUnSV3tS9e2Hytu3/ke5ubkqGx3tf25y0odq065DcIcNsKLeg1eU1FZS1nfWnaSPAzJRCP1n02dKWbxANW+oo76/fkSS1OPJZ3Q697QSxo7S10ey9NLAZxRXp65e+XOCro+rrRat2ujJ7p0VFhamp/7wgsLCwlSv4c26+97W+t0T3RQWFqa4G+up/UNdQnx2Ng19YYD+9ekGHTlyRB3btdJv+vxWa9esUmrqbvmcT5UqV9Zzg4dJkpanLFXSgg8UHh6uiJKlNHzUaP+Hbgf279PBg2lq1OS2UJ7OFVfo12TOuSmSpnmet6aAx2Z6nvfLog5wNX1NhqvvazJ842Jfk/E9OPIh8KsT34MDxRCBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YBiBA4YROGAYgQOGEThgGIEDhhE4YJjzPC/UM1y1nHO9Pc+bFOo58P0Ux98XV/AfpneoB8AlKXa/LwIHDCNwwDAC/2GK1fs5A4rd74sP2QDDuIIDhhE4YBiBXwbnXDvn3Hbn3A7n3POhngeFc85Ndc4dcs59HupZgo3AL5FzLkzSm5LaS2ogqZtzrkFop0IR3pbULtRDhAKBX7rbJe3wPO9Lz/NyJSVK6hjimVAIz/NWScoM9RyhQOCXrqqkvefd/ypvDfjRIXDAMAK/dPskVT/vfrW8NeBHh8Av3QZJdZxztZxzEZK6SvogxDMBBSLwS+R53hlJfSUtlrRV0mzP87aEdioUxjk3S9JaSXWdc18553qFeqZg4U9VAcO4ggOGEThgGIEDhhE4YBiBA4YROGAYgQOG/Q8FDUFUL5NRgAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}